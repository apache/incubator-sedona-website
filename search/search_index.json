{
    "docs": [
        {
            "location": "/", 
            "text": "On 07/19/2020, GeoSpark has been accepted to the Apache Software Foundation under the new name Apache Sedona (incubating). This website will be refactored in the weeks to come.\n\n\n\n\n || \nContact us\n\n\nNews!\n\n\n\n\nGeoSpark 1.3.1 is released. This version provides a complete Python wrapper to GeoSpark RDD and SQL API. It also contains a number of bug fixes and new functions from 12 contributors. See \nPython tutorial: RDD\n, \nPython tutorial: SQL\n, \nRelease note\n\n\n\n\nCompanies are using Sedona\n\n\n(incomplete list)\n\n\n \n \n \n \n\n\nPlease make a Pull Request to add yourself!\n\n\nIntroduction\n\n\nApache Sedona (incubating) is a cluster computing system for processing large-scale spatial data. Sedona extends Apache Spark / SparkSQL with a set of out-of-the-box Spatial Resilient Distributed Datasets (SRDDs)/ SpatialSQL that efficiently load, process, and analyze large-scale spatial data across machines.\n\n\nSedona contains several modules:\n\n\n\n\n\n\n\n\nName\n\n\nAPI\n\n\nSpark compatibility\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\nCore\n\n\nRDD\n\n\nSpark 2.X/1.X\n\n\nSpatialRDDs and Query Operators.\n\n\n\n\n\n\nSQL\n\n\nSQL/DataFrame\n\n\nSparkSQL 2.1+\n\n\nSQL interfaces for Sedona core.\n\n\n\n\n\n\nViz\n\n\nRDD, SQL/DataFrame\n\n\nRDD - Spark 2.X/1.X, SQL - Spark 2.1+\n\n\nVisualization for Spatial RDD and DataFrame.\n\n\n\n\n\n\nZeppelin\n\n\nApache Zeppelin\n\n\nSpark 2.1+, Zeppelin 0.8.1+\n\n\nPlugin for Apache Zeppelin\n\n\n\n\n\n\n\n\nImpact\n\n\nGeoSpark Downloads\n\n\nGeoSpark ecosystem has around 200K downloads per month.\n\n\nFeatures\n\n\n\n\nSpatial RDD\n\n\nSpatial SQL\n\n\nSELECT\n \nsuperhero\n.\nname\n\n\nFROM\n \ncity\n,\n \nsuperhero\n\n\nWHERE\n \nST_Contains\n(\ncity\n.\ngeom\n,\n \nsuperhero\n.\ngeom\n)\n\n\nAND\n \ncity\n.\nname\n \n=\n \nGotham\n;\n\n\n\n\nComplex geometries / trajectories\n: point, polygon, linestring, multi-point, multi-polygon, multi-linestring, GeometryCollection\n\n\nVarious input formats\n: CSV, TSV, WKT, WKB, GeoJSON, NASA NetCDF/HDF, Shapefile (.shp, .shx, .dbf): extension must be in lower case\n\n\nSpatial query\n: range query, range join query, distance join query, K Nearest Neighbor query\n\n\nSpatial index\n: R-Tree, Quad-Tree\n\n\nSpatial partitioning\n: KDB-Tree, Quad-Tree, R-Tree, Voronoi diagram, Hilbert curve, Uniform grids\n\n\nCoordinate Reference System / Spatial Reference System Transformation\n: for exmaple, from WGS84 (EPSG:4326, degree-based), to EPSG:3857 (meter-based)\n\n\nHigh resolution map\n: Scatter plot, heat map, choropleth map\n\n\n\n\nGeoSpark language support\n\n\n\n\n\n\n\n\nLanguage\n\n\nSupported GeoSpark modules\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\nScala\n\n\nRDD, SQL, Viz, Zeppelin\n\n\nRDD\n, \nSQL\n, \nViz\n, \nZeppelin\n\n\n\n\n\n\nJava\n\n\nRDD, SQL, Viz\n\n\nRDD\n, \nSQL\n, \nViz\n\n\n\n\n\n\nSQL\n\n\nSQL, Viz, Zeppelin\n\n\nSQL\n, \nViz\n, \nZeppelin\n\n\n\n\n\n\nPython\n\n\nRDD, SQL\n\n\nRDD\n, \nSQL\n\n\n\n\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\nGeoSpark Visualization Extension (GeoSparkViz)\n\n\nGeoSparkViz is a large-scale in-memory geospatial visualization system.\n\n\nGeoSparkViz provides native support for general cartographic design by extending GeoSpark to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.\n\n\nMore details are available here: \nVisualize Spatial DataFrame/RDD\n\n\nGeoSparkViz Gallery\n\n\n\n\n\n\nWatch the high resolution version on a real map\n\n\n\n\nResearch\n\n\nGeoSpark development team has published many papers about GeoSpark. Please read \nPublications\n. \n\n\nGeoSpark received an evaluation from PVLDB 2018 paper \n\"How Good Are Modern Spatial Analytics Systems?\"\n Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows: \n\n\n\n\nGeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.\n\n\n\n\nPowered by", 
            "title": "Home"
        }, 
        {
            "location": "/#on-07192020-geospark-has-been-accepted-to-the-apache-software-foundation-under-the-new-name-apache-sedona-incubating-this-website-will-be-refactored-in-the-weeks-to-come", 
            "text": "||  Contact us", 
            "title": "On 07/19/2020, GeoSpark has been accepted to the Apache Software Foundation under the new name Apache Sedona (incubating). This website will be refactored in the weeks to come."
        }, 
        {
            "location": "/#news", 
            "text": "GeoSpark 1.3.1 is released. This version provides a complete Python wrapper to GeoSpark RDD and SQL API. It also contains a number of bug fixes and new functions from 12 contributors. See  Python tutorial: RDD ,  Python tutorial: SQL ,  Release note", 
            "title": "News!"
        }, 
        {
            "location": "/#companies-are-using-sedona", 
            "text": "(incomplete list)           Please make a Pull Request to add yourself!", 
            "title": "Companies are using Sedona"
        }, 
        {
            "location": "/#introduction", 
            "text": "Apache Sedona (incubating) is a cluster computing system for processing large-scale spatial data. Sedona extends Apache Spark / SparkSQL with a set of out-of-the-box Spatial Resilient Distributed Datasets (SRDDs)/ SpatialSQL that efficiently load, process, and analyze large-scale spatial data across machines.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#sedona-contains-several-modules", 
            "text": "Name  API  Spark compatibility  Introduction      Core  RDD  Spark 2.X/1.X  SpatialRDDs and Query Operators.    SQL  SQL/DataFrame  SparkSQL 2.1+  SQL interfaces for Sedona core.    Viz  RDD, SQL/DataFrame  RDD - Spark 2.X/1.X, SQL - Spark 2.1+  Visualization for Spatial RDD and DataFrame.    Zeppelin  Apache Zeppelin  Spark 2.1+, Zeppelin 0.8.1+  Plugin for Apache Zeppelin", 
            "title": "Sedona contains several modules:"
        }, 
        {
            "location": "/#impact", 
            "text": "", 
            "title": "Impact"
        }, 
        {
            "location": "/#geospark-downloads", 
            "text": "GeoSpark ecosystem has around 200K downloads per month.", 
            "title": "GeoSpark Downloads"
        }, 
        {
            "location": "/#features", 
            "text": "Spatial RDD  Spatial SQL  SELECT   superhero . name  FROM   city ,   superhero  WHERE   ST_Contains ( city . geom ,   superhero . geom )  AND   city . name   =   Gotham ;   Complex geometries / trajectories : point, polygon, linestring, multi-point, multi-polygon, multi-linestring, GeometryCollection  Various input formats : CSV, TSV, WKT, WKB, GeoJSON, NASA NetCDF/HDF, Shapefile (.shp, .shx, .dbf): extension must be in lower case  Spatial query : range query, range join query, distance join query, K Nearest Neighbor query  Spatial index : R-Tree, Quad-Tree  Spatial partitioning : KDB-Tree, Quad-Tree, R-Tree, Voronoi diagram, Hilbert curve, Uniform grids  Coordinate Reference System / Spatial Reference System Transformation : for exmaple, from WGS84 (EPSG:4326, degree-based), to EPSG:3857 (meter-based)  High resolution map : Scatter plot, heat map, choropleth map", 
            "title": "Features"
        }, 
        {
            "location": "/#geospark-language-support", 
            "text": "Language  Supported GeoSpark modules  Tutorial      Scala  RDD, SQL, Viz, Zeppelin  RDD ,  SQL ,  Viz ,  Zeppelin    Java  RDD, SQL, Viz  RDD ,  SQL ,  Viz    SQL  SQL, Viz, Zeppelin  SQL ,  Viz ,  Zeppelin    Python  RDD, SQL  RDD ,  SQL    R  SQL", 
            "title": "GeoSpark language support"
        }, 
        {
            "location": "/#geospark-visualization-extension-geosparkviz", 
            "text": "GeoSparkViz is a large-scale in-memory geospatial visualization system.  GeoSparkViz provides native support for general cartographic design by extending GeoSpark to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.  More details are available here:  Visualize Spatial DataFrame/RDD  GeoSparkViz Gallery    Watch the high resolution version on a real map", 
            "title": "GeoSpark Visualization Extension (GeoSparkViz)"
        }, 
        {
            "location": "/#research", 
            "text": "GeoSpark development team has published many papers about GeoSpark. Please read  Publications .   GeoSpark received an evaluation from PVLDB 2018 paper  \"How Good Are Modern Spatial Analytics Systems?\"  Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows:    GeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.", 
            "title": "Research"
        }, 
        {
            "location": "/#powered-by", 
            "text": "", 
            "title": "Powered by"
        }, 
        {
            "location": "/download/overview/", 
            "text": "Direct download\n\n\nGeoSpark source code is hosted on \nGeoSpark GitHub repository\n.\n\n\nGeoSpark pre-compiled JARs are hosted on \nGeoSpark GitHub Releases\n.\n\n\nGeoSpark pre-compiled JARs are hosted on \nMaven Central\n.\n\n\nGeoSpark release notes are here \nRelease notes\n.\n\n\nInstall GeoSpark\n\n\nBefore starting the GeoSpark journey, you need to make sure your Apache Spark cluster is ready.\n\n\nThere are two ways to use a Scala or Java library with Apache Spark. You can user either one to run GeoSpark.\n\n\n\n\nSpark interactive Scala shell: easy to start, good for new learners to try simple functions\n\n\nSelf-contained Scala / Java project: a steep learning curve of package management, but good for large projects", 
            "title": "Quick start"
        }, 
        {
            "location": "/download/overview/#direct-download", 
            "text": "GeoSpark source code is hosted on  GeoSpark GitHub repository .  GeoSpark pre-compiled JARs are hosted on  GeoSpark GitHub Releases .  GeoSpark pre-compiled JARs are hosted on  Maven Central .  GeoSpark release notes are here  Release notes .", 
            "title": "Direct download"
        }, 
        {
            "location": "/download/overview/#install-geospark", 
            "text": "Before starting the GeoSpark journey, you need to make sure your Apache Spark cluster is ready.  There are two ways to use a Scala or Java library with Apache Spark. You can user either one to run GeoSpark.   Spark interactive Scala shell: easy to start, good for new learners to try simple functions  Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects", 
            "title": "Install GeoSpark"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/", 
            "text": "v1.3.1\n\n\nThis version includes the official release of GeoSpark Python wrapper. It also contains a number of bug fixes and new functions. The tutorial section provides some articles to explain the usage of GeoSpark Python wrapper.\n\n\nGeoSpark Core\n\n\nBug fix:\n\n\n\n\nIssue #\n344\n and PR #\n365\n: GeoJSON reader cannot handle \"id\"\n\n\nIssue #\n420\n and PR #\n421\n: Cannot handle null value in geojson properties\n\n\nPR #\n422\n: Use HTTPS to resolve dependencies in Maven Build\n\n\n\n\nNew functions:\n\n\n\n\nIssue #\n399\n and PR #\n401\n: saveAsWKB\n\n\nPR #\n402\n: saveAsWKT\n\n\n\n\nGeoSpark SQL\n\n\nNew functions:\n\n\n\n\nPR #\n359\n: ST_NPoints\n\n\nPR #\n373\n: ST_GeometryType\n\n\nPR #\n398\n: ST_SimplifyPreserveTopology\n\n\nPR #\n406\n: ST_MakeValid\n\n\nPR #\n416\n: ST_Intersection_aggr\n\n\n\n\nPerformance:\n\n\n\n\nIssue #\n345\n and PR #\n346\n: the performance issue of Adapter.toDF() function\n\n\n\n\nBug fix:\n\n\n\n\nIssue #\n395\n and PR #\n396\n: Fix the geometry col bug in Adapter\n\n\n\n\nGeoSpark Viz\n\n\nBug fix:\n\n\n\n\nIssue #\n378\n and PR #\n379\n: Classpath issue when integrating GeoSparkViz with s3\n\n\n\n\nGeoSpark Python\n\n\nAdd new GeoSpark python wrapper for RDD and SQL APIs\n\n\nContributors (12)\n\n\n\n\nMariano Gonzalez\n\n\nPawe\u0142 Koci\u0144ski\n\n\nSemen Komissarov\n\n\nJonathan Leitschuh\n\n\nNetanel Malka\n\n\nKeivan Shahida\n\n\nSachio Wakai\n\n\nHui Wang\n\n\nWrussia\n\n\nJia Yu\n\n\nHarry Zhu\n\n\nIlya Zverev\n\n\n\n\nv1.3.0\n\n\nThis release has been skipped due to a bug in GeoSpark Python wrapper.\n\n\nv1.2.0\n\n\nThis version contains numerous bug fixes, new functions, and new GeoSpark module.\n\n\nLicense change\n\n\nFrom MIT to Apache License 2.0\n\n\nGeoSpark Core\n\n\nBug fix:\n\n\n\n\nIssue #\n224\n load GeoJSON non-spatial attributes.\n\n\nIssue #\n228\n Shapefiel Reader fails to handle UNDEFINED type.\n\n\nIssue #\n320\n Read CSV ArrayIndexOutOfBoundsException\n\n\n\n\nNew functions:\n\n\n\n\nPR #\n270\n #\n298\n Add GeoJSON Reader to load GeoJSON with all attributes. See \nGeoSpark doc\n for an example.\n\n\nPR #\n314\n Add WktReader and WkbReader. Their usage is simialr to GeoJSON reader.\n\n\n\n\nGeoSpark SQL\n\n\nBug fix:\n\n\n\n\nIssue #\n244\n JTS side location conflict\n\n\nIssue #\n245\n Drop ST_Circle in 1.2.0\n\n\nIssue #\n288\n ST_isValid fails\n\n\nIssue #\n321\n ST_Point doesn't accept null user data\n\n\nPR #\n284\n ST_Union_Aggr bug\n\n\nPR #\n331\n Adapter doesn't handle null values\n\n\n\n\nNew SQL functions:\n\n\n\n\nST_IsValid\n\n\nST_PrecisionReduce\n\n\nST_Touches\n\n\nST_Overlaps\n\n\nST_Equals\n\n\nST_Crosses\n\n\nST_IsSimple\n\n\nST_AsText\n\n\n\n\nBehavior / API change:\n\n\n\n\nGeoSpark Adapter will automatically carry all attributes between DataFrame and RDD. No need to use UUID in SQL ST functions to pass values. Please read \nGeoSpark doc\n.\n\n\n\n\nGeoSpark Viz\n\n\nBug fix:\n\n\n\n\nIssue #\n231\n Pixel NullPointException\n\n\nIssue #\n234\n OutOfMemory for large images\n\n\n\n\nNew functions\n\n\n\n\nAdd the DataFrame support. Please read \nGeoSpark doc\n\n\nST_Pixelize\n\n\nST_TileName\n\n\nST_Colorize\n\n\nST_EncodeImage\n\n\nST_Render\n\n\n\n\nBehavior / API change\n\n\n\n\nGeoSparkViz Maven coordinate changed. You need to specify Spark version. Please read \nGeoSpark Maven coordinate\n\n\n\n\nGeoSpark-Zeppelin\n\n\nNew functions\n\n\n\n\nAdd the support of connecting GeoSpark and Zeppelin\n\n\nAdd the support of connecting GeoSparkViz and Zeppelin\n\n\n\n\nContributors (13)\n\n\nAnton Peniaziev, Avshalom Orenstein, Jia Yu, Jordan Perr-Sauer, JulienPeloton, Sergii Mikhtoniuk, Netanel Malka, Rishabh Mishra, sagar1993, Shi-Hao Liu, Serhuela, tociek, Wrussia\n\n\nv1.1.3\n\n\nThis version contains a critical bug fix for GeoSpark-core RDD API.\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n222\n: geometry toString() method has cumulative non-spatial attributes. See PR #\n223\n\n\n\n\nGeoSpark SQL\n\n\nNone\n\n\nGeoSpark Viz\n\n\nNone\n\n\nv1.1.2\n\n\nThis version contains several bug fixes and several small improvements.\n\n\nGeoSpark Core\n\n\n\n\nAdded WKB input format support (Issue #\n2\n, \n213\n): See PR #\n203\n, \n216\n. Thanks for the patch from Lucas C.!\n\n\nAdded empty constructors for typed SpatialRDDs. This is especially useful when the users want to load a persisted RDD from disk and assemble a typed SpatialRDD by themselves. See PR #\n211\n\n\nFixed Issue #\n214\n: duplicated geometry parts when print each Geometry in a SpatialRDD to a String using toString() method. See PR #\n216\n\n\n\n\nGeoSpark SQL\n\n\n\n\nAdded ST_GeomFromWKB expression (Issue #\n2\n): See PR #\n203\n. Thanks for the patch from Lucas C.!\n\n\nFixed Issue #\n193\n: IllegalArgumentException in RangeJoin: Number of partitions must be \n= 0. See PR #\n207\n\n\nFixed Issue #\n204\n: Wrong ST_Intersection result. See PR #\n205\n\n\n[For Developer] Separate the expression catalog and the udf registrator to simplify the steps of merging patches among different Spark versions. See PR #\n209\n\n\n\n\nGeoSpark Viz\n\n\nNone\n\n\nv1.1.1\n\n\nThis release has been skipped due to wrong Maven Central configuration.\n\n\nv1.1.0\n\n\nThis version adds very efficient R-Tree and Quad-Tree index serializers and supports Apache Spark and  SparkSQL 2.3. See \nMaven Central coordinate\n to locate the particular version.\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n185\n: CRStransform throws Exception for Bursa wolf parameters. See PR #\n189\n.\n\n\nFixed Issue #\n190\n: Shapefile reader doesn't support Chinese characters (\u4e2d\u6587\u5b57\u7b26). See PR #\n192\n.\n\n\nAdd R-Tree and Quad-Tree index serializer. GeoSpark custom index serializer has around 2 times smaller index size and faster serialization than Apache Spark kryo serializer. See PR #\n177\n.\n\n\n\n\nGeoSpark SQL\n\n\n\n\nFixed Issue #\n194\n: doesn't support Spark 2.3.\n\n\nFixed Issue #\n188\n:ST_ConvexHull should accept any type of geometry as an input. See PR #\n189\n.\n\n\nAdd ST_Intersection function. See Issue #\n110\n and PR #\n189\n.\n\n\n\n\nGeoSpark Viz\n\n\n\n\nFixed Issue #\n154\n: GeoSpark kryp serializer and GeoSparkViz conflict. See PR #\n178\n\n\n\n\nv1.0.1\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n170\n\n\n\n\nGeoSpark SQL\n\n\n\n\nFixed Issue #\n171\n\n\nAdded the support of SparkSQL 2.2. GeoSpark-SQL for Spark 2.1 is published separately (\nMaven Coordinates\n).\n\n\n\n\nGeoSpark Viz\n\nNone\n\n\n\n\nv1.0.0\n\n\nGeoSpark Core\n\n\n\n\nAdd GeoSparkConf class to read GeoSparkConf from SparkConf\n\n\n\n\nGeoSpark SQL\n\n\n\n\nInitial release: fully supports SQL/MM-Part3 Spatial SQL standard\n\n\n\n\nGeoSpark Viz\n\n\n\n\nRepublish GeoSpark Viz under \"GeoSparkViz\" folder. All \"Babylon\" strings have been replaced to \"GeoSparkViz\"\n\n\n\n\n\n\n\n\n\n\nv0.9.1 (GeoSpark-core)\n\n\n\n\nBug fixes\n: Fixed \"Missing values when reading Shapefile\": \nIssue #141\n\n\nPerformance improvement\n: Solved Issue \n#91\n, \n#103\n, \n#104\n, \n#125\n, \n#150\n.\n\n\nAdd GeoSpark customized Kryo Serializer to significantly reduce memory footprint. This serializer which follows Shapefile compression rule takes less memory than the default Kryo. See \nPR 139\n.\n\n\nDelete the duplicate removal by using Reference Point concept. This eliminates one data shuffle but still guarantees the accuracy. See \nPR 131\n.\n\n\n\n\n\n\nNew Functionalities added\n:\n\n\nSpatialJoinQueryFlat/DistanceJoinQueryFlat\n returns the join query in a flat way following database iteration model: Each row has fixed two members [Polygon, Point]. This API is more efficient for unbalanced length of join results. \n\n\nThe left and right shapes in Range query, Distance query, Range join query, Distance join query can be switched.\n\n\nThe index side in Range query, Distance query, Range join query, Distance join query can be switched.\n\n\nThe generic SpatialRdd supports heterogenous geometries\n\n\nAdd KDB-Tree spatial partitioning method which is more balanced than Quad-Tree\n\n\nRange query, Distance query, Range join query, Distance join query, KNN query supports heterogenous inputs.\n\n\n\n\n\n\n\n\nv0.8.2 (GeoSpark-core)\n\n\n\n\nBug fixes\n: Fix the shapefile RDD null pointer bug when running in cluster mode. See Issue \nhttps://github.com/DataSystemsLab/GeoSpark/issues/115\n\n\nNew function added\n: Provide granular control to SpatialRDD sampling utils. SpatialRDD has a setter and getter for a parameter called \"sampleNumber\". The user can manually specify the sample size for spatial partitioning.\n\n\n\n\nv0.8.1 (GeoSpark-core)\n\n\n\n\nBug fixes\n: (1) Fix the blank DBF attribute error when load DBF along with SHX file. (2) Allow user to call CRS transformation function at any time. Previously, it was only allowed in GeoSpark constructors\n\n\n\n\nv0.8.0 (GeoSpark-core)\n\n\n\n\nNew input format added\n: GeoSpark is able to load and query ESRI ShapeFile (.shp, .shx, .dbf) from local disk and HDFS! Users first need to build a Shapefile RDD by giving Spark Context and an input path then call ShapefileRDD.getSpatialRDD to retrieve Spatial RDD. (\nScala Example\n, \nJava Example\n)\n\n\nJoin Query Performance enhancement 1\n: GeoSpark provides a new Quad-Tree Spatial Partitioning method to speed up Join Query. Users need to pass GridType.QUADTREE parameter to RDD1.spatialPartitioning() function. Then users need to use RDD1.partitionTree in RDD2.spatialPartitioning() function. This Quad-Tree partitioning method (1) avoids overflowed spatial objects when partitioning spatial objects. (2) checking a spatial object against the Quad-Tree grids is completed in a log complexity tree search. (\nScala Example\n, \nJava Example\n)\n\n\nJoin Query Performance enhancement 2\n: Internally, GeoSpark uses zipPartitions instead of CoGroup to join two Spatial RDD so that the incurred shuffle overhead decreases.\n\n\nSpatialRDD Initialization Performance enhancement\n: GeoSpark uses mapPartition instead of flatMapToPair to generate Spatial Objects. This will speed up the calculation.\n\n\nAPI changed\n: Since it chooses mapPartition API in mappers, GeoSpark no longer supports the old user supplified format mapper. However, if you are using your own format mapper for old GeoSpark version, you just need to add one more loop to fit in GeoSpark 0.8.0. Please see \nGeoSpark user supplied format mapper examples\n\n\nAlternative SpatialRDD constructor added\n: GeoSpark no longer forces users to provide StorageLevel parameter in their SpatialRDD constructors. This will siginicantly accelerate all Spatial RDD initialization.\n\n\nIf he only needs Spatial Range Query and KNN query, the user can totally remove this parameter from their constructors.\n\n\nIf he needs Spatial Join Query or Distance Join Query but he knows his dataset boundary and approximate total count, the user can also remove StorageLevel parameter and append a Envelope type dataset boundary and an approxmiate total count as additional parameters.\n\n\nIf he needs Spatial Join Query or Distance Join Query but knows nothing about his dataset\n, the user still has to pass StorageLevel parameter.\n\n\nBug fix\n: Fix bug \nIssue #97\n and \nIssue #100\n.\n\n\n\n\nv0.1 - v0.7\n\n\n\n\n\n\n\n\nVersion\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n0.7.0\n\n\nCoordinate Reference System (CRS) Transformation (aka. Coordinate projection) added:\n GeoSpark allows users to transform the original CRS (e.g., degree based coordinates such as EPSG:4326 and WGS84) to any other CRS (e.g., meter based coordinates such as EPSG:3857) so that it can accurately process both geographic data and geometrical data. Please specify your desired CRS in GeoSpark Spatial RDD constructor (\nExample\n); \nUnnecessary dependencies removed\n: NetCDF/HDF support depends on \nSerNetCDF\n. SetNetCDF becomes optional dependency to reduce fat jar size; \nDefault JDK/JRE change to JDK/JRE 1.8\n: To satisfy CRS transformation requirement, GeoSpark is compiled by JDK 1.8 by default; \nBug fix\n: fix a small format bug when output spatial RDD to disk.\n\n\n\n\n\n\n0.6.2\n\n\nNew input format added:\n Add a new input format mapper called EarthdataHDFPointMapper so that GeoSpark can load, query and save NASA Petabytes NetCDF/HDF Earth Data (\nScala Example\n,\nJava Example\n); \nBug fix:\n Print UserData attribute when output Spatial RDDs as GeoJSON or regular text file\n\n\n\n\n\n\n0.6.1\n\n\nBug fixes:\n Fix typos LineString DistanceJoin API\n\n\n\n\n\n\n0.6.0\n\n\nMajor updates:\n (1) DistanceJoin is merged into JoinQuery. GeoSpark now supports complete DistanceJoin between Points, Polygons, and LineStrings. (2) Add Refine Phase to Spatial Range and Join Query. Use real polygon coordinates instead of its MBR to filter the final results. \nAPI changes:\n All spatial range and join  queries now take a parameter called \nConsiderBoundaryIntersection\n. This will tell GeoSpark whether returns the objects intersect with windows.\n\n\n\n\n\n\n0.5.3\n\n\nBug fix:\n Fix \nIssue #69\n: Now, if two objects have the same coordinates but different non-spatial attributes (UserData), GeoSpark treats them as different objects.\n\n\n\n\n\n\n0.5.2\n\n\nBug fix:\n Fix \nIssue #58\n and \nIssue #60\n; \nPerformance enhancement:\n (1) Deprecate all old Spatial RDD constructors. See the JavaDoc \nhere\n. (2) Recommend the new SRDD constructors which take an additional RDD storage level and automatically cache rawSpatialRDD to accelerate internal SRDD analyze step\n\n\n\n\n\n\n0.5.1\n\n\nBug fix:\n (1) GeoSpark: Fix inaccurate KNN result when K is large (2) GeoSpark: Replace incompatible Spark API call \nIssue #55\n; (3) Babylon: Remove JPG output format temporarily due to the lack of OpenJDK support\n\n\n\n\n\n\n0.5.0\n\n\nMajor updates:\n We are pleased to announce the initial version of \nBabylon\n a large-scale in-memory geospatial visualization system extending GeoSpark. Babylon and GeoSpark are integrated together. You can just import GeoSpark and enjoy! More details are available here: \nBabylon GeoSpatial Visualization\n\n\n\n\n\n\n0.4.0\n\n\nMajor updates:\n (\nExample\n) 1. Refactor constrcutor API usage. 2. Simplify Spatial Join Query API. 3. Add native support for LineStringRDD; \nFunctionality enhancement:\n 1. Release the persist function back to users. 2. Add more exception explanations.\n\n\n\n\n\n\n0.3.2\n\n\nFunctionality enhancement: 1. \nJTSplus Spatial Objects\n now carry the original input data. Each object stores \"UserData\" and provides getter and setter. 2. Add a new SpatialRDD constructor to transform a regular data RDD to a spatial partitioned SpatialRDD.\n\n\n\n\n\n\n0.3.1\n\n\nBug fix: Support Apache Spark 2.X version, fix a bug which results in inaccurate results when doing join query, add more unit test cases\n\n\n\n\n\n\n0.3\n\n\nMajor updates: Significantly shorten query time on spatial join for skewed data; Support load balanced spatial partitioning methods (also serve as the global index); Optimize code for iterative spatial data mining\n\n\n\n\n\n\n0.2\n\n\nImprove code structure and refactor API\n\n\n\n\n\n\n0.1\n\n\nSupport spatial range, join and Knn\n\n\n\n\n\n\n\n\nGeoSpark-Viz (old)\n\n\n\n\n\n\n\n\nVersion\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n0.2.2\n\n\nAdd the support of new output storage\n: Now the user is able to output gigapixel or megapixel resolution images (image tiles or stitched single image) to HDFS and Amazon S3. Please use the new ImageGenerator not the BabylonImageGenerator class.\n\n\n\n\n\n\n0.2.1\n\n\nPerformance enhancement\n: significantly accelerate single image generation pipeline. \nBug fix\n:fix a bug in scatter plot parallel rendering.\n\n\n\n\n\n\n0.2.0\n\n\nAPI updates for \nIssue #80\n:\n 1. Babylon now has two different OverlayOperators for raster image and vector image: RasterOverlayOperator and VectorOverlayOperator; 2. Babylon merged old SparkImageGenerator and NativeJavaGenerator into a new BabylonImageGenerator which has neat APIs; \nNew feature:\n Babylon can use Scatter Plot to visualize NASA Petabytes NetCDF/HDF format Earth Data. (\nScala Example\n,\nJava Example\n)\n\n\n\n\n\n\n0.1.1\n\n\nMajor updates:\n Babylon supports vector image and outputs SVG image format\n\n\n\n\n\n\n0.1.0\n\n\nMajor updates:\n Babylon initial version supports raster images", 
            "title": "Release notes"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v131", 
            "text": "This version includes the official release of GeoSpark Python wrapper. It also contains a number of bug fixes and new functions. The tutorial section provides some articles to explain the usage of GeoSpark Python wrapper.  GeoSpark Core  Bug fix:   Issue # 344  and PR # 365 : GeoJSON reader cannot handle \"id\"  Issue # 420  and PR # 421 : Cannot handle null value in geojson properties  PR # 422 : Use HTTPS to resolve dependencies in Maven Build   New functions:   Issue # 399  and PR # 401 : saveAsWKB  PR # 402 : saveAsWKT   GeoSpark SQL  New functions:   PR # 359 : ST_NPoints  PR # 373 : ST_GeometryType  PR # 398 : ST_SimplifyPreserveTopology  PR # 406 : ST_MakeValid  PR # 416 : ST_Intersection_aggr   Performance:   Issue # 345  and PR # 346 : the performance issue of Adapter.toDF() function   Bug fix:   Issue # 395  and PR # 396 : Fix the geometry col bug in Adapter   GeoSpark Viz  Bug fix:   Issue # 378  and PR # 379 : Classpath issue when integrating GeoSparkViz with s3   GeoSpark Python  Add new GeoSpark python wrapper for RDD and SQL APIs  Contributors (12)   Mariano Gonzalez  Pawe\u0142 Koci\u0144ski  Semen Komissarov  Jonathan Leitschuh  Netanel Malka  Keivan Shahida  Sachio Wakai  Hui Wang  Wrussia  Jia Yu  Harry Zhu  Ilya Zverev", 
            "title": "v1.3.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v130", 
            "text": "This release has been skipped due to a bug in GeoSpark Python wrapper.", 
            "title": "v1.3.0"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v120", 
            "text": "This version contains numerous bug fixes, new functions, and new GeoSpark module.  License change  From MIT to Apache License 2.0  GeoSpark Core  Bug fix:   Issue # 224  load GeoJSON non-spatial attributes.  Issue # 228  Shapefiel Reader fails to handle UNDEFINED type.  Issue # 320  Read CSV ArrayIndexOutOfBoundsException   New functions:   PR # 270  # 298  Add GeoJSON Reader to load GeoJSON with all attributes. See  GeoSpark doc  for an example.  PR # 314  Add WktReader and WkbReader. Their usage is simialr to GeoJSON reader.   GeoSpark SQL  Bug fix:   Issue # 244  JTS side location conflict  Issue # 245  Drop ST_Circle in 1.2.0  Issue # 288  ST_isValid fails  Issue # 321  ST_Point doesn't accept null user data  PR # 284  ST_Union_Aggr bug  PR # 331  Adapter doesn't handle null values   New SQL functions:   ST_IsValid  ST_PrecisionReduce  ST_Touches  ST_Overlaps  ST_Equals  ST_Crosses  ST_IsSimple  ST_AsText   Behavior / API change:   GeoSpark Adapter will automatically carry all attributes between DataFrame and RDD. No need to use UUID in SQL ST functions to pass values. Please read  GeoSpark doc .   GeoSpark Viz  Bug fix:   Issue # 231  Pixel NullPointException  Issue # 234  OutOfMemory for large images   New functions   Add the DataFrame support. Please read  GeoSpark doc  ST_Pixelize  ST_TileName  ST_Colorize  ST_EncodeImage  ST_Render   Behavior / API change   GeoSparkViz Maven coordinate changed. You need to specify Spark version. Please read  GeoSpark Maven coordinate   GeoSpark-Zeppelin  New functions   Add the support of connecting GeoSpark and Zeppelin  Add the support of connecting GeoSparkViz and Zeppelin   Contributors (13)  Anton Peniaziev, Avshalom Orenstein, Jia Yu, Jordan Perr-Sauer, JulienPeloton, Sergii Mikhtoniuk, Netanel Malka, Rishabh Mishra, sagar1993, Shi-Hao Liu, Serhuela, tociek, Wrussia", 
            "title": "v1.2.0"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v113", 
            "text": "This version contains a critical bug fix for GeoSpark-core RDD API.  GeoSpark Core   Fixed Issue # 222 : geometry toString() method has cumulative non-spatial attributes. See PR # 223   GeoSpark SQL  None  GeoSpark Viz  None", 
            "title": "v1.1.3"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v112", 
            "text": "This version contains several bug fixes and several small improvements.  GeoSpark Core   Added WKB input format support (Issue # 2 ,  213 ): See PR # 203 ,  216 . Thanks for the patch from Lucas C.!  Added empty constructors for typed SpatialRDDs. This is especially useful when the users want to load a persisted RDD from disk and assemble a typed SpatialRDD by themselves. See PR # 211  Fixed Issue # 214 : duplicated geometry parts when print each Geometry in a SpatialRDD to a String using toString() method. See PR # 216   GeoSpark SQL   Added ST_GeomFromWKB expression (Issue # 2 ): See PR # 203 . Thanks for the patch from Lucas C.!  Fixed Issue # 193 : IllegalArgumentException in RangeJoin: Number of partitions must be  = 0. See PR # 207  Fixed Issue # 204 : Wrong ST_Intersection result. See PR # 205  [For Developer] Separate the expression catalog and the udf registrator to simplify the steps of merging patches among different Spark versions. See PR # 209   GeoSpark Viz  None", 
            "title": "v1.1.2"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v111", 
            "text": "This release has been skipped due to wrong Maven Central configuration.", 
            "title": "v1.1.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v110", 
            "text": "This version adds very efficient R-Tree and Quad-Tree index serializers and supports Apache Spark and  SparkSQL 2.3. See  Maven Central coordinate  to locate the particular version.  GeoSpark Core   Fixed Issue # 185 : CRStransform throws Exception for Bursa wolf parameters. See PR # 189 .  Fixed Issue # 190 : Shapefile reader doesn't support Chinese characters (\u4e2d\u6587\u5b57\u7b26). See PR # 192 .  Add R-Tree and Quad-Tree index serializer. GeoSpark custom index serializer has around 2 times smaller index size and faster serialization than Apache Spark kryo serializer. See PR # 177 .   GeoSpark SQL   Fixed Issue # 194 : doesn't support Spark 2.3.  Fixed Issue # 188 :ST_ConvexHull should accept any type of geometry as an input. See PR # 189 .  Add ST_Intersection function. See Issue # 110  and PR # 189 .   GeoSpark Viz   Fixed Issue # 154 : GeoSpark kryp serializer and GeoSparkViz conflict. See PR # 178", 
            "title": "v1.1.0"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v101", 
            "text": "GeoSpark Core   Fixed Issue # 170   GeoSpark SQL   Fixed Issue # 171  Added the support of SparkSQL 2.2. GeoSpark-SQL for Spark 2.1 is published separately ( Maven Coordinates ).   GeoSpark Viz \nNone", 
            "title": "v1.0.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v100", 
            "text": "GeoSpark Core   Add GeoSparkConf class to read GeoSparkConf from SparkConf   GeoSpark SQL   Initial release: fully supports SQL/MM-Part3 Spatial SQL standard   GeoSpark Viz   Republish GeoSpark Viz under \"GeoSparkViz\" folder. All \"Babylon\" strings have been replaced to \"GeoSparkViz\"", 
            "title": "v1.0.0"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v091-geospark-core", 
            "text": "Bug fixes : Fixed \"Missing values when reading Shapefile\":  Issue #141  Performance improvement : Solved Issue  #91 ,  #103 ,  #104 ,  #125 ,  #150 .  Add GeoSpark customized Kryo Serializer to significantly reduce memory footprint. This serializer which follows Shapefile compression rule takes less memory than the default Kryo. See  PR 139 .  Delete the duplicate removal by using Reference Point concept. This eliminates one data shuffle but still guarantees the accuracy. See  PR 131 .    New Functionalities added :  SpatialJoinQueryFlat/DistanceJoinQueryFlat  returns the join query in a flat way following database iteration model: Each row has fixed two members [Polygon, Point]. This API is more efficient for unbalanced length of join results.   The left and right shapes in Range query, Distance query, Range join query, Distance join query can be switched.  The index side in Range query, Distance query, Range join query, Distance join query can be switched.  The generic SpatialRdd supports heterogenous geometries  Add KDB-Tree spatial partitioning method which is more balanced than Quad-Tree  Range query, Distance query, Range join query, Distance join query, KNN query supports heterogenous inputs.", 
            "title": "v0.9.1 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v082-geospark-core", 
            "text": "Bug fixes : Fix the shapefile RDD null pointer bug when running in cluster mode. See Issue  https://github.com/DataSystemsLab/GeoSpark/issues/115  New function added : Provide granular control to SpatialRDD sampling utils. SpatialRDD has a setter and getter for a parameter called \"sampleNumber\". The user can manually specify the sample size for spatial partitioning.", 
            "title": "v0.8.2 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v081-geospark-core", 
            "text": "Bug fixes : (1) Fix the blank DBF attribute error when load DBF along with SHX file. (2) Allow user to call CRS transformation function at any time. Previously, it was only allowed in GeoSpark constructors", 
            "title": "v0.8.1 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v080-geospark-core", 
            "text": "New input format added : GeoSpark is able to load and query ESRI ShapeFile (.shp, .shx, .dbf) from local disk and HDFS! Users first need to build a Shapefile RDD by giving Spark Context and an input path then call ShapefileRDD.getSpatialRDD to retrieve Spatial RDD. ( Scala Example ,  Java Example )  Join Query Performance enhancement 1 : GeoSpark provides a new Quad-Tree Spatial Partitioning method to speed up Join Query. Users need to pass GridType.QUADTREE parameter to RDD1.spatialPartitioning() function. Then users need to use RDD1.partitionTree in RDD2.spatialPartitioning() function. This Quad-Tree partitioning method (1) avoids overflowed spatial objects when partitioning spatial objects. (2) checking a spatial object against the Quad-Tree grids is completed in a log complexity tree search. ( Scala Example ,  Java Example )  Join Query Performance enhancement 2 : Internally, GeoSpark uses zipPartitions instead of CoGroup to join two Spatial RDD so that the incurred shuffle overhead decreases.  SpatialRDD Initialization Performance enhancement : GeoSpark uses mapPartition instead of flatMapToPair to generate Spatial Objects. This will speed up the calculation.  API changed : Since it chooses mapPartition API in mappers, GeoSpark no longer supports the old user supplified format mapper. However, if you are using your own format mapper for old GeoSpark version, you just need to add one more loop to fit in GeoSpark 0.8.0. Please see  GeoSpark user supplied format mapper examples  Alternative SpatialRDD constructor added : GeoSpark no longer forces users to provide StorageLevel parameter in their SpatialRDD constructors. This will siginicantly accelerate all Spatial RDD initialization.  If he only needs Spatial Range Query and KNN query, the user can totally remove this parameter from their constructors.  If he needs Spatial Join Query or Distance Join Query but he knows his dataset boundary and approximate total count, the user can also remove StorageLevel parameter and append a Envelope type dataset boundary and an approxmiate total count as additional parameters.  If he needs Spatial Join Query or Distance Join Query but knows nothing about his dataset , the user still has to pass StorageLevel parameter.  Bug fix : Fix bug  Issue #97  and  Issue #100 .", 
            "title": "v0.8.0 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v01-v07", 
            "text": "Version  Summary      0.7.0  Coordinate Reference System (CRS) Transformation (aka. Coordinate projection) added:  GeoSpark allows users to transform the original CRS (e.g., degree based coordinates such as EPSG:4326 and WGS84) to any other CRS (e.g., meter based coordinates such as EPSG:3857) so that it can accurately process both geographic data and geometrical data. Please specify your desired CRS in GeoSpark Spatial RDD constructor ( Example );  Unnecessary dependencies removed : NetCDF/HDF support depends on  SerNetCDF . SetNetCDF becomes optional dependency to reduce fat jar size;  Default JDK/JRE change to JDK/JRE 1.8 : To satisfy CRS transformation requirement, GeoSpark is compiled by JDK 1.8 by default;  Bug fix : fix a small format bug when output spatial RDD to disk.    0.6.2  New input format added:  Add a new input format mapper called EarthdataHDFPointMapper so that GeoSpark can load, query and save NASA Petabytes NetCDF/HDF Earth Data ( Scala Example , Java Example );  Bug fix:  Print UserData attribute when output Spatial RDDs as GeoJSON or regular text file    0.6.1  Bug fixes:  Fix typos LineString DistanceJoin API    0.6.0  Major updates:  (1) DistanceJoin is merged into JoinQuery. GeoSpark now supports complete DistanceJoin between Points, Polygons, and LineStrings. (2) Add Refine Phase to Spatial Range and Join Query. Use real polygon coordinates instead of its MBR to filter the final results.  API changes:  All spatial range and join  queries now take a parameter called  ConsiderBoundaryIntersection . This will tell GeoSpark whether returns the objects intersect with windows.    0.5.3  Bug fix:  Fix  Issue #69 : Now, if two objects have the same coordinates but different non-spatial attributes (UserData), GeoSpark treats them as different objects.    0.5.2  Bug fix:  Fix  Issue #58  and  Issue #60 ;  Performance enhancement:  (1) Deprecate all old Spatial RDD constructors. See the JavaDoc  here . (2) Recommend the new SRDD constructors which take an additional RDD storage level and automatically cache rawSpatialRDD to accelerate internal SRDD analyze step    0.5.1  Bug fix:  (1) GeoSpark: Fix inaccurate KNN result when K is large (2) GeoSpark: Replace incompatible Spark API call  Issue #55 ; (3) Babylon: Remove JPG output format temporarily due to the lack of OpenJDK support    0.5.0  Major updates:  We are pleased to announce the initial version of  Babylon  a large-scale in-memory geospatial visualization system extending GeoSpark. Babylon and GeoSpark are integrated together. You can just import GeoSpark and enjoy! More details are available here:  Babylon GeoSpatial Visualization    0.4.0  Major updates:  ( Example ) 1. Refactor constrcutor API usage. 2. Simplify Spatial Join Query API. 3. Add native support for LineStringRDD;  Functionality enhancement:  1. Release the persist function back to users. 2. Add more exception explanations.    0.3.2  Functionality enhancement: 1.  JTSplus Spatial Objects  now carry the original input data. Each object stores \"UserData\" and provides getter and setter. 2. Add a new SpatialRDD constructor to transform a regular data RDD to a spatial partitioned SpatialRDD.    0.3.1  Bug fix: Support Apache Spark 2.X version, fix a bug which results in inaccurate results when doing join query, add more unit test cases    0.3  Major updates: Significantly shorten query time on spatial join for skewed data; Support load balanced spatial partitioning methods (also serve as the global index); Optimize code for iterative spatial data mining    0.2  Improve code structure and refactor API    0.1  Support spatial range, join and Knn", 
            "title": "v0.1 - v0.7"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#geospark-viz-old", 
            "text": "Version  Summary      0.2.2  Add the support of new output storage : Now the user is able to output gigapixel or megapixel resolution images (image tiles or stitched single image) to HDFS and Amazon S3. Please use the new ImageGenerator not the BabylonImageGenerator class.    0.2.1  Performance enhancement : significantly accelerate single image generation pipeline.  Bug fix :fix a bug in scatter plot parallel rendering.    0.2.0  API updates for  Issue #80 :  1. Babylon now has two different OverlayOperators for raster image and vector image: RasterOverlayOperator and VectorOverlayOperator; 2. Babylon merged old SparkImageGenerator and NativeJavaGenerator into a new BabylonImageGenerator which has neat APIs;  New feature:  Babylon can use Scatter Plot to visualize NASA Petabytes NetCDF/HDF format Earth Data. ( Scala Example , Java Example )    0.1.1  Major updates:  Babylon supports vector image and outputs SVG image format    0.1.0  Major updates:  Babylon initial version supports raster images", 
            "title": "GeoSpark-Viz (old)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/", 
            "text": "Apache Spark 2.X versions\n\n\nPlease add the following dependencies into your POM.xml or build.sbt\n\n\nGeoSpark-Core\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nGeoSpark-SQL\n\n\nFor SparkSQL-2.3\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n3\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nFor SparkSQL-2.2\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n2\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nFor SparkSQL-2.1\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n1\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nGeoSpark-Viz 1.2.0 and later\n\n\nFor SparkSQL-2.3\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz_2\n.\n3\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nFor SparkSQL-2.2\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz_2\n.\n2\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nFor SparkSQL-2.1\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz_2\n.\n1\n\n\nversion\n:\n \n1.3\n.\n1\n\n\n\n\n\nGeoSpark-Viz 1.1.3 and earlier\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz\n\n\nversion\n:\n \n1.1\n.\n3\n\n\n\n\n\n\n\nApache Spark 1.X versions\n\n\nPlease add the following dependencies into your POM.xml or build.sbt\n\n\nGeoSpark-Core\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n\n\nversion\n:\n \n0.8\n.\n2\n-\nspark\n-\n1\n.\nx\n\n\n\n\n\nGeoSpark-Viz\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \nbabylon\n\n\nversion\n:\n \n0.2\n.\n1\n-\nspark\n-\n1\n.\nx\n\n\n\n\n\n\n\nSNAPSHOT versions\n\n\nSometimes GeoSpark has a SNAPSHOT version for the upcoming release. \"SNAPSHOT\" is uppercase.\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n\n\nversion\n:\n \n1.3\n.\n2\n-\nSNAPSHOT\n\n\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n3\n\n\nversion\n:\n \n1.3\n.\n2\n-\nSNAPSHOT\n\n\n\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz\n\n\nversion\n:\n \n1.3\n.\n2\n-\nSNAPSHOT\n\n\n\n\n\nIn order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt\n\n\nbuild.sbt\n\n\nresolvers +=\n  \"Sonatype OSS Snapshots\" at \"\nhttps://oss.sonatype.org/content/repositories/snapshots\n\"\n\n\nPOM.XML\n\n\nprofiles\n\n    \nprofile\n\n        \nid\nallow-snapshots\n/id\n\n        \nactivation\nactiveByDefault\ntrue\n/activeByDefault\n/activation\n\n        \nrepositories\n\n            \nrepository\n\n                \nid\nsnapshots-repo\n/id\n\n                \nurl\nhttps://oss.sonatype.org/content/repositories/snapshots\n/url\n\n                \nreleases\nenabled\nfalse\n/enabled\n/releases\n\n                \nsnapshots\nenabled\ntrue\n/enabled\n/snapshots\n\n            \n/repository\n\n        \n/repositories\n\n    \n/profile\n\n\n/profiles", 
            "title": "Maven Central coordinate"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#apache-spark-2x-versions", 
            "text": "Please add the following dependencies into your POM.xml or build.sbt", 
            "title": "Apache Spark 2.X versions"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-core", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark  version :   1.3 . 1", 
            "title": "GeoSpark-Core"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-sql", 
            "text": "", 
            "title": "GeoSpark-SQL"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-23", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 3  version :   1.3 . 1", 
            "title": "For SparkSQL-2.3"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-22", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 2  version :   1.3 . 1", 
            "title": "For SparkSQL-2.2"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-21", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 1  version :   1.3 . 1", 
            "title": "For SparkSQL-2.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz-120-and-later", 
            "text": "", 
            "title": "GeoSpark-Viz 1.2.0 and later"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-23_1", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - viz_2 . 3  version :   1.3 . 1", 
            "title": "For SparkSQL-2.3"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-22_1", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - viz_2 . 2  version :   1.3 . 1", 
            "title": "For SparkSQL-2.2"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-21_1", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - viz_2 . 1  version :   1.3 . 1", 
            "title": "For SparkSQL-2.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz-113-and-earlier", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - viz  version :   1.1 . 3", 
            "title": "GeoSpark-Viz 1.1.3 and earlier"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#apache-spark-1x-versions", 
            "text": "Please add the following dependencies into your POM.xml or build.sbt", 
            "title": "Apache Spark 1.X versions"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-core_1", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark  version :   0.8 . 2 - spark - 1 . x", 
            "title": "GeoSpark-Core"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz", 
            "text": "groupId :   org . datasyslab  artifactId :   babylon  version :   0.2 . 1 - spark - 1 . x", 
            "title": "GeoSpark-Viz"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#snapshot-versions", 
            "text": "Sometimes GeoSpark has a SNAPSHOT version for the upcoming release. \"SNAPSHOT\" is uppercase. groupId :   org . datasyslab  artifactId :   geospark  version :   1.3 . 2 - SNAPSHOT   groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 3  version :   1.3 . 2 - SNAPSHOT   groupId :   org . datasyslab  artifactId :   geospark - viz  version :   1.3 . 2 - SNAPSHOT   In order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt", 
            "title": "SNAPSHOT versions"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#buildsbt", 
            "text": "resolvers +=\n  \"Sonatype OSS Snapshots\" at \" https://oss.sonatype.org/content/repositories/snapshots \"", 
            "title": "build.sbt"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#pomxml", 
            "text": "profiles \n     profile \n         id allow-snapshots /id \n         activation activeByDefault true /activeByDefault /activation \n         repositories \n             repository \n                 id snapshots-repo /id \n                 url https://oss.sonatype.org/content/repositories/snapshots /url \n                 releases enabled false /enabled /releases \n                 snapshots enabled true /enabled /snapshots \n             /repository \n         /repositories \n     /profile  /profiles", 
            "title": "POM.XML"
        }, 
        {
            "location": "/download/cluster/", 
            "text": "Set up your Apache Spark cluster\n\n\nDownload a Spark distribution from \nSpark download page\n.\n\n\nPreliminary\n\n\n\n\nSet up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.\n\n\nMake sure you have installed JRE 1.8 or later.\n\n\nAdd the list of your workers' IP address in ./conf/slaves\n\n\nBesides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid GeoSpark memory errors:\n\n\n\n\nIn \n./conf/spark-defaults.conf\n\n\nspark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n\n\n\n\n\n\nspark.driver.memory\n tells Spark to allocate enough memory for the driver program because GeoSpark needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.\n\n\nspark.network.timeout\n is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.\n\n\nspark.driver.maxResultSize\n is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.\n\n\n\n\nFor more details of Spark parameters, please visit \nSpark Website\n.\n\n\nStart your cluster\n\n\nGo the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal\n\n\n./sbin/start-all.sh", 
            "title": "Set up Spark cluser"
        }, 
        {
            "location": "/download/cluster/#set-up-your-apache-spark-cluster", 
            "text": "Download a Spark distribution from  Spark download page .", 
            "title": "Set up your Apache Spark cluster"
        }, 
        {
            "location": "/download/cluster/#preliminary", 
            "text": "Set up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.  Make sure you have installed JRE 1.8 or later.  Add the list of your workers' IP address in ./conf/slaves  Besides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid GeoSpark memory errors:   In  ./conf/spark-defaults.conf  spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g   spark.driver.memory  tells Spark to allocate enough memory for the driver program because GeoSpark needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.  spark.network.timeout  is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.  spark.driver.maxResultSize  is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.   For more details of Spark parameters, please visit  Spark Website .", 
            "title": "Preliminary"
        }, 
        {
            "location": "/download/cluster/#start-your-cluster", 
            "text": "Go the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal  ./sbin/start-all.sh", 
            "title": "Start your cluster"
        }, 
        {
            "location": "/download/scalashell/", 
            "text": "Spark Scala shell\n\n\nSpark distribution provides an interactive Scala shell that allows a user to execute Scala code in a terminal.\n\n\nThis mode currently works with GeoSpark-core and GeoSparkViz.\n\n\nDownload GeoSpark jar automatically\n\n\n\n\n\n\nHave your Spark cluster ready.\n\n\n\n\n\n\nRun Spark shell with \n--packages\n option. This command will automatically download GeoSpark jars from Maven Central.\n\n./bin/spark-shell --packages org.datasyslab:geospark:GEOSPARK_VERSION\n\n\n\n\n\n\n\nLocal mode: test GeoSpark without setting up a cluster\n\n./bin/spark-shell --packages org.datasyslab:geospark:1.2.0,org.datasyslab:geospark-sql_2.3:1.2.0,org.datasyslab:geospark-viz_2.3:1.2.0\n\n\n\n\n\n\n\nCluster mode: you need to specify Spark Master IP\n\n./bin/spark-shell --master spark://localhost:7077 --packages org.datasyslab:geospark:1.2.0,org.datasyslab:geospark-sql_2.3:1.2.0,org.datasyslab:geospark-viz_2.3:1.2.0\n\n\n\n\n\n\n\nDownload GeoSpark jar manually\n\n\n\n\n\n\nHave your Spark cluster ready.\n\n\n\n\n\n\nDownload GeoSpark jars:\n\n\n\n\nDownload the pre-compiled jars from \nGeoSpark Releases on GitHub\n\n\nDownload / Git clone GeoSpark source code and compile the code by yourself:\n\nmvn clean install -DskipTests\n\n\n\n\n\n\n\n\n\nRun Spark shell with \n--jars\n option.\n\n./bin/spark-shell --jars /Path/To/GeoSparkJars.jar\n\n\n\n\n\n\n\nLocal mode: test GeoSpark without setting up a cluster\n\n./bin/spark-shell --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar\n\n\n\n\n\n\n\nCluster mode: you need to specify Spark Master IP\n\n\n./bin/spark-shell --master spark://localhost:7077 --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar", 
            "title": "Spark Scala shell"
        }, 
        {
            "location": "/download/scalashell/#spark-scala-shell", 
            "text": "Spark distribution provides an interactive Scala shell that allows a user to execute Scala code in a terminal.  This mode currently works with GeoSpark-core and GeoSparkViz.", 
            "title": "Spark Scala shell"
        }, 
        {
            "location": "/download/scalashell/#download-geospark-jar-automatically", 
            "text": "Have your Spark cluster ready.    Run Spark shell with  --packages  option. This command will automatically download GeoSpark jars from Maven Central. ./bin/spark-shell --packages org.datasyslab:geospark:GEOSPARK_VERSION    Local mode: test GeoSpark without setting up a cluster ./bin/spark-shell --packages org.datasyslab:geospark:1.2.0,org.datasyslab:geospark-sql_2.3:1.2.0,org.datasyslab:geospark-viz_2.3:1.2.0    Cluster mode: you need to specify Spark Master IP ./bin/spark-shell --master spark://localhost:7077 --packages org.datasyslab:geospark:1.2.0,org.datasyslab:geospark-sql_2.3:1.2.0,org.datasyslab:geospark-viz_2.3:1.2.0", 
            "title": "Download GeoSpark jar automatically"
        }, 
        {
            "location": "/download/scalashell/#download-geospark-jar-manually", 
            "text": "Have your Spark cluster ready.    Download GeoSpark jars:   Download the pre-compiled jars from  GeoSpark Releases on GitHub  Download / Git clone GeoSpark source code and compile the code by yourself: mvn clean install -DskipTests     Run Spark shell with  --jars  option. ./bin/spark-shell --jars /Path/To/GeoSparkJars.jar    Local mode: test GeoSpark without setting up a cluster ./bin/spark-shell --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar    Cluster mode: you need to specify Spark Master IP  ./bin/spark-shell --master spark://localhost:7077 --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar", 
            "title": "Download GeoSpark jar manually"
        }, 
        {
            "location": "/download/project/", 
            "text": "Self-contained Spark projects\n\n\nA self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use GeoSpark in your self-contained Spark project, you just need to add GeoSpark as a dependency in your POM.xml or build.sbt.\n\n\nQuick start\n\n\n\n\nTo add GeoSpark as dependencies, please read \nGeoSpark Maven Central coordinates\n\n\nUse GeoSpark Template project to start: \nGeoSpark Template Project\n\n\nCompile your project using SBT or Maven. Make sure you obtain the fat jar which packages all dependencies.\n\n\nSubmit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command:\n\n./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n\n\n\n\n\n\n\nNote\n\n\nThe detailed explanation of spark-submit is available on \nSpark website\n.\n\n\n\n\nHow to use GeoSpark in an IDE\n\n\nSelect an IDE\n\n\nTo develop a complex GeoSpark project, we suggest you use IntelliJ IDEA. It supports JVM languages, Scala and Java, and many dependency management systems, Maven and SBT.\n\n\nEclipse is also fine if you just want to use Java and Maven.\n\n\nOpen GeoSpark template project\n\n\nSelect a proper GeoSpark project you want from \nGeoSpark Template Project\n. In this tutorial, we use GeoSparkSQL Scala project as an example.\n\n\nOpen the folder that contains \nbuild.sbt\n file in your IDE. The IDE may take a while to index dependencies and source code.\n\n\nTry GeoSpark SQL functions\n\n\nIn your IDE, run \nScalaExample.scala\n file.\n\n\nYou don't need to change anything in this file. The IDE will run all SQL queries in this example in local mode.\n\n\nPackage the project\n\n\nTo run this project in cluster mode, you have to package this project to a JAR and then run it using \nspark-submit\n command.\n\n\nBefore packaging this project, you always need to check two places:\n\n\n\n\n\n\nRemove the hardcoded Master IP \nmaster(\nlocal[*]\n)\n. This hardcoded IP is only needed when you run this project in an IDE.\n\nvar\n \nsparkSession\n:\nSparkSession\n \n=\n \nSparkSession\n.\nbuilder\n()\n\n    \n.\nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n    \n.\nconfig\n(\nspark.kryo.registrator\n,\nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n)\n\n    \n.\nmaster\n(\nlocal[*]\n)\n\n    \n.\nappName\n(\nGeoSparkSQL-demo\n).\ngetOrCreate\n()\n\n\n\n\n\n\n\n\nIn build.sbt (or POM.xml), set Spark dependency scope to \nprovided\n instead of \ncompile\n. \ncompile\n is only needed when you run this project in an IDE.\n\norg.apache.spark\n %% \nspark-core\n % SparkVersion % \ncompile,\norg.apache.spark\n %% \nspark-sql\n % SparkVersion % \ncompile\n\n\n\n\n\n\n\n\n\nWarning\n\n\nForgetting to change the package scope will lead to a very big fat JAR and dependency conflicts when call \nspark-submit\n. For more details, please visit \nMaven Dependency Scope\n.\n\n\n\n\n\n\nMake sure your downloaded Spark binary distribution is the same version with the Spark used in your \nbuild.sbt\n or \nPOM.xml\n.\n\n\n\n\nSubmit the compiled jar\n\n\n\n\nGo to \n./target/scala-2.11\n folder and find a jar called \nGeoSparkSQLScalaTemplate-0.1.0.jar\n. Note that, this JAR normally is larger than 1MB. (If you use POM.xml, the jar is under \n./target\n folder)\n\n\n\n\nSubmit this JAR using \nspark-submit\n.\n\n\n\n\n\n\nLocal mode:\n\n./bin/spark-submit /Path/To/YourJar.jar\n\n\n\n\n\n\n\nCluster mode:\n\n./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar", 
            "title": "Self-contained project"
        }, 
        {
            "location": "/download/project/#self-contained-spark-projects", 
            "text": "A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use GeoSpark in your self-contained Spark project, you just need to add GeoSpark as a dependency in your POM.xml or build.sbt.", 
            "title": "Self-contained Spark projects"
        }, 
        {
            "location": "/download/project/#quick-start", 
            "text": "To add GeoSpark as dependencies, please read  GeoSpark Maven Central coordinates  Use GeoSpark Template project to start:  GeoSpark Template Project  Compile your project using SBT or Maven. Make sure you obtain the fat jar which packages all dependencies.  Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command: ./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar    Note  The detailed explanation of spark-submit is available on  Spark website .", 
            "title": "Quick start"
        }, 
        {
            "location": "/download/project/#how-to-use-geospark-in-an-ide", 
            "text": "", 
            "title": "How to use GeoSpark in an IDE"
        }, 
        {
            "location": "/download/project/#select-an-ide", 
            "text": "To develop a complex GeoSpark project, we suggest you use IntelliJ IDEA. It supports JVM languages, Scala and Java, and many dependency management systems, Maven and SBT.  Eclipse is also fine if you just want to use Java and Maven.", 
            "title": "Select an IDE"
        }, 
        {
            "location": "/download/project/#open-geospark-template-project", 
            "text": "Select a proper GeoSpark project you want from  GeoSpark Template Project . In this tutorial, we use GeoSparkSQL Scala project as an example.  Open the folder that contains  build.sbt  file in your IDE. The IDE may take a while to index dependencies and source code.", 
            "title": "Open GeoSpark template project"
        }, 
        {
            "location": "/download/project/#try-geospark-sql-functions", 
            "text": "In your IDE, run  ScalaExample.scala  file.  You don't need to change anything in this file. The IDE will run all SQL queries in this example in local mode.", 
            "title": "Try GeoSpark SQL functions"
        }, 
        {
            "location": "/download/project/#package-the-project", 
            "text": "To run this project in cluster mode, you have to package this project to a JAR and then run it using  spark-submit  command.  Before packaging this project, you always need to check two places:    Remove the hardcoded Master IP  master( local[*] ) . This hardcoded IP is only needed when you run this project in an IDE. var   sparkSession : SparkSession   =   SparkSession . builder () \n     . config ( spark.serializer , classOf [ KryoSerializer ]. getName ) \n     . config ( spark.kryo.registrator , classOf [ GeoSparkKryoRegistrator ]. getName ) \n     . master ( local[*] ) \n     . appName ( GeoSparkSQL-demo ). getOrCreate ()     In build.sbt (or POM.xml), set Spark dependency scope to  provided  instead of  compile .  compile  is only needed when you run this project in an IDE. org.apache.spark  %%  spark-core  % SparkVersion %  compile,\norg.apache.spark  %%  spark-sql  % SparkVersion %  compile     Warning  Forgetting to change the package scope will lead to a very big fat JAR and dependency conflicts when call  spark-submit . For more details, please visit  Maven Dependency Scope .    Make sure your downloaded Spark binary distribution is the same version with the Spark used in your  build.sbt  or  POM.xml .", 
            "title": "Package the project"
        }, 
        {
            "location": "/download/project/#submit-the-compiled-jar", 
            "text": "Go to  ./target/scala-2.11  folder and find a jar called  GeoSparkSQLScalaTemplate-0.1.0.jar . Note that, this JAR normally is larger than 1MB. (If you use POM.xml, the jar is under  ./target  folder)   Submit this JAR using  spark-submit .    Local mode: ./bin/spark-submit /Path/To/YourJar.jar    Cluster mode: ./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar", 
            "title": "Submit the compiled jar"
        }, 
        {
            "location": "/download/zeppelin/", 
            "text": "Install GeoSpark-Zeppelin\n\n\n\n\nWarning\n\n\nKnown issue\n: due to an issue in Leaflet JS, GeoSpark-core can only plot each geometry (point, line string and polygon) as a point on Zeppelin map. To enjoy the scalable and full-fleged visualization, please use GeoSparkViz to plot scatter plots and heat maps on Zeppelin map.\n\n\n\n\nCompatibility\n\n\nApache Spark 2.1+\n\n\nApache Zeppelin 0.8.1+\n\n\nGeoSpark 1.2.0+: GeoSpark-core, GeoSpark-SQL, GeoSpark-Viz\n\n\nInstallation\n\n\n\n\nNote\n\n\nYou only need to do Step 1 and 2 only if you cannot see GeoSpark-Zeppelin in Zeppelin Helium package list.\n\n\n\n\nCreate Helium folder (optional)\n\n\nCreate a folder called \nhelium\n in Zeppelin root folder.\n\n\nAdd GeoSpark-Zeppelin description (optional)\n\n\nCreate a file called \ngeospark-zeppelin.json\n in this folder and put the following content in this file. You need to change the artifact path!\n\n\n{\n  \ntype\n: \nVISUALIZATION\n,\n  \nname\n: \ngeospark-zeppelin\n,\n  \ndescription\n: \nZeppelin visualization support for GeoSpark\n,\n  \nartifact\n: \n/Absolute/Path/GeoSpark/geospark-zeppelin\n,\n  \nlicense\n: \nBSD-2-Clause\n,\n  \nicon\n: \ni\n \nclass=\nfa fa-globe\n/i\n\n}\n\n\n\n\nEnable GeoSpark-Zeppelin\n\n\nRestart Zeppelin then open Zeppelin Helium interface and enable GeoSpark-Zeppelin.\n\n\n\n\nAdd GeoSpark dependencies in Zeppelin Spark Interpreter\n\n\n\n\nVisualize GeoSparkSQL results\n\n\n\n\nDisplay GeoSparkViz results\n\n\n\n\nNow, you are good to go! Please read \nGeoSpark-Zeppelin tutorial\n for a hands-on tutorial.", 
            "title": "Install GeoSpark-Zeppelin"
        }, 
        {
            "location": "/download/zeppelin/#install-geospark-zeppelin", 
            "text": "Warning  Known issue : due to an issue in Leaflet JS, GeoSpark-core can only plot each geometry (point, line string and polygon) as a point on Zeppelin map. To enjoy the scalable and full-fleged visualization, please use GeoSparkViz to plot scatter plots and heat maps on Zeppelin map.", 
            "title": "Install GeoSpark-Zeppelin"
        }, 
        {
            "location": "/download/zeppelin/#compatibility", 
            "text": "Apache Spark 2.1+  Apache Zeppelin 0.8.1+  GeoSpark 1.2.0+: GeoSpark-core, GeoSpark-SQL, GeoSpark-Viz", 
            "title": "Compatibility"
        }, 
        {
            "location": "/download/zeppelin/#installation", 
            "text": "Note  You only need to do Step 1 and 2 only if you cannot see GeoSpark-Zeppelin in Zeppelin Helium package list.", 
            "title": "Installation"
        }, 
        {
            "location": "/download/zeppelin/#create-helium-folder-optional", 
            "text": "Create a folder called  helium  in Zeppelin root folder.", 
            "title": "Create Helium folder (optional)"
        }, 
        {
            "location": "/download/zeppelin/#add-geospark-zeppelin-description-optional", 
            "text": "Create a file called  geospark-zeppelin.json  in this folder and put the following content in this file. You need to change the artifact path!  {\n   type :  VISUALIZATION ,\n   name :  geospark-zeppelin ,\n   description :  Zeppelin visualization support for GeoSpark ,\n   artifact :  /Absolute/Path/GeoSpark/geospark-zeppelin ,\n   license :  BSD-2-Clause ,\n   icon :  i   class= fa fa-globe /i \n}", 
            "title": "Add GeoSpark-Zeppelin description (optional)"
        }, 
        {
            "location": "/download/zeppelin/#enable-geospark-zeppelin", 
            "text": "Restart Zeppelin then open Zeppelin Helium interface and enable GeoSpark-Zeppelin.", 
            "title": "Enable GeoSpark-Zeppelin"
        }, 
        {
            "location": "/download/zeppelin/#add-geospark-dependencies-in-zeppelin-spark-interpreter", 
            "text": "", 
            "title": "Add GeoSpark dependencies in Zeppelin Spark Interpreter"
        }, 
        {
            "location": "/download/zeppelin/#visualize-geosparksql-results", 
            "text": "", 
            "title": "Visualize GeoSparkSQL results"
        }, 
        {
            "location": "/download/zeppelin/#display-geosparkviz-results", 
            "text": "Now, you are good to go! Please read  GeoSpark-Zeppelin tutorial  for a hands-on tutorial.", 
            "title": "Display GeoSparkViz results"
        }, 
        {
            "location": "/download/compile/", 
            "text": "Compile GeoSpark\n\n\nSome GeoSpark hackers may want to change some source code to fit in their own scenarios. To compile GeoSpark source code, you first need to download GeoSpark source code:\n\n\n\n\nDownload / Git clone GeoSpark source code from \nGeoSpark Github repo\n.\n\n\n\n\nCompile the source code\n\n\nGeoSpark is a a project with three modules, core, sql, and viz. Each module is a Scala/Java mixed project which is managed by Apache Maven 3. \n\n\n\n\nMake sure your machine has Java 1.8 and Apache Maven 3.\n\n\n\n\nTo compile all modules, please make sure you are in the root folder of three modules. Then enter the following command in the terminal:\n\n\nmvn clean install -DskipTests\n\n\nThis command will first delete the old binary files and compile all three modules. This compilation will skip the unit tests of GeoSpark.\n\n\nTo compile a module of GeoSpark, please make sure you are in the folder of that module. Then enter the same command.\n\n\nTo run unit tests, just simply remove \n-DskipTests\n option. The command is like this:\n\nmvn clean install\n\n\n\n\n\nWarning\n\n\nThe unit tests of all three modules may take up to 30 minutes. \n\n\n\n\nCompile the documentation\n\n\nThe source code of GeoSpark documentation website is written in Markdown and then compiled by MkDocs. The website is built upon \nMaterial for MkDocs template\n.\n\n\nIn GeoSpark repository, MkDocs configuration file \nmkdocs.yml\n is in the root folder and all documentation source code is in docs folder.\n\n\nTo compile the source code and test the website on your local machine, please read \nMkDocs Tutorial\n and \nMaterials for MkDocs Tutorial\n.\n\n\nAfter installing MkDocs and MkDocs-Material, run the command in GeoSpark root folder:\n\n\nmkdocs serve", 
            "title": "Compile the source code"
        }, 
        {
            "location": "/download/compile/#compile-geospark", 
            "text": "Some GeoSpark hackers may want to change some source code to fit in their own scenarios. To compile GeoSpark source code, you first need to download GeoSpark source code:   Download / Git clone GeoSpark source code from  GeoSpark Github repo .", 
            "title": "Compile GeoSpark"
        }, 
        {
            "location": "/download/compile/#compile-the-source-code", 
            "text": "GeoSpark is a a project with three modules, core, sql, and viz. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.    Make sure your machine has Java 1.8 and Apache Maven 3.   To compile all modules, please make sure you are in the root folder of three modules. Then enter the following command in the terminal:  mvn clean install -DskipTests \nThis command will first delete the old binary files and compile all three modules. This compilation will skip the unit tests of GeoSpark.  To compile a module of GeoSpark, please make sure you are in the folder of that module. Then enter the same command.  To run unit tests, just simply remove  -DskipTests  option. The command is like this: mvn clean install   Warning  The unit tests of all three modules may take up to 30 minutes.", 
            "title": "Compile the source code"
        }, 
        {
            "location": "/download/compile/#compile-the-documentation", 
            "text": "The source code of GeoSpark documentation website is written in Markdown and then compiled by MkDocs. The website is built upon  Material for MkDocs template .  In GeoSpark repository, MkDocs configuration file  mkdocs.yml  is in the root folder and all documentation source code is in docs folder.  To compile the source code and test the website on your local machine, please read  MkDocs Tutorial  and  Materials for MkDocs Tutorial .  After installing MkDocs and MkDocs-Material, run the command in GeoSpark root folder:  mkdocs serve", 
            "title": "Compile the documentation"
        }, 
        {
            "location": "/tutorial/rdd/", 
            "text": "The page outlines the steps to create Spatial RDDs and run spatial queries using GeoSpark-core. \nThe example code is written in Scala but also works for Java\n.\n\n\nSet up dependencies\n\n\n\n\nRead \nGeoSpark Maven Central coordinates\n\n\nSelect \nthe minimum dependencies\n: Add Apache Spark (only the Spark core) and GeoSpark (core).\n\n\nAdd the dependencies in build.sbt or pom.xml.\n\n\n\n\n\n\nNote\n\n\nTo enjoy the full functions of GeoSpark, we suggest you include \nthe full dependencies\n: \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n, \nGeoSparkViz\n\n\n\n\nInitiate SparkContext\n\n\nval\n \nconf\n \n=\n \nnew\n \nSparkConf\n()\n\n\nconf\n.\nsetAppName\n(\nGeoSparkRunnableExample\n)\n \n// Change this to a proper name\n\n\nconf\n.\nsetMaster\n(\nlocal[*]\n)\n \n// Delete this if run in cluster mode\n\n\n// Enable GeoSpark custom Kryo serializer\n\n\nconf\n.\nset\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\nconf\n.\nset\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n)\n\n\nval\n \nsc\n \n=\n \nnew\n \nSparkContext\n(\nconf\n)\n\n\n\n\n\n\n\nWarning\n\n\nGeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.\n\n\n\n\nIf you add \nthe GeoSpark full dependencies\n as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead:\n\nconf\n.\nset\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\nconf\n.\nset\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkVizKryoRegistrator\n].\ngetName\n)\n\n\n\n\nCreate a SpatialRDD\n\n\nCreate a typed SpatialRDD\n\n\nGeoSpark-core provides three special SpatialRDDs: \nPointRDD, PolygonRDD, and LineStringRDD\n. They can be loaded from CSV, TSV, WKT, WKB, Shapefiles, GeoJSON and NetCDF/HDF format.\n\n\nPointRDD from CSV/TSV\n\n\nSuppose we have a \ncheckin.csv\n CSV file at Path \n/Download/checkin.csv\n as follows:\n\n-88.331492,32.324142,hotel\n-88.175933,32.360763,gas\n-88.388954,32.357073,bar\n-88.221102,32.35078,restaurant\n\n\nThis file has three columns and corresponding \noffsets\n(Column IDs) are 0, 1, 2.\nUse the following code to create a PointRDD\n\n\nval\n \npointRDDInputLocation\n \n=\n \n/Download/checkin.csv\n\n\nval\n \npointRDDOffset\n \n=\n \n0\n \n// The point long/lat starts from Column 0\n\n\nval\n \npointRDDSplitter\n \n=\n \nFileDataSplitter\n.\nCSV\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 2 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n,\n \npointRDDInputLocation\n,\n \npointRDDOffset\n,\n \npointRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\n\n\nIf the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter:\n\nval\n \npointRDDSplitter\n \n=\n \nFileDataSplitter\n.\nTSV\n\n\n\n\nPolygonRDD/LineStringRDD from CSV/TSV\n\n\nIn general, polygon and line string data is stored in WKT, WKB, GeoJSON and Shapefile formats instead of CSV/TSV because the geometries in a file may have different lengths. However, if all polygons / line strings in your CSV/TSV possess the same length, you can create PolygonRDD and LineStringRDD from these files.\n\n\nSuppose we have a \ncheckinshape.csv\n CSV file at Path \n/Download/checkinshape.csv\n as follows:\n\n-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,hotel\n-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,gas\n-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,bar\n-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,restaurant\n\n\n\nThis file has 11 columns and corresponding offsets (Column IDs) are 0 - 10. Column 0 - 9 are 5 coordinates (longitude/latitude pairs). In this file, all geometries have the same number of coordinates. The geometries can be polyons or line strings.\n\n\n\n\nWarning\n\n\nFor polygon data, the last coordinate must be the same as the first coordinate because a polygon is a closed linear ring.\n\n\n\n\nUse the following code to create a PolygonRDD.\n\nval\n \npolygonRDDInputLocation\n \n=\n \n/Download/checkinshape.csv\n\n\nval\n \npolygonRDDStartOffset\n \n=\n \n0\n \n// The coordinates start from Column 0\n\n\nval\n \npolygonRDDEndOffset\n \n=\n \n9\n \n// The coordinates end at Column 9\n\n\nval\n \npolygonRDDSplitter\n \n=\n \nFileDataSplitter\n.\nCSV\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 10 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPolygonRDD\n(\nsc\n,\n \npolygonRDDInputLocation\n,\n \npolygonRDDStartOffset\n,\n \npolygonRDDEndOffset\n,\n \npolygonRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\n\nIf the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter:\n\nval\n \npolygonRDDSplitter\n \n=\n \nFileDataSplitter\n.\nTSV\n\n\n\n\nThe way to create a LineStringRDD is the same as PolygonRDD.\n\n\nCreate a generic SpatialRDD (behavoir changed in v1.2.0)\n\n\nA generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instance, a WKT file contains three types gemetries \nLineString\n, \nPolygon\n and \nMultiPolygon\n.\n\n\nFrom WKT/WKB\n\n\nGeometries in a WKT and WKB file always occupy a single column no matter how many coordinates they have. Therefore, creating a typed SpatialRDD is easy.\n\n\nSuppose we have a \ncheckin.tsv\n WKT TSV file at Path \n/Download/checkin.tsv\n as follows:\n\nPOINT (-88.331492 32.324142)    hotel\nPOINT (-88.175933 32.360763)    gas\nPOINT (-88.388954 32.357073)    bar\nPOINT (-88.221102 32.35078) restaurant\n\n\nThis file has two columns and corresponding \noffsets\n(Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.\n\n\nUse the following code to create a SpatialRDD\n\n\nval\n \ninputLocation\n \n=\n \n/Download/checkin.tsv\n\n\nval\n \nwktColumn\n \n=\n \n0\n \n// The WKT string starts from Column 0\n\n\nval\n \nallowTopologyInvalidGeometries\n \n=\n \ntrue\n \n// Optional\n\n\nval\n \nskipSyntaxInvalidGeometries\n \n=\n \nfalse\n \n// Optional\n\n\nval\n \nspatialRDD\n \n=\n \nWktReader\n.\nreadToGeometryRDD\n(\nsparkSession\n.\nsparkContext\n,\n \ninputLocation\n,\n \nwktColumn\n,\n \nallowTopologyInvalidGeometries\n,\n \nskipSyntaxInvalidGeometries\n)\n\n\n\n\n\nFrom GeoJSON\n\n\nGeometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.\n\n\nSuppose we have a \npolygon.json\n GeoJSON file at Path \n/Download/polygon.json\n as follows:\n\n\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n077\n, \nTRACTCE\n: \n011501\n, \nBLKGRPCE\n: \n5\n, \nAFFGEOID\n: \n1500000US010770115015\n, \nGEOID\n: \n010770115015\n, \nNAME\n: \n5\n, \nLSAD\n: \nBG\n, \nALAND\n: 6844991, \nAWATER\n: 32636 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n045\n, \nTRACTCE\n: \n021102\n, \nBLKGRPCE\n: \n4\n, \nAFFGEOID\n: \n1500000US010450211024\n, \nGEOID\n: \n010450211024\n, \nNAME\n: \n4\n, \nLSAD\n: \nBG\n, \nALAND\n: 11360854, \nAWATER\n: 0 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n055\n, \nTRACTCE\n: \n001300\n, \nBLKGRPCE\n: \n3\n, \nAFFGEOID\n: \n1500000US010550013003\n, \nGEOID\n: \n010550013003\n, \nNAME\n: \n3\n, \nLSAD\n: \nBG\n, \nALAND\n: 1378742, \nAWATER\n: 247387 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n089\n, \nTRACTCE\n: \n001700\n, \nBLKGRPCE\n: \n2\n, \nAFFGEOID\n: \n1500000US010890017002\n, \nGEOID\n: \n010890017002\n, \nNAME\n: \n2\n, \nLSAD\n: \nBG\n, \nALAND\n: 1040641, \nAWATER\n: 0 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },\n\n\n\n\nUse the following code to create a generic SpatialRDD:\n\nval\n \ninputLocation\n \n=\n \n/Download/polygon.json\n\n\nval\n \nallowTopologyInvalidGeometries\n \n=\n \ntrue\n \n// Optional\n\n\nval\n \nskipSyntaxInvalidGeometries\n \n=\n \nfalse\n \n// Optional\n\n\nval\n \nspatialRDD\n \n=\n \nGeoJsonReader\n.\nreadToGeometryRDD\n(\nsparkSession\n.\nsparkContext\n,\n \ninputLocation\n,\n \nallowTopologyInvalidGeometries\n,\n \nskipSyntaxInvalidGeometries\n)\n\n\n\n\n\n\nWarning\n\n\nThe way that GeoSpark reads JSON file is different from SparkSQL\n\n\n\n\nFrom Shapefile\n\n\nval\n \nshapefileInputLocation\n=\n/Download/myshapefile\n\n\nval\n \nspatialRDD\n \n=\n \nShapefileReader\n.\nreadToGeometryRDD\n(\nsparkSession\n.\nsparkContext\n,\n \nshapefileInputLocation\n)\n\n\n\n\n\n\n\nNote\n\n\nThe file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called \nmyShapefile\n, the file structure should be like this:\n\n- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n\n\n\n\n\nIf the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding\nvia \ngeospark.global.charset\n system property before the call to \nShapefileReader.readToGeometryRDD\n.\n\n\nExample:\n\n\nSystem\n.\nsetProperty\n(\ngeospark.global.charset\n,\n \nutf8\n)\n\n\n\n\n\nFrom SparkSQL DataFrame\n\n\nTo create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you can use GeoSparkSQL. Make sure you include \nthe full dependencies\n of GeoSpark. Read \nGeoSparkSQL API\n.\n\n\nWe use \ncheckin.csv CSV file\n as the example. You can create a generic SpatialRDD using the following steps:\n\n\n\n\nLoad data in GeoSparkSQL.\n\nvar\n \ndf\n \n=\n \nsparkSession\n.\nread\n.\nformat\n(\ncsv\n).\noption\n(\nheader\n,\n \nfalse\n).\nload\n(\ncsvPointInputLocation\n)\n\n\ndf\n.\ncreateOrReplaceTempView\n(\ninputtable\n)\n\n\n\n\nCreate a Geometry type column in GeoSparkSQL\n\nvar\n \nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n    \n\n\n        |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin\n\n\n        |FROM inputtable\n\n\n    \n.\nstripMargin\n)\n\n\n\n\nUse GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nAdapter\n.\ntoRdd\n(\nspatialDf\n)\n\n\n\n\n\n\nFor WKT/WKB/GeoJSON data, please use \nST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON\n instead.\n\n\nTransform the Coordinate Reference System\n\n\nGeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in GeoSpark is same as the unit of all geometries in an SpatialRDD.\n\n\nTo convert Coordinate Reference System of an SpatialRDD, use the following code:\n\n\nval\n \nsourceCrsCode\n \n=\n \nepsg:4326\n \n// WGS84, the most common degree-based CRS\n\n\nval\n \ntargetCrsCode\n \n=\n \nepsg:3857\n \n// The most common meter-based CRS\n\n\nobjectRDD\n.\nCRSTransform\n(\nsourceCrsCode\n,\n \ntargetCrsCode\n)\n\n\n\n\n\n\n\nWarning\n\n\nCRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instace, use something like this:\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n,\n \npointRDDInputLocation\n,\n \npointRDDOffset\n,\n \npointRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\nobjectRDD\n.\nCRSTransform\n(\nepsg:4326\n,\n \nepsg:3857\n)\n\n\n\n\n\n\nThe details CRS information can be found on \nEPSG.io\n\n\nRead other attributes in an SpatialRDD\n\n\nEach SpatialRDD can carry non-spatial attributes such as price, age and name as long as the user sets \ncarryOtherAttributes\n as \nTRUE\n.\n\n\nThe other attributes are combined together to a string and stored in \nUserData\n field of each geometry.\n\n\nTo retrieve the UserData field, use the following code:\n\nval\n \nrddWithOtherAttributes\n \n=\n \nobjectRDD\n.\nrawSpatialRDD\n.\nrdd\n.\nmap\n[\nString\n](\nf\n=\nf\n.\ngetUserData\n.\nasInstanceOf\n[\nString\n])\n\n\n\n\nWrite a Spatial Range Query\n\n\nA spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that intersect / are fully covered by the query window.\n\n\nAssume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial Range Query on it.\n\n\nval\n \nrangeQueryWindow\n \n=\n \nnew\n \nEnvelope\n(-\n90.01\n,\n \n-\n80.01\n,\n \n30.01\n,\n \n40.01\n)\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by the window\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\nvar\n \nqueryResult\n \n=\n \nRangeQuery\n.\nSpatialRangeQuery\n(\nspatialRDD\n,\n \nrangeQueryWindow\n,\n \nconsiderBoundaryIntersection\n,\n \nusingIndex\n)\n\n\n\n\n\nconsiderBoundaryIntersection\n can be set to TRUE to return all geometries intersect with query window.\n\n\n\n\nNote\n\n\nSpatial range query is equal to \nST_Within\n and \nST_Intersects\n in Spatial SQL. An example query is as follows:\n\nSELECT\n \n*\n\n\nFROM\n \ncheckin\n\n\nWHERE\n \nST_Intersects\n(\nqueryWindow\n,\n \ncheckin\n.\nlocation\n)\n\n\n\n\n\n\nRange query window\n\n\nBesides the rectangle (Envelope) type range query window, GeoSpark range query window can be Point/Polygon/LineString.\n\n\nThe code to create a point is as follows:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \npointObject\n \n=\n \ngeometryFactory\n.\ncreatePoint\n(\nnew\n \nCoordinate\n(-\n84.01\n,\n \n34.01\n))\n\n\n\n\n\nThe code to create a polygon (with 4 vertexes) is as follows:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \ncoordinates\n \n=\n \nnew\n \nArray\n[\nCoordinate\n](\n5\n)\n\n\ncoordinates\n(\n0\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n0\n)\n\n\ncoordinates\n(\n1\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n4\n)\n\n\ncoordinates\n(\n2\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n4\n)\n\n\ncoordinates\n(\n3\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n0\n)\n\n\ncoordinates\n(\n4\n)\n \n=\n \ncoordinates\n(\n0\n)\n \n// The last coordinate is the same as the first coordinate in order to compose a closed ring\n\n\nval\n \npolygonObject\n \n=\n \ngeometryFactory\n.\ncreatePolygon\n(\ncoordinates\n)\n\n\n\n\n\nThe code to create a line string (with 4 vertexes) is as follows:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \ncoordinates\n \n=\n \nnew\n \nArray\n[\nCoordinate\n](\n4\n)\n\n\ncoordinates\n(\n0\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n0\n)\n\n\ncoordinates\n(\n1\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n4\n)\n\n\ncoordinates\n(\n2\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n4\n)\n\n\ncoordinates\n(\n3\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n0\n)\n\n\nval\n \nlinestringObject\n \n=\n \ngeometryFactory\n.\ncreateLineString\n(\ncoordinates\n)\n\n\n\n\n\nUse spatial indexes\n\n\nGeoSpark provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, GeoSpark will build a local tree index on each of the SpatialRDD partition.\n\n\nTo utilize a spatial index in a spatial range query, use the following code:\n\n\nval\n \nrangeQueryWindow\n \n=\n \nnew\n \nEnvelope\n(-\n90.01\n,\n \n-\n80.01\n,\n \n30.01\n,\n \n40.01\n)\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by the window\n\n\n\nval\n \nbuildOnSpatialPartitionedRDD\n \n=\n \nfalse\n \n// Set to TRUE only if run join query\n\n\nspatialRDD\n.\nbuildIndex\n(\nIndexType\n.\nQUADTREE\n,\n \nbuildOnSpatialPartitionedRDD\n)\n\n\n\nval\n \nusingIndex\n \n=\n \ntrue\n\n\nvar\n \nqueryResult\n \n=\n \nRangeQuery\n.\nSpatialRangeQuery\n(\nspatialRDD\n,\n \nrangeQueryWindow\n,\n \nconsiderBoundaryIntersection\n,\n \nusingIndex\n)\n\n\n\n\n\n\n\nTip\n\n\nUsing an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.\n\n\n\n\nOutput format\n\n\nThe output format of the spatial range query is another SpatialRDD.\n\n\nWrite a Spatial KNN Query\n\n\nA spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.\n\n\nAssume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \npointObject\n \n=\n \ngeometryFactory\n.\ncreatePoint\n(\nnew\n \nCoordinate\n(-\n84.01\n,\n \n34.01\n))\n\n\nval\n \nK\n \n=\n \n1000\n \n// K Nearest Neighbors\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\nval\n \nresult\n \n=\n \nKNNQuery\n.\nSpatialKnnQuery\n(\nobjectRDD\n,\n \npointObject\n,\n \nK\n,\n \nusingIndex\n)\n\n\n\n\n\n\n\nNote\n\n\nSpatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL\n\nSELECT\n \nck\n.\nname\n,\n \nck\n.\nrating\n,\n \nST_Distance\n(\nck\n.\nlocation\n,\n \nmyLocation\n)\n \nAS\n \ndistance\n\n\nFROM\n \ncheckins\n \nck\n\n\nORDER\n \nBY\n \ndistance\n \nDESC\n\n\nLIMIT\n \n5\n\n\n\n\n\n\nQuery center geometry\n\n\nBesides the Point type, GeoSpark KNN query center can be Polygon and LineString.\n\n\nTo learn how to create Polygon and LineString object, see \nRange query window\n.\n\n\nUse spatial indexes\n\n\nTo utilize a spatial index in a spatial KNN query, use the following code:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \npointObject\n \n=\n \ngeometryFactory\n.\ncreatePoint\n(\nnew\n \nCoordinate\n(-\n84.01\n,\n \n34.01\n))\n\n\nval\n \nK\n \n=\n \n1000\n \n// K Nearest Neighbors\n\n\n\n\nval\n \nbuildOnSpatialPartitionedRDD\n \n=\n \nfalse\n \n// Set to TRUE only if run join query\n\n\nobjectRDD\n.\nbuildIndex\n(\nIndexType\n.\nRTREE\n,\n \nbuildOnSpatialPartitionedRDD\n)\n\n\n\nval\n \nusingIndex\n \n=\n \ntrue\n\n\nval\n \nresult\n \n=\n \nKNNQuery\n.\nSpatialKnnQuery\n(\nobjectRDD\n,\n \npointObject\n,\n \nK\n,\n \nusingIndex\n)\n\n\n\n\n\n\n\nWarning\n\n\nOnly R-Tree index supports Spatial KNN query\n\n\n\n\nOutput format\n\n\nThe output format of the spatial KNN query is a list of geometries. The list has K geometry objects.\n\n\nWrite a Spatial Join Query\n\n\nA spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.\n\n\nAssume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by each query window in queryWindowRDD\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\n\nobjectRDD\n.\nanalyze\n()\n\n\n\nobjectRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nobjectRDD\n.\ngetPartitioner\n)\n\n\n\nval\n \nresult\n \n=\n \nJoinQuery\n.\nSpatialJoinQuery\n(\nobjectRDD\n,\n \nqueryWindowRDD\n,\n \nusingIndex\n,\n \nconsiderBoundaryIntersection\n)\n\n\n\n\n\n\n\nNote\n\n\nSpatial join query is equal to the following query in Spatial SQL:\n\nSELECT\n \nsuperhero\n.\nname\n\n\nFROM\n \ncity\n,\n \nsuperhero\n\n\nWHERE\n \nST_Contains\n(\ncity\n.\ngeom\n,\n \nsuperhero\n.\ngeom\n);\n\n\n\nFind the super heros in each city\n\n\n\n\nUse spatial partitioning\n\n\nGeoSpark spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.\n\n\nIf you first partition SpatialRDD A, then you must use the partitioner of A to partition B.\n\n\nobjectRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nobjectRDD\n.\ngetPartitioner\n)\n\n\n\n\n\nOr \n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nobjectRDD\n.\nspatialPartitioning\n(\nqueryWindowRDD\n.\ngetPartitioner\n)\n\n\n\n\n\nUse spatial indexes\n\n\nTo utilize a spatial index in a spatial join query, use the following code:\n\n\nobjectRDD\n.\nspatialPartitioning\n(\njoinQueryPartitioningType\n)\n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nobjectRDD\n.\ngetPartitioner\n)\n\n\n\nval\n \nbuildOnSpatialPartitionedRDD\n \n=\n \ntrue\n \n// Set to TRUE only if run join query\n\n\nval\n \nusingIndex\n \n=\n \ntrue\n\n\nqueryWindowRDD\n.\nbuildIndex\n(\nIndexType\n.\nQUADTREE\n,\n \nbuildOnSpatialPartitionedRDD\n)\n\n\n\nval\n \nresult\n \n=\n \nJoinQuery\n.\nSpatialJoinQueryFlat\n(\nobjectRDD\n,\n \nqueryWindowRDD\n,\n \nusingIndex\n,\n \nconsiderBoundaryIntersection\n)\n\n\n\n\n\nThe index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.\n\n\nOutput format\n\n\nThe output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.\n\n\nPoint,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n\n\n\n\nEach object on the left is covered/intersected by the object on the right.\n\n\nWrite a Distance Join Query\n\n\nA distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained \nhere\n.\n\n\nAssume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Distance Join Query on them.\n\n\nobjectRddA\n.\nanalyze\n()\n\n\n\nval\n \ncircleRDD\n \n=\n \nnew\n \nCircleRDD\n(\nobjectRddA\n,\n \n0.1\n)\n \n// Create a CircleRDD using the given distance\n\n\n\ncircleRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nobjectRddB\n.\nspatialPartitioning\n(\ncircleRDD\n.\ngetPartitioner\n)\n\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by each query window in queryWindowRDD\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\n\nval\n \nresult\n \n=\n \nJoinQuery\n.\nDistanceJoinQueryFlat\n(\nobjectRddB\n,\n \ncircleRDD\n,\n \nusingIndex\n,\n \nconsiderBoundaryIntersection\n)\n\n\n\n\n\nThe rest part of the join query is same as the spatial join query.\n\n\nThe details of spatial partitioning in join query is \nhere\n.\n\n\nThe details of using spatial indexes in join query is \nhere\n.\n\n\nThe output format of the distance join query is \nhere\n.\n\n\n\n\nNote\n\n\nDistance join query is equal to the following query in Spatial SQL:\n\nSELECT\n \nsuperhero\n.\nname\n\n\nFROM\n \ncity\n,\n \nsuperhero\n\n\nWHERE\n \nST_Distance\n(\ncity\n.\ngeom\n,\n \nsuperhero\n.\ngeom\n)\n \n=\n \n10\n;\n\n\n\nFind the super heros within 10 miles of each city\n\n\n\n\nSave to permanent storage\n\n\nYou can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.\n\n\n\n\nNote\n\n\nNon-spatial attributes such as price, age and name will also be stored to permanent storage.\n\n\n\n\nSave an SpatialRDD (not indexed)\n\n\nTyped SpatialRDD and generic SpatialRDD can be saved to permanent storage.\n\n\nSave to distributed WKT text file\n\n\nUse the following code to save an SpatialRDD as a distributed WKT text file:\n\n\nobjectRDD\n.\nrawSpatialRDD\n.\nsaveAsTextFile\n(\nhdfs://PATH\n)\n\n\nobjectRDD\n.\nsaveAsWKT\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed WKB text file\n\n\nUse the following code to save an SpatialRDD as a distributed WKB text file:\n\n\nobjectRDD\n.\nsaveAsWKB\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed GeoJSON text file\n\n\nUse the following code to save an SpatialRDD as a distributed GeoJSON text file:\n\n\nobjectRDD\n.\nsaveAsGeoJSON\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed object file\n\n\nUse the following code to save an SpatialRDD as a distributed object file:\n\n\nobjectRDD\n.\nrawSpatialRDD\n.\nsaveAsObjectFile\n(\nhdfs://PATH\n)\n\n\n\n\n\n\n\nNote\n\n\nEach object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.\n\n\n\n\nSave an SpatialRDD (indexed)\n\n\nIndexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.\n\n\nSave to distributed object file\n\n\nUse the following code to save an SpatialRDD as a distributed object file:\n\n\nobjectRDD.indexedRawRDD.saveAsObjectFile(\nhdfs://PATH\n)\n\n\n\n\nSave an SpatialRDD (spatialPartitioned W/O indexed)\n\n\nA spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!\n\n\nReload a saved SpatialRDD\n\n\nYou can easily reload an SpatialRDD that has been saved to \na distributed object file\n.\n\n\nLoad to a typed SpatialRDD\n\n\nUse the following code to reload the PointRDD/PolygonRDD/LineStringRDD:\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n.\nobjectFile\n[\nPoint\n](\nhdfs://PATH\n))\n\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n.\nobjectFile\n[\nPolygon\n](\nhdfs://PATH\n))\n\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n.\nobjectFile\n[\nLineString\n](\nhdfs://PATH\n))\n\n\n\n\n\nLoad to a generic SpatialRDD\n\n\nUse the following code to reload the SpatialRDD:\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nsavedRDD\n.\nrawSpatialRDD\n \n=\n \nsc\n.\nobjectFile\n[\nGeometry\n](\nhdfs://PATH\n)\n\n\n\n\n\nUse the following code to reload the indexed SpatialRDD:\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nsavedRDD\n.\nindexedRawRDD\n \n=\n \nsc\n.\nobjectFile\n[\nSpatialIndex\n](\nhdfs://PATH\n)", 
            "title": "Spatial RDD application"
        }, 
        {
            "location": "/tutorial/rdd/#set-up-dependencies", 
            "text": "Read  GeoSpark Maven Central coordinates  Select  the minimum dependencies : Add Apache Spark (only the Spark core) and GeoSpark (core).  Add the dependencies in build.sbt or pom.xml.    Note  To enjoy the full functions of GeoSpark, we suggest you include  the full dependencies :  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL ,  GeoSparkViz", 
            "title": "Set up dependencies"
        }, 
        {
            "location": "/tutorial/rdd/#initiate-sparkcontext", 
            "text": "val   conf   =   new   SparkConf ()  conf . setAppName ( GeoSparkRunnableExample )   // Change this to a proper name  conf . setMaster ( local[*] )   // Delete this if run in cluster mode  // Enable GeoSpark custom Kryo serializer  conf . set ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  conf . set ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName )  val   sc   =   new   SparkContext ( conf )    Warning  GeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.   If you add  the GeoSpark full dependencies  as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead: conf . set ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  conf . set ( spark.kryo.registrator ,   classOf [ GeoSparkVizKryoRegistrator ]. getName )", 
            "title": "Initiate SparkContext"
        }, 
        {
            "location": "/tutorial/rdd/#create-a-spatialrdd", 
            "text": "", 
            "title": "Create a SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#create-a-typed-spatialrdd", 
            "text": "GeoSpark-core provides three special SpatialRDDs:  PointRDD, PolygonRDD, and LineStringRDD . They can be loaded from CSV, TSV, WKT, WKB, Shapefiles, GeoJSON and NetCDF/HDF format.", 
            "title": "Create a typed SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#pointrdd-from-csvtsv", 
            "text": "Suppose we have a  checkin.csv  CSV file at Path  /Download/checkin.csv  as follows: -88.331492,32.324142,hotel\n-88.175933,32.360763,gas\n-88.388954,32.357073,bar\n-88.221102,32.35078,restaurant \nThis file has three columns and corresponding  offsets (Column IDs) are 0, 1, 2.\nUse the following code to create a PointRDD  val   pointRDDInputLocation   =   /Download/checkin.csv  val   pointRDDOffset   =   0   // The point long/lat starts from Column 0  val   pointRDDSplitter   =   FileDataSplitter . CSV  val   carryOtherAttributes   =   true   // Carry Column 2 (hotel, gas, bar...)  var   objectRDD   =   new   PointRDD ( sc ,   pointRDDInputLocation ,   pointRDDOffset ,   pointRDDSplitter ,   carryOtherAttributes )   If the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter: val   pointRDDSplitter   =   FileDataSplitter . TSV", 
            "title": "PointRDD from CSV/TSV"
        }, 
        {
            "location": "/tutorial/rdd/#polygonrddlinestringrdd-from-csvtsv", 
            "text": "In general, polygon and line string data is stored in WKT, WKB, GeoJSON and Shapefile formats instead of CSV/TSV because the geometries in a file may have different lengths. However, if all polygons / line strings in your CSV/TSV possess the same length, you can create PolygonRDD and LineStringRDD from these files.  Suppose we have a  checkinshape.csv  CSV file at Path  /Download/checkinshape.csv  as follows: -88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,hotel\n-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,gas\n-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,bar\n-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,restaurant  This file has 11 columns and corresponding offsets (Column IDs) are 0 - 10. Column 0 - 9 are 5 coordinates (longitude/latitude pairs). In this file, all geometries have the same number of coordinates. The geometries can be polyons or line strings.   Warning  For polygon data, the last coordinate must be the same as the first coordinate because a polygon is a closed linear ring.   Use the following code to create a PolygonRDD. val   polygonRDDInputLocation   =   /Download/checkinshape.csv  val   polygonRDDStartOffset   =   0   // The coordinates start from Column 0  val   polygonRDDEndOffset   =   9   // The coordinates end at Column 9  val   polygonRDDSplitter   =   FileDataSplitter . CSV  val   carryOtherAttributes   =   true   // Carry Column 10 (hotel, gas, bar...)  var   objectRDD   =   new   PolygonRDD ( sc ,   polygonRDDInputLocation ,   polygonRDDStartOffset ,   polygonRDDEndOffset ,   polygonRDDSplitter ,   carryOtherAttributes )   If the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter: val   polygonRDDSplitter   =   FileDataSplitter . TSV   The way to create a LineStringRDD is the same as PolygonRDD.", 
            "title": "PolygonRDD/LineStringRDD from CSV/TSV"
        }, 
        {
            "location": "/tutorial/rdd/#create-a-generic-spatialrdd-behavoir-changed-in-v120", 
            "text": "A generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instance, a WKT file contains three types gemetries  LineString ,  Polygon  and  MultiPolygon .", 
            "title": "Create a generic SpatialRDD (behavoir changed in v1.2.0)"
        }, 
        {
            "location": "/tutorial/rdd/#from-wktwkb", 
            "text": "Geometries in a WKT and WKB file always occupy a single column no matter how many coordinates they have. Therefore, creating a typed SpatialRDD is easy.  Suppose we have a  checkin.tsv  WKT TSV file at Path  /Download/checkin.tsv  as follows: POINT (-88.331492 32.324142)    hotel\nPOINT (-88.175933 32.360763)    gas\nPOINT (-88.388954 32.357073)    bar\nPOINT (-88.221102 32.35078) restaurant \nThis file has two columns and corresponding  offsets (Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.  Use the following code to create a SpatialRDD  val   inputLocation   =   /Download/checkin.tsv  val   wktColumn   =   0   // The WKT string starts from Column 0  val   allowTopologyInvalidGeometries   =   true   // Optional  val   skipSyntaxInvalidGeometries   =   false   // Optional  val   spatialRDD   =   WktReader . readToGeometryRDD ( sparkSession . sparkContext ,   inputLocation ,   wktColumn ,   allowTopologyInvalidGeometries ,   skipSyntaxInvalidGeometries )", 
            "title": "From WKT/WKB"
        }, 
        {
            "location": "/tutorial/rdd/#from-geojson", 
            "text": "Geometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.  Suppose we have a  polygon.json  GeoJSON file at Path  /Download/polygon.json  as follows:  {  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  077 ,  TRACTCE :  011501 ,  BLKGRPCE :  5 ,  AFFGEOID :  1500000US010770115015 ,  GEOID :  010770115015 ,  NAME :  5 ,  LSAD :  BG ,  ALAND : 6844991,  AWATER : 32636 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  045 ,  TRACTCE :  021102 ,  BLKGRPCE :  4 ,  AFFGEOID :  1500000US010450211024 ,  GEOID :  010450211024 ,  NAME :  4 ,  LSAD :  BG ,  ALAND : 11360854,  AWATER : 0 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  055 ,  TRACTCE :  001300 ,  BLKGRPCE :  3 ,  AFFGEOID :  1500000US010550013003 ,  GEOID :  010550013003 ,  NAME :  3 ,  LSAD :  BG ,  ALAND : 1378742,  AWATER : 247387 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  089 ,  TRACTCE :  001700 ,  BLKGRPCE :  2 ,  AFFGEOID :  1500000US010890017002 ,  GEOID :  010890017002 ,  NAME :  2 ,  LSAD :  BG ,  ALAND : 1040641,  AWATER : 0 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },  Use the following code to create a generic SpatialRDD: val   inputLocation   =   /Download/polygon.json  val   allowTopologyInvalidGeometries   =   true   // Optional  val   skipSyntaxInvalidGeometries   =   false   // Optional  val   spatialRDD   =   GeoJsonReader . readToGeometryRDD ( sparkSession . sparkContext ,   inputLocation ,   allowTopologyInvalidGeometries ,   skipSyntaxInvalidGeometries )    Warning  The way that GeoSpark reads JSON file is different from SparkSQL", 
            "title": "From GeoJSON"
        }, 
        {
            "location": "/tutorial/rdd/#from-shapefile", 
            "text": "val   shapefileInputLocation = /Download/myshapefile  val   spatialRDD   =   ShapefileReader . readToGeometryRDD ( sparkSession . sparkContext ,   shapefileInputLocation )    Note  The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called  myShapefile , the file structure should be like this: - shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...   If the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding\nvia  geospark.global.charset  system property before the call to  ShapefileReader.readToGeometryRDD .  Example:  System . setProperty ( geospark.global.charset ,   utf8 )", 
            "title": "From Shapefile"
        }, 
        {
            "location": "/tutorial/rdd/#from-sparksql-dataframe", 
            "text": "To create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you can use GeoSparkSQL. Make sure you include  the full dependencies  of GeoSpark. Read  GeoSparkSQL API .  We use  checkin.csv CSV file  as the example. You can create a generic SpatialRDD using the following steps:   Load data in GeoSparkSQL. var   df   =   sparkSession . read . format ( csv ). option ( header ,   false ). load ( csvPointInputLocation )  df . createOrReplaceTempView ( inputtable )   Create a Geometry type column in GeoSparkSQL var   spatialDf   =   sparkSession . sql ( \n              |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin          |FROM inputtable       . stripMargin )   Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   Adapter . toRdd ( spatialDf )    For WKT/WKB/GeoJSON data, please use  ST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON  instead.", 
            "title": "From SparkSQL DataFrame"
        }, 
        {
            "location": "/tutorial/rdd/#transform-the-coordinate-reference-system", 
            "text": "GeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in GeoSpark is same as the unit of all geometries in an SpatialRDD.  To convert Coordinate Reference System of an SpatialRDD, use the following code:  val   sourceCrsCode   =   epsg:4326   // WGS84, the most common degree-based CRS  val   targetCrsCode   =   epsg:3857   // The most common meter-based CRS  objectRDD . CRSTransform ( sourceCrsCode ,   targetCrsCode )    Warning  CRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instace, use something like this: var   objectRDD   =   new   PointRDD ( sc ,   pointRDDInputLocation ,   pointRDDOffset ,   pointRDDSplitter ,   carryOtherAttributes )  objectRDD . CRSTransform ( epsg:4326 ,   epsg:3857 )    The details CRS information can be found on  EPSG.io", 
            "title": "Transform the Coordinate Reference System"
        }, 
        {
            "location": "/tutorial/rdd/#read-other-attributes-in-an-spatialrdd", 
            "text": "Each SpatialRDD can carry non-spatial attributes such as price, age and name as long as the user sets  carryOtherAttributes  as  TRUE .  The other attributes are combined together to a string and stored in  UserData  field of each geometry.  To retrieve the UserData field, use the following code: val   rddWithOtherAttributes   =   objectRDD . rawSpatialRDD . rdd . map [ String ]( f = f . getUserData . asInstanceOf [ String ])", 
            "title": "Read other attributes in an SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-spatial-range-query", 
            "text": "A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that intersect / are fully covered by the query window.  Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial Range Query on it.  val   rangeQueryWindow   =   new   Envelope (- 90.01 ,   - 80.01 ,   30.01 ,   40.01 )  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by the window  val   usingIndex   =   false  var   queryResult   =   RangeQuery . SpatialRangeQuery ( spatialRDD ,   rangeQueryWindow ,   considerBoundaryIntersection ,   usingIndex )   considerBoundaryIntersection  can be set to TRUE to return all geometries intersect with query window.   Note  Spatial range query is equal to  ST_Within  and  ST_Intersects  in Spatial SQL. An example query is as follows: SELECT   *  FROM   checkin  WHERE   ST_Intersects ( queryWindow ,   checkin . location )", 
            "title": "Write a Spatial Range Query"
        }, 
        {
            "location": "/tutorial/rdd/#range-query-window", 
            "text": "Besides the rectangle (Envelope) type range query window, GeoSpark range query window can be Point/Polygon/LineString.  The code to create a point is as follows:  val   geometryFactory   =   new   GeometryFactory ()  val   pointObject   =   geometryFactory . createPoint ( new   Coordinate (- 84.01 ,   34.01 ))   The code to create a polygon (with 4 vertexes) is as follows:  val   geometryFactory   =   new   GeometryFactory ()  val   coordinates   =   new   Array [ Coordinate ]( 5 )  coordinates ( 0 )   =   new   Coordinate ( 0 , 0 )  coordinates ( 1 )   =   new   Coordinate ( 0 , 4 )  coordinates ( 2 )   =   new   Coordinate ( 4 , 4 )  coordinates ( 3 )   =   new   Coordinate ( 4 , 0 )  coordinates ( 4 )   =   coordinates ( 0 )   // The last coordinate is the same as the first coordinate in order to compose a closed ring  val   polygonObject   =   geometryFactory . createPolygon ( coordinates )   The code to create a line string (with 4 vertexes) is as follows:  val   geometryFactory   =   new   GeometryFactory ()  val   coordinates   =   new   Array [ Coordinate ]( 4 )  coordinates ( 0 )   =   new   Coordinate ( 0 , 0 )  coordinates ( 1 )   =   new   Coordinate ( 0 , 4 )  coordinates ( 2 )   =   new   Coordinate ( 4 , 4 )  coordinates ( 3 )   =   new   Coordinate ( 4 , 0 )  val   linestringObject   =   geometryFactory . createLineString ( coordinates )", 
            "title": "Range query window"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-indexes", 
            "text": "GeoSpark provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, GeoSpark will build a local tree index on each of the SpatialRDD partition.  To utilize a spatial index in a spatial range query, use the following code:  val   rangeQueryWindow   =   new   Envelope (- 90.01 ,   - 80.01 ,   30.01 ,   40.01 )  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by the window  val   buildOnSpatialPartitionedRDD   =   false   // Set to TRUE only if run join query  spatialRDD . buildIndex ( IndexType . QUADTREE ,   buildOnSpatialPartitionedRDD )  val   usingIndex   =   true  var   queryResult   =   RangeQuery . SpatialRangeQuery ( spatialRDD ,   rangeQueryWindow ,   considerBoundaryIntersection ,   usingIndex )    Tip  Using an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/rdd/#output-format", 
            "text": "The output format of the spatial range query is another SpatialRDD.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-spatial-knn-query", 
            "text": "A spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.  Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.  val   geometryFactory   =   new   GeometryFactory ()  val   pointObject   =   geometryFactory . createPoint ( new   Coordinate (- 84.01 ,   34.01 ))  val   K   =   1000   // K Nearest Neighbors  val   usingIndex   =   false  val   result   =   KNNQuery . SpatialKnnQuery ( objectRDD ,   pointObject ,   K ,   usingIndex )    Note  Spatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL SELECT   ck . name ,   ck . rating ,   ST_Distance ( ck . location ,   myLocation )   AS   distance  FROM   checkins   ck  ORDER   BY   distance   DESC  LIMIT   5", 
            "title": "Write a Spatial KNN Query"
        }, 
        {
            "location": "/tutorial/rdd/#query-center-geometry", 
            "text": "Besides the Point type, GeoSpark KNN query center can be Polygon and LineString.  To learn how to create Polygon and LineString object, see  Range query window .", 
            "title": "Query center geometry"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-indexes_1", 
            "text": "To utilize a spatial index in a spatial KNN query, use the following code:  val   geometryFactory   =   new   GeometryFactory ()  val   pointObject   =   geometryFactory . createPoint ( new   Coordinate (- 84.01 ,   34.01 ))  val   K   =   1000   // K Nearest Neighbors  val   buildOnSpatialPartitionedRDD   =   false   // Set to TRUE only if run join query  objectRDD . buildIndex ( IndexType . RTREE ,   buildOnSpatialPartitionedRDD )  val   usingIndex   =   true  val   result   =   KNNQuery . SpatialKnnQuery ( objectRDD ,   pointObject ,   K ,   usingIndex )    Warning  Only R-Tree index supports Spatial KNN query", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/rdd/#output-format_1", 
            "text": "The output format of the spatial KNN query is a list of geometries. The list has K geometry objects.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-spatial-join-query", 
            "text": "A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.  Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by each query window in queryWindowRDD  val   usingIndex   =   false  objectRDD . analyze ()  objectRDD . spatialPartitioning ( GridType . KDBTREE )  queryWindowRDD . spatialPartitioning ( objectRDD . getPartitioner )  val   result   =   JoinQuery . SpatialJoinQuery ( objectRDD ,   queryWindowRDD ,   usingIndex ,   considerBoundaryIntersection )    Note  Spatial join query is equal to the following query in Spatial SQL: SELECT   superhero . name  FROM   city ,   superhero  WHERE   ST_Contains ( city . geom ,   superhero . geom );  \nFind the super heros in each city", 
            "title": "Write a Spatial Join Query"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-partitioning", 
            "text": "GeoSpark spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.  If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.  objectRDD . spatialPartitioning ( GridType . KDBTREE )  queryWindowRDD . spatialPartitioning ( objectRDD . getPartitioner )   Or   queryWindowRDD . spatialPartitioning ( GridType . KDBTREE )  objectRDD . spatialPartitioning ( queryWindowRDD . getPartitioner )", 
            "title": "Use spatial partitioning"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-indexes_2", 
            "text": "To utilize a spatial index in a spatial join query, use the following code:  objectRDD . spatialPartitioning ( joinQueryPartitioningType )  queryWindowRDD . spatialPartitioning ( objectRDD . getPartitioner )  val   buildOnSpatialPartitionedRDD   =   true   // Set to TRUE only if run join query  val   usingIndex   =   true  queryWindowRDD . buildIndex ( IndexType . QUADTREE ,   buildOnSpatialPartitionedRDD )  val   result   =   JoinQuery . SpatialJoinQueryFlat ( objectRDD ,   queryWindowRDD ,   usingIndex ,   considerBoundaryIntersection )   The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/rdd/#output-format_2", 
            "text": "The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.  Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...  Each object on the left is covered/intersected by the object on the right.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-distance-join-query", 
            "text": "A distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained  here .  Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Distance Join Query on them.  objectRddA . analyze ()  val   circleRDD   =   new   CircleRDD ( objectRddA ,   0.1 )   // Create a CircleRDD using the given distance  circleRDD . spatialPartitioning ( GridType . KDBTREE )  objectRddB . spatialPartitioning ( circleRDD . getPartitioner )  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by each query window in queryWindowRDD  val   usingIndex   =   false  val   result   =   JoinQuery . DistanceJoinQueryFlat ( objectRddB ,   circleRDD ,   usingIndex ,   considerBoundaryIntersection )   The rest part of the join query is same as the spatial join query.  The details of spatial partitioning in join query is  here .  The details of using spatial indexes in join query is  here .  The output format of the distance join query is  here .   Note  Distance join query is equal to the following query in Spatial SQL: SELECT   superhero . name  FROM   city ,   superhero  WHERE   ST_Distance ( city . geom ,   superhero . geom )   =   10 ;  \nFind the super heros within 10 miles of each city", 
            "title": "Write a Distance Join Query"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-permanent-storage", 
            "text": "You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.   Note  Non-spatial attributes such as price, age and name will also be stored to permanent storage.", 
            "title": "Save to permanent storage"
        }, 
        {
            "location": "/tutorial/rdd/#save-an-spatialrdd-not-indexed", 
            "text": "Typed SpatialRDD and generic SpatialRDD can be saved to permanent storage.", 
            "title": "Save an SpatialRDD (not indexed)"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-wkt-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed WKT text file:  objectRDD . rawSpatialRDD . saveAsTextFile ( hdfs://PATH )  objectRDD . saveAsWKT ( hdfs://PATH )", 
            "title": "Save to distributed WKT text file"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-wkb-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed WKB text file:  objectRDD . saveAsWKB ( hdfs://PATH )", 
            "title": "Save to distributed WKB text file"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-geojson-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed GeoJSON text file:  objectRDD . saveAsGeoJSON ( hdfs://PATH )", 
            "title": "Save to distributed GeoJSON text file"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-object-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed object file:  objectRDD . rawSpatialRDD . saveAsObjectFile ( hdfs://PATH )    Note  Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.", 
            "title": "Save to distributed object file"
        }, 
        {
            "location": "/tutorial/rdd/#save-an-spatialrdd-indexed", 
            "text": "Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.", 
            "title": "Save an SpatialRDD (indexed)"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-object-file_1", 
            "text": "Use the following code to save an SpatialRDD as a distributed object file:  objectRDD.indexedRawRDD.saveAsObjectFile( hdfs://PATH )", 
            "title": "Save to distributed object file"
        }, 
        {
            "location": "/tutorial/rdd/#save-an-spatialrdd-spatialpartitioned-wo-indexed", 
            "text": "A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!", 
            "title": "Save an SpatialRDD (spatialPartitioned W/O indexed)"
        }, 
        {
            "location": "/tutorial/rdd/#reload-a-saved-spatialrdd", 
            "text": "You can easily reload an SpatialRDD that has been saved to  a distributed object file .", 
            "title": "Reload a saved SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#load-to-a-typed-spatialrdd", 
            "text": "Use the following code to reload the PointRDD/PolygonRDD/LineStringRDD:  var   savedRDD   =   new   PointRDD ( sc . objectFile [ Point ]( hdfs://PATH ))  var   savedRDD   =   new   PointRDD ( sc . objectFile [ Polygon ]( hdfs://PATH ))  var   savedRDD   =   new   PointRDD ( sc . objectFile [ LineString ]( hdfs://PATH ))", 
            "title": "Load to a typed SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#load-to-a-generic-spatialrdd", 
            "text": "Use the following code to reload the SpatialRDD:  var   savedRDD   =   new   SpatialRDD [ Geometry ]  savedRDD . rawSpatialRDD   =   sc . objectFile [ Geometry ]( hdfs://PATH )   Use the following code to reload the indexed SpatialRDD: var   savedRDD   =   new   SpatialRDD [ Geometry ]  savedRDD . indexedRawRDD   =   sc . objectFile [ SpatialIndex ]( hdfs://PATH )", 
            "title": "Load to a generic SpatialRDD"
        }, 
        {
            "location": "/tutorial/sql/", 
            "text": "The page outlines the steps to manage spatial data using GeoSparkSQL. \nThe example code is written in Scala but also works for Java\n.\n\n\nGeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:\n\nvar\n \nmyDataFrame\n \n=\n \nsparkSession\n.\nsql\n(\nYOUR_SQL\n)\n\n\n\n\nDetailed GeoSparkSQL APIs are available here: \nGeoSparkSQL API\n\n\nSet up dependencies\n\n\n\n\nRead \nGeoSpark Maven Central coordinates\n\n\nSelect \nthe minimum dependencies\n: Add \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n\n\nAdd the dependencies in build.sbt or pom.xml.\n\n\n\n\n\n\nNote\n\n\nTo enjoy the full functions of GeoSpark, we suggest you include \nthe full dependencies\n: \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n, \nGeoSparkViz\n\n\n\n\nInitiate SparkSession\n\n\nUse the following code to initiate your SparkSession at the beginning:\n\nvar\n \nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n()\n\n\n.\nmaster\n(\nlocal[*]\n)\n \n// Delete this if run in cluster mode\n\n\n.\nappName\n(\nreadTestScala\n)\n \n// Change this to a proper name\n\n\n// Enable GeoSpark custom Kryo serializer\n\n\n.\nconfig\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\n.\nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n)\n\n\n.\ngetOrCreate\n()\n\n\n\n\n\n\nWarning\n\n\nGeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.\n\n\n\n\nIf you add \nthe GeoSpark full dependencies\n as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead:\n\n.\nconfig\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\n.\nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkVizKryoRegistrator\n].\ngetName\n)\n\n\n\n\nRegister GeoSparkSQL\n\n\nAdd the following line after your SparkSession declaration\n\n\nGeoSparkSQLRegistrator\n.\nregisterAll\n(\nsparkSession\n)\n\n\n\n\n\nThis function will register GeoSpark User Defined Type, User Defined Function and optimized join query strategy.\n\n\nLoad data from files\n\n\nAssume we have a WKT file, namely \nusa-county.tsv\n, at Path \n/Download/usa-county.tsv\n as follows:\n\n\nPOLYGON (..., ...)  Cuming County   \nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County\n\n\nThe file may have many other columns.\n\n\nUse the following code to load the data and create a raw DataFrame:\n\n\nvar\n \nrawDf\n \n=\n \nsparkSession\n.\nread\n.\nformat\n(\ncsv\n).\noption\n(\ndelimiter\n,\n \n\\t\n).\noption\n(\nheader\n,\n \nfalse\n).\nload\n(\n/Download/usa-county.tsv\n)\n\n\nrawDf\n.\ncreateOrReplaceTempView\n(\nrawdf\n)\n\n\nrawDf\n.\nshow\n()\n\n\n\n\n\nThe output will be like this:\n\n\n|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n\n\n\n\nCreate a Geometry type column\n\n\nAll geometrical operations in GeoSparkSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.\n\n\nvar\n \nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2\n\n\n    |FROM rawdf\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\nYou can select many other attributes to compose this \nspatialdDf\n. The output will be something like this:\n\n\n|                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n\n\n\n\nAlthough it looks same with the input, but actually the type of column countyshape has been changed to \nGeometry\n type.\n\n\nTo verify this, use the following code to print the schema of the DataFrame:\n\n\nspatialDf\n.\nprintSchema\n()\n\n\n\n\n\nThe output will be like this:\n\n\nroot\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n\n\n\n\n\n\nNote\n\n\nGeoSparkSQL provides more than 10 different functions to create a Geometry column, please read \nGeoSparkSQL constructor API\n.\n\n\n\n\nLoad Shapefile and GeoJSON\n\n\nShapefile and GeoJSON must be loaded by SpatialRDD and converted to DataFrame using Adapter. Please read \nLoad SpatialRDD\n and \nDataFrame \n-\n RDD\n.\n\n\nTransform the Coordinate Reference System\n\n\nGeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in GeoSparkSQL is same as the unit of all geometries in a Geometry column.\n\n\nTo convert Coordinate Reference System of the Geometry column created before, use the following code:\n\n\nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT ST_Transform(countyshape, \nepsg:4326\n, \nepsg:3857\n) AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7\n\n\n    |FROM spatialdf\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\nThe first EPSG code EPSG:4326 in \nST_Transform\n is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.\n\n\nThe second EPSG code EPSG:3857 in \nST_Transform\n is the target CRS of the geometries. It is the most common meter-based CRS.\n\n\nThis \nST_Transform\n transform the CRS of these geomtries from EPSG:4326 to EPSG:3857. The details CRS information can be found on \nEPSG.io\n\n\nThe coordinates of polygons have been changed. The output will be like this:\n\n\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|\n\n\n\n\nRun spatial queries\n\n\nAfter creating a Geometry type column, you are able to run spatial queries.\n\n\nRange query\n\n\nUse \nST_Contains\n, \nST_Intersects\n, \nST_Within\n to run a range query over a single column.\n\n\nThe following example finds all counties that are within the given polygon:\n\n\nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT *\n\n\n    |FROM spatialdf\n\n\n    |WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\n\n\nNote\n\n\nRead \nGeoSparkSQL constructor API\n to learn how to create a Geometry type query window\n\n\n\n\nKNN query\n\n\nUse \nST_Distance\n to calculate the distance and rank the distance.\n\n\nThe following code returns the 5 nearest neighbor of the given polygon.\n\n\nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\n\n\n    |FROM spatialdf\n\n\n    |ORDER BY distance DESC\n\n\n    |LIMIT 5\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\nJoin query\n\n\nThe details of a join query is available here \nJoin query\n.\n\n\nOther queries\n\n\nThere are lots of other functions can be combined with these queries. Please read \nGeoSparkSQL functions\n and \nGeoSparkSQL aggregate functions\n.\n\n\nSave to permanent storage\n\n\nTo save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.\n\n\nUse the following code to convert the Geometry column in a DataFrame back to a WKT string column:\n\nsparkSession\n.\nudf\n.\nregister\n(\nST_SaveAsWKT\n,\n \n(\ngeometry\n:\n \nGeometry\n)\n \n=\n \n(\ngeometry\n.\ntoText\n))\n\n\nvar\n \nstringDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT ST_SaveAsWKT(countyshape)\n\n\n    |FROM polygondf\n\n\n  \n.\nstripMargin\n)\n\n\n\n\n\n\nNote\n\n\nWe are working on providing more user-friendly output functions such as \nST_SaveAsWKT\n and \nST_SaveAsWKB\n. Stay tuned!\n\n\n\n\nTo load the DataFrame back, you first use the regular method to load the saved string DataFrame from the permanent storage and use \nST_GeomFromWKT\n to re-build the Geometry type column.\n\n\nConvert between DataFrame and SpatialRDD\n\n\nDataFrame to SpatialRDD\n\n\nUse GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD\n\n\nGeoSpark 1.2.0+\n\n\nvar\n \nspatialRDD\n \n=\n \nAdapter\n.\ntoSpatialRdd\n(\nspatialDf\n,\n \nusacounty\n)\n\n\n\n\n\"usacounty\" is the name of the geometry column\n\n\nBefore GeoSpark 1.2.0\n\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nAdapter\n.\ntoRdd\n(\nspatialDf\n)\n\n\n\n\nGeometry must be the first column in the DataFrame\n\n\n\n\nWarning\n\n\nOnly one Geometry type column is allowed per DataFrame.\n\n\n\n\n\n\nNote\n\n\nBefore GeoSpark 1.2.0, other non-spatial columns need be brought to SpatialRDD using the UUIDs. Please read \nGeoSparkSQL constructor API\n. In GeoSpark 1.2.0+, all other non-spatial columns are automatically kept in SpatialRDD.\n\n\n\n\nSpatialRDD to DataFrame\n\n\nUse GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD\n\nvar\n \nspatialDf\n \n=\n \nAdapter\n.\ntoDf\n(\nspatialRDD\n,\n \nsparkSession\n)\n\n\n\n\nAll other attributes such as price and age will be also brought to the DataFrame as long as you specify \ncarryOtherAttributes\n (see \nRead other attributes in an SpatialRDD\n).\n\n\nSpatialPairRDD to DataFrame\n\n\nPairRDD is the result of a spatial join query or distance join query. GeoSparkSQL DataFrame-RDD Adapter can convert the result to a DataFrame:\n\n\nvar\n \njoinResultDf\n \n=\n \nAdapter\n.\ntoDf\n(\njoinResultPairRDD\n,\n \nsparkSession\n)\n\n\n\n\n\nAll other attributes such as price and age will be also brought to the DataFrame as long as you specify \ncarryOtherAttributes\n (see \nRead other attributes in an SpatialRDD\n).", 
            "title": "Spatial SQL application"
        }, 
        {
            "location": "/tutorial/sql/#set-up-dependencies", 
            "text": "Read  GeoSpark Maven Central coordinates  Select  the minimum dependencies : Add  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL  Add the dependencies in build.sbt or pom.xml.    Note  To enjoy the full functions of GeoSpark, we suggest you include  the full dependencies :  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL ,  GeoSparkViz", 
            "title": "Set up dependencies"
        }, 
        {
            "location": "/tutorial/sql/#initiate-sparksession", 
            "text": "Use the following code to initiate your SparkSession at the beginning: var   sparkSession   =   SparkSession . builder ()  . master ( local[*] )   // Delete this if run in cluster mode  . appName ( readTestScala )   // Change this to a proper name  // Enable GeoSpark custom Kryo serializer  . config ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  . config ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName )  . getOrCreate ()    Warning  GeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.   If you add  the GeoSpark full dependencies  as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead: . config ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  . config ( spark.kryo.registrator ,   classOf [ GeoSparkVizKryoRegistrator ]. getName )", 
            "title": "Initiate SparkSession"
        }, 
        {
            "location": "/tutorial/sql/#register-geosparksql", 
            "text": "Add the following line after your SparkSession declaration  GeoSparkSQLRegistrator . registerAll ( sparkSession )   This function will register GeoSpark User Defined Type, User Defined Function and optimized join query strategy.", 
            "title": "Register GeoSparkSQL"
        }, 
        {
            "location": "/tutorial/sql/#load-data-from-files", 
            "text": "Assume we have a WKT file, namely  usa-county.tsv , at Path  /Download/usa-county.tsv  as follows:  POLYGON (..., ...)  Cuming County   \nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County \nThe file may have many other columns.  Use the following code to load the data and create a raw DataFrame:  var   rawDf   =   sparkSession . read . format ( csv ). option ( delimiter ,   \\t ). option ( header ,   false ). load ( /Download/usa-county.tsv )  rawDf . createOrReplaceTempView ( rawdf )  rawDf . show ()   The output will be like this:  |                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|", 
            "title": "Load data from files"
        }, 
        {
            "location": "/tutorial/sql/#create-a-geometry-type-column", 
            "text": "All geometrical operations in GeoSparkSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.  var   spatialDf   =   sparkSession . sql ( \n        |SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2      |FROM rawdf     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()   You can select many other attributes to compose this  spatialdDf . The output will be something like this:  |                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|  Although it looks same with the input, but actually the type of column countyshape has been changed to  Geometry  type.  To verify this, use the following code to print the schema of the DataFrame:  spatialDf . printSchema ()   The output will be like this:  root\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)   Note  GeoSparkSQL provides more than 10 different functions to create a Geometry column, please read  GeoSparkSQL constructor API .", 
            "title": "Create a Geometry type column"
        }, 
        {
            "location": "/tutorial/sql/#load-shapefile-and-geojson", 
            "text": "Shapefile and GeoJSON must be loaded by SpatialRDD and converted to DataFrame using Adapter. Please read  Load SpatialRDD  and  DataFrame  -  RDD .", 
            "title": "Load Shapefile and GeoJSON"
        }, 
        {
            "location": "/tutorial/sql/#transform-the-coordinate-reference-system", 
            "text": "GeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in GeoSparkSQL is same as the unit of all geometries in a Geometry column.  To convert Coordinate Reference System of the Geometry column created before, use the following code:  spatialDf   =   sparkSession . sql ( \n        |SELECT ST_Transform(countyshape,  epsg:4326 ,  epsg:3857 ) AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7      |FROM spatialdf     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()   The first EPSG code EPSG:4326 in  ST_Transform  is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.  The second EPSG code EPSG:3857 in  ST_Transform  is the target CRS of the geometries. It is the most common meter-based CRS.  This  ST_Transform  transform the CRS of these geomtries from EPSG:4326 to EPSG:3857. The details CRS information can be found on  EPSG.io  The coordinates of polygons have been changed. The output will be like this:  +--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|", 
            "title": "Transform the Coordinate Reference System"
        }, 
        {
            "location": "/tutorial/sql/#run-spatial-queries", 
            "text": "After creating a Geometry type column, you are able to run spatial queries.", 
            "title": "Run spatial queries"
        }, 
        {
            "location": "/tutorial/sql/#range-query", 
            "text": "Use  ST_Contains ,  ST_Intersects ,  ST_Within  to run a range query over a single column.  The following example finds all counties that are within the given polygon:  spatialDf   =   sparkSession . sql ( \n        |SELECT *      |FROM spatialdf      |WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()    Note  Read  GeoSparkSQL constructor API  to learn how to create a Geometry type query window", 
            "title": "Range query"
        }, 
        {
            "location": "/tutorial/sql/#knn-query", 
            "text": "Use  ST_Distance  to calculate the distance and rank the distance.  The following code returns the 5 nearest neighbor of the given polygon.  spatialDf   =   sparkSession . sql ( \n        |SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance      |FROM spatialdf      |ORDER BY distance DESC      |LIMIT 5     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()", 
            "title": "KNN query"
        }, 
        {
            "location": "/tutorial/sql/#join-query", 
            "text": "The details of a join query is available here  Join query .", 
            "title": "Join query"
        }, 
        {
            "location": "/tutorial/sql/#other-queries", 
            "text": "There are lots of other functions can be combined with these queries. Please read  GeoSparkSQL functions  and  GeoSparkSQL aggregate functions .", 
            "title": "Other queries"
        }, 
        {
            "location": "/tutorial/sql/#save-to-permanent-storage", 
            "text": "To save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.  Use the following code to convert the Geometry column in a DataFrame back to a WKT string column: sparkSession . udf . register ( ST_SaveAsWKT ,   ( geometry :   Geometry )   =   ( geometry . toText ))  var   stringDf   =   sparkSession . sql ( \n        |SELECT ST_SaveAsWKT(countyshape)      |FROM polygondf     . stripMargin )    Note  We are working on providing more user-friendly output functions such as  ST_SaveAsWKT  and  ST_SaveAsWKB . Stay tuned!   To load the DataFrame back, you first use the regular method to load the saved string DataFrame from the permanent storage and use  ST_GeomFromWKT  to re-build the Geometry type column.", 
            "title": "Save to permanent storage"
        }, 
        {
            "location": "/tutorial/sql/#convert-between-dataframe-and-spatialrdd", 
            "text": "", 
            "title": "Convert between DataFrame and SpatialRDD"
        }, 
        {
            "location": "/tutorial/sql/#dataframe-to-spatialrdd", 
            "text": "Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD  GeoSpark 1.2.0+  var   spatialRDD   =   Adapter . toSpatialRdd ( spatialDf ,   usacounty )   \"usacounty\" is the name of the geometry column  Before GeoSpark 1.2.0  var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   Adapter . toRdd ( spatialDf )   Geometry must be the first column in the DataFrame   Warning  Only one Geometry type column is allowed per DataFrame.    Note  Before GeoSpark 1.2.0, other non-spatial columns need be brought to SpatialRDD using the UUIDs. Please read  GeoSparkSQL constructor API . In GeoSpark 1.2.0+, all other non-spatial columns are automatically kept in SpatialRDD.", 
            "title": "DataFrame to SpatialRDD"
        }, 
        {
            "location": "/tutorial/sql/#spatialrdd-to-dataframe", 
            "text": "Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD var   spatialDf   =   Adapter . toDf ( spatialRDD ,   sparkSession )   All other attributes such as price and age will be also brought to the DataFrame as long as you specify  carryOtherAttributes  (see  Read other attributes in an SpatialRDD ).", 
            "title": "SpatialRDD to DataFrame"
        }, 
        {
            "location": "/tutorial/sql/#spatialpairrdd-to-dataframe", 
            "text": "PairRDD is the result of a spatial join query or distance join query. GeoSparkSQL DataFrame-RDD Adapter can convert the result to a DataFrame:  var   joinResultDf   =   Adapter . toDf ( joinResultPairRDD ,   sparkSession )   All other attributes such as price and age will be also brought to the DataFrame as long as you specify  carryOtherAttributes  (see  Read other attributes in an SpatialRDD ).", 
            "title": "SpatialPairRDD to DataFrame"
        }, 
        {
            "location": "/tutorial/viz/", 
            "text": "The page outlines the steps to visualize spatial data using GeoSparkViz. \nThe example code is written in Scala but also works for Java\n.\n\n\nStarting from 1.2.0, GeoSparkViz provides the DataFrame support. This offers users a more flexible way to design beautiful map visualization effects including scatter plots and heat maps. In the meantime, GeoSparkViz RDD API remains the same.\n\n\n\n\nNote\n\n\nAll GeoSparkViz SQL/DataFrame APIs are explained in \nGeoSparkViz API\n.\n\n\n\n\nWhy scalable map visualization?\n\n\nData visualization allows users to summarize, analyze and reason about data. Guaranteeing detailed and accurate geospatial map visualization (e.g., at multiple zoom levels) requires extremely high-resolution maps. Classic visualization solutions such as Google Maps, MapBox and ArcGIS suffer from limited computation resources and hence take a tremendous amount of time to generate maps for large-scale geospatial data. In big spatial data scenarios, these tools just crash or run forever.\n\n\nGeoSparkViz encapsulates the main steps of map visualization process, e.g., pixelize, aggregate, and render, into a set of massively parallelized GeoViz operators and the user can assemble any customized styles.\n\n\nVisualize SpatialRDD\n\n\nThis tutorial mainly focuses on explaining SQL/DataFrame API. GeoSparkViz RDD example can be found in \nGeoSpark template project\n.\n\n\nSet up dependencies\n\n\n\n\nRead \nGeoSpark Maven Central coordinates\n\n\nAdd \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n, \nGeoSparkViz\n\n\n\n\nInitiate SparkSession\n\n\nUse the following code to initiate your SparkSession at the beginning:\nThis will register GeoSparkVizKryo serializer.\n\n\nvar\n \nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n()\n\n\n.\nmaster\n(\nlocal[*]\n)\n \n// Delete this if run in cluster mode\n\n\n.\nappName\n(\nreadTestScala\n)\n \n// Change this to a proper name\n\n\n// Enable GeoSpark custom Kryo serializer\n\n\n.\nconfig\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\n.\nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkVizKryoRegistrator\n].\ngetName\n)\n\n\n.\ngetOrCreate\n()\n\n\n\n\n\nRegister GeoSparkSQL and GeoSparkViz\n\n\nAdd the following line after your SparkSession declaration\n\n\nGeoSparkSQLRegistrator\n.\nregisterAll\n(\nsparkSession\n)\n\n\nGeoSparkVizRegistrator\n.\nregisterAll\n(\nsparkSession\n)\n\n\n\n\n\nThis will register all User Defined Tyeps, functions and optimizations in GeoSparkSQL and GeoSparkViz.\n\n\nCreate Spatial DataFrame\n\n\nThere is a DataFrame as follows:\n\n\n+----------+---------+\n|       _c0|      _c1|\n+----------+---------+\n|-88.331492|32.324142|\n|-88.175933|32.360763|\n|-88.388954|32.357073|\n|-88.221102| 32.35078|\n\n\n\n\nYou first need to create a Geometry type column.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \npointtable\n \nAS\n\n\nSELECT\n \nST_Point\n(\ncast\n(\npointtable\n.\n_c0\n \nas\n \nDecimal\n(\n24\n,\n20\n)),\ncast\n(\npointtable\n.\n_c1\n \nas\n \nDecimal\n(\n24\n,\n20\n)))\n \nas\n \nshape\n\n\nFROM\n \npointtable\n\n\n\n\n\nAs you know, GeoSpark provides many different methods to load various spatial data formats. Please read \nWrite an Spatial DataFrame application\n.\n\n\nGenerate a single image\n\n\nIn most cases, you just want to see a single image out of your spatial dataset.\n\n\nPixelize spatial objects\n\n\nTo put spatial objects on a map image, you first need to convert them to pixels.\n\n\nFirst, compute the spatial boundary of this column.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \nboundtable\n \nAS\n\n\nSELECT\n \nST_Envelope_Aggr\n(\nshape\n)\n \nas\n \nbound\n \nFROM\n \npointtable\n\n\n\n\n\nThen use ST_Pixelize to conver them to pixels.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \npixels\n \nAS\n\n\nSELECT\n \npixel\n,\n \nshape\n \nFROM\n \npointtable\n\n\nLATERAL\n \nVIEW\n \nST_Pixelize\n(\nST_Transform\n(\nshape\n,\n \nepsg:4326\n,\nepsg:3857\n),\n \n256\n,\n \n256\n,\n \n(\nSELECT\n \nST_Transform\n(\nbound\n,\n \nepsg:4326\n,\nepsg:3857\n)\n \nFROM\n \nboundtable\n))\n \nAS\n \npixel\n\n\n\n\n\nThis will give you a 256*256 resolution image after you run ST_Render at the end of this tutorial.\n\n\n\n\nWarning\n\n\nWe highly suggest that you should use ST_Transform to transfrom coordiantes to a visualization-specific coordinate sysmte such as epsg:3857. Otherwise you map may look distorted.\n\n\n\n\nAggregate pixels\n\n\nMany objects may be pixelized to the same pixel locations. You now need to aggregate them based on either their spatial aggregation or spatial observations such as temperature or humidity.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \npixelaggregates\n \nAS\n\n\nSELECT\n \npixel\n,\n \ncount\n(\n*\n)\n \nas\n \nweight\n\n\nFROM\n \npixels\n\n\nGROUP\n \nBY\n \npixel\n\n\n\n\n\nThe weight indicates the degree of spatial aggregation or spatial observations. Later on, it will determine the color of this pixel.\n\n\nColorize pixels\n\n\nRun the following command to assign colors for pixels based on their weights.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \npixelaggregates\n \nAS\n\n\nSELECT\n \npixel\n,\n \nST_Colorize\n(\nweight\n,\n \n(\nSELECT\n \nmax\n(\nweight\n)\n \nFROM\n \npixelaggregates\n))\n \nas\n \ncolor\n\n\nFROM\n \npixelaggregates\n\n\n\n\n\nPlease read \nST_Colorize\n for a detailed API description.\n\n\nRender the image\n\n\nUse ST_Render to plot all pixels on a single image.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \nimages\n \nAS\n\n\nSELECT\n \nST_Render\n(\npixel\n,\n \ncolor\n)\n \nAS\n \nimage\n,\n \n(\nSELECT\n \nST_AsText\n(\nbound\n)\n \nFROM\n \nboundtable\n)\n \nAS\n \nboundary\n\n\nFROM\n \npixelaggregates\n\n\n\n\n\nThis DataFrame will contain a Image type column which has only one image.\n\n\nStore the image on disk\n\n\nFetch the image from the previous DataFrame\n\n\nvar image = sparkSession.table(\nimages\n).take(1)(0)(0).asInstanceOf[ImageSerializableWrapper].getImage\n\n\n\n\nUse GeoSparkViz ImageGenerator to store this image on disk.\n\n\nvar\n \nimageGenerator\n \n=\n \nnew\n \nImageGenerator\n\n\nimageGenerator\n.\nSaveRasterImageAsLocalFile\n(\nimage\n,\n \nSystem\n.\ngetProperty\n(\nuser.dir\n)+\n/target/points\n,\n \nImageType\n.\nPNG\n)\n\n\n\n\n\nGenerate map tiles\n\n\nIf you are a map tile professional, you may need to generate map tiles for different zoom levels and eventually create the map tile layer.\n\n\nPixelization and pixel aggregation\n\n\nPlease first do pixelization and pixel aggregation using the same commands in single image generation. In ST_Pixelize, you need specify a very high resolution.\n\n\nCreate tile name\n\n\nRun the following command to compute the tile name for every pixels\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \npixelaggregates\n \nAS\n\n\nSELECT\n \npixel\n,\n \nweight\n,\n \nST_TileName\n(\npixel\n,\n \n3\n)\n \nAS\n \npid\n\n\nFROM\n \npixelaggregates\n\n\n\n\n\n\"3\" is the zoom level for these map tiles.\n\n\nColorize pixels\n\n\nUse the same command explained in single image generation to assign colors.\n\n\nRender map tiles\n\n\nYou now need to group pixels by tiles and then render map tile images in parallel.\n\n\nCREATE\n \nOR\n \nREPLACE\n \nTEMP\n \nVIEW\n \nimages\n \nAS\n\n\nSELECT\n \nST_Render\n(\npixel\n,\n \ncolor\n)\n \nAS\n \nimage\n\n\nFROM\n \npixelaggregates\n\n\nGROUP\n \nBY\n \npid\n\n\n\n\n\nStore map tiles on disk\n\n\nYou can use the same commands in single image generation to fetch all map tiles and store them one by one.", 
            "title": "Visualize Spatial DataFrame"
        }, 
        {
            "location": "/tutorial/viz/#why-scalable-map-visualization", 
            "text": "Data visualization allows users to summarize, analyze and reason about data. Guaranteeing detailed and accurate geospatial map visualization (e.g., at multiple zoom levels) requires extremely high-resolution maps. Classic visualization solutions such as Google Maps, MapBox and ArcGIS suffer from limited computation resources and hence take a tremendous amount of time to generate maps for large-scale geospatial data. In big spatial data scenarios, these tools just crash or run forever.  GeoSparkViz encapsulates the main steps of map visualization process, e.g., pixelize, aggregate, and render, into a set of massively parallelized GeoViz operators and the user can assemble any customized styles.", 
            "title": "Why scalable map visualization?"
        }, 
        {
            "location": "/tutorial/viz/#visualize-spatialrdd", 
            "text": "This tutorial mainly focuses on explaining SQL/DataFrame API. GeoSparkViz RDD example can be found in  GeoSpark template project .", 
            "title": "Visualize SpatialRDD"
        }, 
        {
            "location": "/tutorial/viz/#set-up-dependencies", 
            "text": "Read  GeoSpark Maven Central coordinates  Add  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL ,  GeoSparkViz", 
            "title": "Set up dependencies"
        }, 
        {
            "location": "/tutorial/viz/#initiate-sparksession", 
            "text": "Use the following code to initiate your SparkSession at the beginning:\nThis will register GeoSparkVizKryo serializer.  var   sparkSession   =   SparkSession . builder ()  . master ( local[*] )   // Delete this if run in cluster mode  . appName ( readTestScala )   // Change this to a proper name  // Enable GeoSpark custom Kryo serializer  . config ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  . config ( spark.kryo.registrator ,   classOf [ GeoSparkVizKryoRegistrator ]. getName )  . getOrCreate ()", 
            "title": "Initiate SparkSession"
        }, 
        {
            "location": "/tutorial/viz/#register-geosparksql-and-geosparkviz", 
            "text": "Add the following line after your SparkSession declaration  GeoSparkSQLRegistrator . registerAll ( sparkSession )  GeoSparkVizRegistrator . registerAll ( sparkSession )   This will register all User Defined Tyeps, functions and optimizations in GeoSparkSQL and GeoSparkViz.", 
            "title": "Register GeoSparkSQL and GeoSparkViz"
        }, 
        {
            "location": "/tutorial/viz/#create-spatial-dataframe", 
            "text": "There is a DataFrame as follows:  +----------+---------+\n|       _c0|      _c1|\n+----------+---------+\n|-88.331492|32.324142|\n|-88.175933|32.360763|\n|-88.388954|32.357073|\n|-88.221102| 32.35078|  You first need to create a Geometry type column.  CREATE   OR   REPLACE   TEMP   VIEW   pointtable   AS  SELECT   ST_Point ( cast ( pointtable . _c0   as   Decimal ( 24 , 20 )), cast ( pointtable . _c1   as   Decimal ( 24 , 20 )))   as   shape  FROM   pointtable   As you know, GeoSpark provides many different methods to load various spatial data formats. Please read  Write an Spatial DataFrame application .", 
            "title": "Create Spatial DataFrame"
        }, 
        {
            "location": "/tutorial/viz/#generate-a-single-image", 
            "text": "In most cases, you just want to see a single image out of your spatial dataset.", 
            "title": "Generate a single image"
        }, 
        {
            "location": "/tutorial/viz/#pixelize-spatial-objects", 
            "text": "To put spatial objects on a map image, you first need to convert them to pixels.  First, compute the spatial boundary of this column.  CREATE   OR   REPLACE   TEMP   VIEW   boundtable   AS  SELECT   ST_Envelope_Aggr ( shape )   as   bound   FROM   pointtable   Then use ST_Pixelize to conver them to pixels.  CREATE   OR   REPLACE   TEMP   VIEW   pixels   AS  SELECT   pixel ,   shape   FROM   pointtable  LATERAL   VIEW   ST_Pixelize ( ST_Transform ( shape ,   epsg:4326 , epsg:3857 ),   256 ,   256 ,   ( SELECT   ST_Transform ( bound ,   epsg:4326 , epsg:3857 )   FROM   boundtable ))   AS   pixel   This will give you a 256*256 resolution image after you run ST_Render at the end of this tutorial.   Warning  We highly suggest that you should use ST_Transform to transfrom coordiantes to a visualization-specific coordinate sysmte such as epsg:3857. Otherwise you map may look distorted.", 
            "title": "Pixelize spatial objects"
        }, 
        {
            "location": "/tutorial/viz/#aggregate-pixels", 
            "text": "Many objects may be pixelized to the same pixel locations. You now need to aggregate them based on either their spatial aggregation or spatial observations such as temperature or humidity.  CREATE   OR   REPLACE   TEMP   VIEW   pixelaggregates   AS  SELECT   pixel ,   count ( * )   as   weight  FROM   pixels  GROUP   BY   pixel   The weight indicates the degree of spatial aggregation or spatial observations. Later on, it will determine the color of this pixel.", 
            "title": "Aggregate pixels"
        }, 
        {
            "location": "/tutorial/viz/#colorize-pixels", 
            "text": "Run the following command to assign colors for pixels based on their weights.  CREATE   OR   REPLACE   TEMP   VIEW   pixelaggregates   AS  SELECT   pixel ,   ST_Colorize ( weight ,   ( SELECT   max ( weight )   FROM   pixelaggregates ))   as   color  FROM   pixelaggregates   Please read  ST_Colorize  for a detailed API description.", 
            "title": "Colorize pixels"
        }, 
        {
            "location": "/tutorial/viz/#render-the-image", 
            "text": "Use ST_Render to plot all pixels on a single image.  CREATE   OR   REPLACE   TEMP   VIEW   images   AS  SELECT   ST_Render ( pixel ,   color )   AS   image ,   ( SELECT   ST_AsText ( bound )   FROM   boundtable )   AS   boundary  FROM   pixelaggregates   This DataFrame will contain a Image type column which has only one image.", 
            "title": "Render the image"
        }, 
        {
            "location": "/tutorial/viz/#store-the-image-on-disk", 
            "text": "Fetch the image from the previous DataFrame  var image = sparkSession.table( images ).take(1)(0)(0).asInstanceOf[ImageSerializableWrapper].getImage  Use GeoSparkViz ImageGenerator to store this image on disk.  var   imageGenerator   =   new   ImageGenerator  imageGenerator . SaveRasterImageAsLocalFile ( image ,   System . getProperty ( user.dir )+ /target/points ,   ImageType . PNG )", 
            "title": "Store the image on disk"
        }, 
        {
            "location": "/tutorial/viz/#generate-map-tiles", 
            "text": "If you are a map tile professional, you may need to generate map tiles for different zoom levels and eventually create the map tile layer.", 
            "title": "Generate map tiles"
        }, 
        {
            "location": "/tutorial/viz/#pixelization-and-pixel-aggregation", 
            "text": "Please first do pixelization and pixel aggregation using the same commands in single image generation. In ST_Pixelize, you need specify a very high resolution.", 
            "title": "Pixelization and pixel aggregation"
        }, 
        {
            "location": "/tutorial/viz/#create-tile-name", 
            "text": "Run the following command to compute the tile name for every pixels  CREATE   OR   REPLACE   TEMP   VIEW   pixelaggregates   AS  SELECT   pixel ,   weight ,   ST_TileName ( pixel ,   3 )   AS   pid  FROM   pixelaggregates   \"3\" is the zoom level for these map tiles.", 
            "title": "Create tile name"
        }, 
        {
            "location": "/tutorial/viz/#colorize-pixels_1", 
            "text": "Use the same command explained in single image generation to assign colors.", 
            "title": "Colorize pixels"
        }, 
        {
            "location": "/tutorial/viz/#render-map-tiles", 
            "text": "You now need to group pixels by tiles and then render map tile images in parallel.  CREATE   OR   REPLACE   TEMP   VIEW   images   AS  SELECT   ST_Render ( pixel ,   color )   AS   image  FROM   pixelaggregates  GROUP   BY   pid", 
            "title": "Render map tiles"
        }, 
        {
            "location": "/tutorial/viz/#store-map-tiles-on-disk", 
            "text": "You can use the same commands in single image generation to fetch all map tiles and store them one by one.", 
            "title": "Store map tiles on disk"
        }, 
        {
            "location": "/tutorial/zeppelin/", 
            "text": "Starting from 1.2.0, GeoSpark provides a Helium visualization plugin tailored for \nApache Zeppelin\n. This finally bridges the gap between GeoSpark and Zeppelin.  Please read \nInstall GeoSpark-Zeppelin\n to learn how to install this plugin in Zeppelin.\n\n\nGeoSpark-Zeppelin equips two approaches to visualize spatial data in Zeppelin. The first approach uses Zeppelin to plot all spatial objects on the map. The second one leverages GeoSparkViz to generate map images and overlay them on maps.\n\n\nSmall-scale without GeoSparkViz\n\n\n\n\nDanger\n\n\nZeppelin is just a front-end visualization framework. This approach is not scalable and will fail at large-scale geospatial data. Please scroll down to read GeoSparkViz solution.\n\n\n\n\nYou can use Apache Zeppelin to plot a small number of spatial objects, such as 1000 points. Assume you already have a Spatial DataFrame, you need to convert the geometry column to WKT string column use the following command in your Zeppelin Spark notebook Scala paragraph:\n\n\nspark\n.\nsql\n(\n\n  \n\n\n    |CREATE OR REPLACE TEMP VIEW wktpoint AS\n\n\n    |SELECT ST_AsText(shape) as geom\n\n\n    |FROM pointtable\n\n\n  \n.\nstripMargin\n)\n\n\n\n\n\nThen create an SQL paragraph to fetch the data\n\n%\nsql\n\n\nSELECT\n \n*\n\n\nFROM\n \nwktpoint\n\n\n\n\nSelect the geometry column to visualize:\n\n\n\n\nLarge-scale with GeoSparkViz\n\n\nGeoSparkViz is a distributed visualization system that allows you to visualize big spatial data at scale. Please read \nHow to use GeoSparkViz\n.\n\n\nYou can use GeoSpark-Zeppelin to ask Zeppelin to overlay GeoSparkViz images on a map background. This way, you can easily visualize 1 billion spatial objects or more (depends on your cluster size).\n\n\nFirst, encode images of GeoSparkViz DataFrame in Zeppelin Spark notebook Scala paragraph,\n\n\nspark.sql(\n  \n\n    |CREATE OR REPLACE TEMP VIEW images AS\n    |SELECT ST_EncodeImage(image) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\n    |FROM images\n  \n.stripMargin)\n\n\n\n\nThen create an SQL paragraph to fetch the data\n\n%\nsql\n\n\nSELECT\n \n*\n,\n \nI am the map center!\n\n\nFROM\n \nimages\n\n\n\n\nSelect the image and its geospatial boundary:\n\n\n\n\nZeppelin Spark notebook demo\n\n\nWe provide a full Zeppelin Spark notebook which demonstrates al functions. Please download \nGeoSpark-Zeppelin notebook template\n and \ntest data - arealm.csv\n.\n\n\nYou need to use Zeppelin to import this notebook JSON file and modify the input data path in the notebook.", 
            "title": "Run GeoSpark via Zeppelin"
        }, 
        {
            "location": "/tutorial/zeppelin/#small-scale-without-geosparkviz", 
            "text": "Danger  Zeppelin is just a front-end visualization framework. This approach is not scalable and will fail at large-scale geospatial data. Please scroll down to read GeoSparkViz solution.   You can use Apache Zeppelin to plot a small number of spatial objects, such as 1000 points. Assume you already have a Spatial DataFrame, you need to convert the geometry column to WKT string column use the following command in your Zeppelin Spark notebook Scala paragraph:  spark . sql ( \n        |CREATE OR REPLACE TEMP VIEW wktpoint AS      |SELECT ST_AsText(shape) as geom      |FROM pointtable     . stripMargin )   Then create an SQL paragraph to fetch the data % sql  SELECT   *  FROM   wktpoint   Select the geometry column to visualize:", 
            "title": "Small-scale without GeoSparkViz"
        }, 
        {
            "location": "/tutorial/zeppelin/#large-scale-with-geosparkviz", 
            "text": "GeoSparkViz is a distributed visualization system that allows you to visualize big spatial data at scale. Please read  How to use GeoSparkViz .  You can use GeoSpark-Zeppelin to ask Zeppelin to overlay GeoSparkViz images on a map background. This way, you can easily visualize 1 billion spatial objects or more (depends on your cluster size).  First, encode images of GeoSparkViz DataFrame in Zeppelin Spark notebook Scala paragraph,  spark.sql(\n   \n    |CREATE OR REPLACE TEMP VIEW images AS\n    |SELECT ST_EncodeImage(image) AS image, (SELECT ST_AsText(bound) FROM boundtable) AS boundary\n    |FROM images\n   .stripMargin)  Then create an SQL paragraph to fetch the data % sql  SELECT   * ,   I am the map center!  FROM   images   Select the image and its geospatial boundary:", 
            "title": "Large-scale with GeoSparkViz"
        }, 
        {
            "location": "/tutorial/zeppelin/#zeppelin-spark-notebook-demo", 
            "text": "We provide a full Zeppelin Spark notebook which demonstrates al functions. Please download  GeoSpark-Zeppelin notebook template  and  test data - arealm.csv .  You need to use Zeppelin to import this notebook JSON file and modify the input data path in the notebook.", 
            "title": "Zeppelin Spark notebook demo"
        }, 
        {
            "location": "/tutorial/geospark-core-python/", 
            "text": "Spatial RDD Applications in Python\n\n\nIntroduction\n\n\n\nGeoSpark provides a Python wrapper on GeoSpark core Java/Scala library.\nGeoSpark SpatialRDDs (and other classes when it was necessary) have implemented meta classes which allow \nto use overloaded functions, methods and constructors to be the most similar to Java/Scala API as possible. \n\n\n\nGeoSpark-core provides five special SpatialRDDs:\n\n\n PointRDD \n\n\n\n PolygonRDD \n\n\n\n LineStringRDD \n\n\n\n CircleRDD \n\n\n\n RectangleRDD \n\n\n\n\n\n\nAll of them can be imported from \n geospark.core.SpatialRDD \n module\n\n\n geospark \n has written serializers which convert GeoSpark SpatialRDD to Python objects.\nConverting will produce GeoData objects which have 2 attributes:\n\n\n\n\n\n\n geom: shapely.geometry.BaseGeometry \n\n\n\n userData: str \n\n\n\ngeom attribute holds geometry representation as shapely objects. \nuserData is string representation of other attributes separated by \"\\t\"\n\n\n\nGeoData has one method to get user data.\n\n getUserData() -\n str \n\n\nInstalling the package\n\n\nGeoSpark extends pyspark functions which depend on Python packages and Scala libraries. To see all dependencies\nplease look at the dependencies section.\n\nhttps://pypi.org/project/pyspark/\n.\n\n\nThis package needs 2 jar files to work properly:\n\n\n\n\ngeospark.jar\n\n\ngeo_wrapper.jar\n\n\n\n\n\n\nNote\n\n\nTo enable GeoSpark Core functionality without GeoSparkSQL there is no need to copy jar files from geospark/jars\nlocation. You can use jar files from Maven repositories. Since GeoSpark 1.3.0 it is possible also to use maven \njars for GeoSparkSQL instead of geospark/jars/../geospark-sql jars files.\n\n\n\n\nThis package automatically copies the newest GeoSpark jar files using upload_jars function, please follow the example below.\n\n\n upload_jars \n\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\nfrom\n \ngeospark.register\n \nimport\n \nupload_jars\n\n\nfrom\n \ngeospark.register\n \nimport\n \nGeoSparkRegistrator\n\n\n\nupload_jars\n()\n\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\n\\\n      \ngetOrCreate\n()\n\n\n\nGeoSparkRegistrator\n.\nregisterAll\n(\nspark\n)\n\n\n\n\n\nFunction\n\n\nupload_jars\n()\n\n\n\n\n\nuses findspark Python package to upload jar files to executor and nodes. To avoid copying all the time, jar files can be put in directory SPARK_HOME/jars or any other path specified in Spark config files.\n\n\nInstalling from PyPi repositories\n\n\nPlease use command below\n\n\npip install geospark\n\n\n\n\nInstalling from wheel file\n\n\npipenv run python -m pip install dist/geospark-1.3.1-py3-none-any.whl\n\n\n\n\nor\n\n\npip install dist/geospark-1.3.1-py3-none-any.whl\n\n\n\n\nInstalling from source\n\n\npython3 setup.py install\n\n\n\n\nGeoSpark Serializers\n\n\nGeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.\n\n\nconf\n.\nset\n(\nspark.serializer\n,\n \nKryoSerializer\n.\ngetName\n)\n\n\nconf\n.\nset\n(\nspark.kryo.registrator\n,\n \nGeoSparkKryoRegistrator\n.\ngetName\n)\n\n\n\n\n\nCreate a SpatialRDD\n\n\nCreate a typed SpatialRDD\n\n\nGeoSpark-core provides three special SpatialRDDs:\n\n PointRDD \n\n\n PolygonRDD \n \n\n LineStringRDD \n\n\n CircleRDD \n\n\n RectangleRDD \n\n\n\n\nThey can be loaded from CSV, TSV, WKT, WKB, Shapefiles, GeoJSON formats.\nTo pass the format to SpatialRDD constructor please use \n FileDataSplitter \n enumeration. \n\n\ngeospark SpatialRDDs (and other classes when it was necessary) have implemented meta classes which allow \nto use overloaded functions how Scala/Java GeoSpark API allows. ex. \n\n\nfrom\n \npyspark\n \nimport\n \nStorageLevel\n\n\nfrom\n \ngeospark.core.SpatialRDD\n \nimport\n \nPointRDD\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nFileDataSplitter\n\n\n\ninput_location\n \n=\n \ncheckin.csv\n\n\noffset\n \n=\n \n0\n  \n# The point long/lat starts from Column 0\n\n\nsplitter\n \n=\n \nFileDataSplitter\n.\nCSV\n \n# FileDataSplitter enumeration\n\n\ncarry_other_attributes\n \n=\n \nTrue\n  \n# Carry Column 2 (hotel, gas, bar...)\n\n\nlevel\n \n=\n \nStorageLevel\n.\nMEMORY_ONLY\n \n# Storage level from pyspark\n\n\ns_epsg\n \n=\n \nepsg:4326\n \n# Source epsg code\n\n\nt_epsg\n \n=\n \nepsg:5070\n \n# target epsg code\n\n\n\npoint_rdd\n \n=\n \nPointRDD\n(\nsc\n,\n \ninput_location\n,\n \noffset\n,\n \nsplitter\n,\n \ncarry_other_attributes\n)\n\n\n\npoint_rdd\n \n=\n \nPointRDD\n(\nsc\n,\n \ninput_location\n,\n \nsplitter\n,\n \ncarry_other_attributes\n,\n \nlevel\n,\n \ns_epsg\n,\n \nt_epsg\n)\n\n\n\npoint_rdd\n \n=\n \nPointRDD\n(\n\n    \nsparkContext\n=\nsc\n,\n\n    \nInputLocation\n=\ninput_location\n,\n\n    \nOffset\n=\noffset\n,\n\n    \nsplitter\n=\nsplitter\n,\n\n    \ncarryInputData\n=\ncarry_other_attributes\n\n\n)\n\n\n\n\n\nFrom SparkSQL DataFrame\n\n\nTo create spatialRDD from other formats you can use adapter between Spark DataFrame and SpatialRDD\n\n\n Load data in GeoSparkSQL. \n\n\n\ncsv_point_input_location\n=\n \n/tests/resources/county_small.tsv\n\n\n\ndf\n \n=\n \nspark\n.\nread\n.\n\\\n    \nformat\n(\ncsv\n)\n.\n\\\n    \noption\n(\ndelimiter\n,\n \n\\t\n)\n.\n\\\n    \noption\n(\nheader\n,\n \nfalse\n)\n.\n\\\n    \nload\n(\ncsv_point_input_location\n)\n\n\n\ndf\n.\ncreateOrReplaceTempView\n(\ncounties\n)\n\n\n\n\n\n Create a Geometry type column in GeoSparkSQL \n\n\n\nspatial_df\n \n=\n \nspark\n.\nsql\n(\n\n    \n\n\n        SELECT ST_GeomFromWKT(_c0) as geom, _c6 as county_name\n\n\n        FROM counties\n\n\n    \n\n\n)\n\n\nspatial_df\n.\nprintSchema\n()\n\n\n\n\n\nroot\n |-- geom: geometry (nullable = false)\n |-- county_name: string (nullable = true)\n\n\n\n\n Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD \n\n\n\nNote that, you have to name your column geometry\n\n\nfrom\n \ngeospark.utils.adapter\n \nimport\n \nAdapter\n\n\n\nspatial_rdd\n \n=\n \nAdapter\n.\ntoSpatialRdd\n(\nspatial_df\n)\n\n\nspatial_rdd\n.\nanalyze\n()\n\n\n\nspatial_rdd\n.\nboundaryEnvelope\n\n\n\n\n\ngeospark.core.geom_types.Envelope object at 0x7f1e5f29fe10\n\n\n\n\n\nor pass Geometry column name as a second argument\n\n\nspatial_rdd\n \n=\n \nAdapter\n.\ntoSpatialRdd\n(\nspatial_df\n,\n \ngeom\n)\n\n\n\n\n\nFor WKT/WKB/GeoJSON data, please use \nST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON\n instead.\n\n\nRead other attributes in an SpatialRDD\n\n\nEach SpatialRDD can carry non-spatial attributes such as price, age and name as long as the user sets \ncarryOtherAttributes\n as \nTRUE\n.\n\n\nThe other attributes are combined together to a string and stored in \nUserData\n field of each geometry.\n\n\nTo retrieve the UserData field, use the following code:\n\nrdd_with_other_attributes\n \n=\n \nobject_rdd\n.\nrawSpatialRDD\n.\nmap\n(\nlambda\n \nx\n:\n \nx\n.\ngetUserData\n())\n\n\n\n\nWrite a Spatial Range Query\n\n\nfrom\n \ngeospark.core.geom.envelope\n \nimport\n \nEnvelope\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nRangeQuery\n\n\n\nrange_query_window\n \n=\n \nEnvelope\n(\n-\n90.01\n,\n \n-\n80.01\n,\n \n30.01\n,\n \n40.01\n)\n\n\nconsider_boundary_intersection\n \n=\n \nFalse\n  \n## Only return gemeotries fully covered by the window\n\n\nusing_index\n \n=\n \nFalse\n\n\nquery_result\n \n=\n \nRangeQuery\n.\nSpatialRangeQuery\n(\nspatial_rdd\n,\n \nrange_query_window\n,\n \nconsider_boundary_intersection\n,\n \nusing_index\n)\n\n\n\n\n\nRange query window\n\n\nBesides the rectangle (Envelope) type range query window, GeoSpark range query window can be \n\n Point \n \n\n Polygon \n\n\n LineString \n\n\n\n\nThe code to create a point is as follows:\nTo create shapely geometries please follow official shapely \n documentation \n  \n\n\nUse spatial indexes\n\n\nGeoSpark provides two types of spatial indexes,\n\n Quad-Tree \n\n\n R-Tree \n\nOnce you specify an index type, \nGeoSpark will build a local tree index on each of the SpatialRDD partition.\n\n\nTo utilize a spatial index in a spatial range query, use the following code:\n\n\nfrom\n \ngeospark.core.geom.envelope\n \nimport\n \nEnvelope\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nIndexType\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nRangeQuery\n\n\n\nrange_query_window\n \n=\n \nEnvelope\n(\n-\n90.01\n,\n \n-\n80.01\n,\n \n30.01\n,\n \n40.01\n)\n\n\nconsider_boundary_intersection\n \n=\n \nFalse\n \n## Only return gemeotries fully covered by the window\n\n\n\nbuild_on_spatial_partitioned_rdd\n \n=\n \nFalse\n \n## Set to TRUE only if run join query\n\n\nspatial_rdd\n.\nbuildIndex\n(\nIndexType\n.\nQUADTREE\n,\n \nbuild_on_spatial_partitioned_rdd\n)\n\n\n\nusing_index\n \n=\n \nTrue\n\n\n\nquery_result\n \n=\n \nRangeQuery\n.\nSpatialRangeQuery\n(\n\n    \nspatial_rdd\n,\n\n    \nrange_query_window\n,\n\n    \nconsider_boundary_intersection\n,\n\n    \nusing_index\n\n\n)\n\n\n\n\n\nOutput format\n\n\nThe output format of the spatial range query is another RDD which consists of GeoData objects.\n\n\nSpatialRangeQuery result can be used as RDD with map or other spark RDD funtions. Also it can be used as \nPython objects when using collect method.\nExample:\n\n\nquery_result\n.\nmap\n(\nlambda\n \nx\n:\n \nx\n.\ngeom\n.\nlength\n)\n.\ncollect\n()\n\n\n\n\n\n[\n 1.5900840000000045,\n 1.5906639999999896,\n 1.1110299999999995,\n 1.1096700000000084,\n 1.1415619999999933,\n 1.1386399999999952,\n 1.1415619999999933,\n 1.1418860000000137,\n 1.1392780000000045,\n ...\n]\n\n\n\n\nOr transformed to GeoPandas GeoDataFrame\n\n\nimport\n \ngeopandas\n \nas\n \ngpd\n\n\ngpd\n.\nGeoDataFrame\n(\n\n    \nquery_result\n.\nmap\n(\nlambda\n \nx\n:\n \n[\nx\n.\ngeom\n,\n \nx\n.\nuserData\n])\n.\ncollect\n(),\n\n    \ncolumns\n=\n[\ngeom\n,\n \nuser_data\n],\n\n    \ngeometry\n=\ngeom\n\n\n)\n\n\n\n\n\nWrite a Spatial KNN Query\n\n\nA spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.\n\n\nAssume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nKNNQuery\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nPoint\n\n\n\npoint\n \n=\n \nPoint\n(\n-\n84.01\n,\n \n34.01\n)\n\n\nk\n \n=\n \n1000\n \n## K Nearest Neighbors\n\n\nusing_index\n \n=\n \nFalse\n\n\nresult\n \n=\n \nKNNQuery\n.\nSpatialKnnQuery\n(\nobject_rdd\n,\n \npoint\n,\n \nk\n,\n \nusing_index\n)\n\n\n\n\n\nQuery center geometry\n\n\nBesides the Point type, GeoSpark KNN query center can be \n\n Polygon \n\n\n LineString \n\n\nTo create Polygon or Linestring object please follow Shapely official \n documentation \n\n\nUse spatial indexes\n\n\nTo utilize a spatial index in a spatial KNN query, use the following code:\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nKNNQuery\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nIndexType\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nPoint\n\n\n\npoint\n \n=\n \nPoint\n(\n-\n84.01\n,\n \n34.01\n)\n\n\nk\n \n=\n \n5\n \n## K Nearest Neighbors\n\n\n\nbuild_on_spatial_partitioned_rdd\n \n=\n \nFalse\n \n## Set to TRUE only if run join query\n\n\nspatial_rdd\n.\nbuildIndex\n(\nIndexType\n.\nRTREE\n,\n \nbuild_on_spatial_partitioned_rdd\n)\n\n\n\nusing_index\n \n=\n \nTrue\n\n\nresult\n \n=\n \nKNNQuery\n.\nSpatialKnnQuery\n(\nspatial_rdd\n,\n \npoint\n,\n \nk\n,\n \nusing_index\n)\n\n\n\n\n\n\n\nWarning\n\n\nOnly R-Tree index supports Spatial KNN query\n\n\n\n\nOutput format\n\n\nThe output format of the spatial KNN query is a list of GeoData objects. \nThe list has K GeoData objects.\n\n\nExample:\n\n \nresult\n\n\n\n[\nGeoData\n,\n \nGeoData\n,\n \nGeoData\n,\n \nGeoData\n,\n \nGeoData\n]\n\n\n\n\nWrite a Spatial Join Query\n\n\nA spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.\n\n\nAssume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nGridType\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nJoinQuery\n\n\n\nconsider_boundary_intersection\n \n=\n \nFalse\n \n## Only return geometries fully covered by each query window in queryWindowRDD\n\n\nusing_index\n \n=\n \nFalse\n\n\n\nobject_rdd\n.\nanalyze\n()\n\n\n\nobject_rdd\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nquery_window_rdd\n.\nspatialPartitioning\n(\nobject_rdd\n.\ngetPartitioner\n())\n\n\n\nresult\n \n=\n \nJoinQuery\n.\nSpatialJoinQuery\n(\nobject_rdd\n,\n \nquery_window_rdd\n,\n \nusing_index\n,\n \nconsider_boundary_intersection\n)\n\n\n\n\n\nResult of SpatialJoinQuery is RDD which consists of GeoData instance and list of GeoData instances which spatially intersects or \nare covered by GeoData. \n\n\nresult\n.\ncollect\n())\n\n\n\n\n\n[\n    [GeoData, [GeoData, GeoData, GeoData, GeoData]],\n    [GeoData, [GeoData, GeoData, GeoData]],\n    [GeoData, [GeoData]],\n    [GeoData, [GeoData, GeoData]],\n    ...\n    [GeoData, [GeoData, GeoData]]\n]\n\n\n\n\nUse spatial partitioning\n\n\nGeoSpark spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.\n\n\nIf you first partition SpatialRDD A, then you must use the partitioner of A to partition B.\n\n\nobject_rdd\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nquery_window_rdd\n.\nspatialPartitioning\n(\nobject_rdd\n.\ngetPartitioner\n())\n\n\n\n\n\nOr \n\n\nquery_window_rdd\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nobject_rdd\n.\nspatialPartitioning\n(\nquery_window_rdd\n.\ngetPartitioner\n())\n\n\n\n\n\nUse spatial indexes\n\n\nTo utilize a spatial index in a spatial join query, use the following code:\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nGridType\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nIndexType\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nJoinQuery\n\n\n\nobject_rdd\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nquery_window_rdd\n.\nspatialPartitioning\n(\nobject_rdd\n.\ngetPartitioner\n())\n\n\n\nbuild_on_spatial_partitioned_rdd\n \n=\n \nTrue\n \n## Set to TRUE only if run join query\n\n\nusing_index\n \n=\n \nTrue\n\n\nquery_window_rdd\n.\nbuildIndex\n(\nIndexType\n.\nQUADTREE\n,\n \nbuild_on_spatial_partitioned_rdd\n)\n\n\n\nresult\n \n=\n \nJoinQuery\n.\nSpatialJoinQueryFlat\n(\nobject_rdd\n,\n \nquery_window_rdd\n,\n \nusing_index\n,\n \nTrue\n)\n\n\n\n\n\nThe index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.\n\n\nOutput format\n\n\nThe output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two GeoData objects.\nThe left one is the GeoData from object_rdd and the right one is the GeoData from the query_window_rdd.\n\n\nPoint,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n\n\n\n\nexample \n\nresult\n.\ncollect\n()\n\n\n\n\n[\n [GeoData, GeoData],\n [GeoData, GeoData],\n [GeoData, GeoData],\n [GeoData, GeoData],\n ...\n [GeoData, GeoData],\n [GeoData, GeoData]\n]\n\n\n\n\nEach object on the left is covered/intersected by the object on the right.\n\n\nWrite a Distance Join Query\n\n\nA distance join query takes two spatial RDD assuming that we have two SpatialRDD's:\n\n object_rdd \n\n\n spatial_rdd \n\n\nAnd finds the geometries (from spatial_rdd) are within given distance to it. spatial_rdd and object_rdd\ncan be any geometry type (point, line, polygon) and are not necessary to have the same geometry type\n\n\nYou can use the following code to issue an Distance Join Query on them.\n\n\nfrom\n \ngeospark.core.SpatialRDD\n \nimport\n \nCircleRDD\n\n\nfrom\n \ngeospark.core.enums\n \nimport\n \nGridType\n\n\nfrom\n \ngeospark.core.spatialOperator\n \nimport\n \nJoinQuery\n\n\n\nobject_rdd\n.\nanalyze\n()\n\n\n\ncircle_rdd\n \n=\n \nCircleRDD\n(\nobject_rdd\n,\n \n0.1\n)\n \n## Create a CircleRDD using the given distance\n\n\ncircle_rdd\n.\nanalyze\n()\n\n\n\ncircle_rdd\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nspatial_rdd\n.\nspatialPartitioning\n(\ncircle_rdd\n.\ngetPartitioner\n())\n\n\n\nconsider_boundary_intersection\n \n=\n \nFalse\n \n## Only return gemeotries fully covered by each query window in queryWindowRDD\n\n\nusing_index\n \n=\n \nFalse\n\n\n\nresult\n \n=\n \nJoinQuery\n.\nDistanceJoinQueryFlat\n(\nspatial_rdd\n,\n \ncircle_rdd\n,\n \nusing_index\n,\n \nconsider_boundary_intersection\n)\n\n\n\n\n\nOutput format\n\n\nResult for this query is RDD which holds two GeoData objects within list of lists.\nExample:\n\nresult\n.\ncollect\n()\n\n\n\n\n[[GeoData, GeoData], [GeoData, GeoData] ...]\n\n\n\n\nIt is possible to do some RDD operation on result data ex. Getting polygon centroid.\n\nresult\n.\nmap\n(\nlambda\n \nx\n:\n \nx\n[\n0\n]\n.\ngeom\n.\ncentroid\n)\n.\ncollect\n()\n\n\n\n\n[\n \nshapely.geometry.point.Point at 0x7efee2d28128\n,\n \nshapely.geometry.point.Point at 0x7efee2d280b8\n,\n \nshapely.geometry.point.Point at 0x7efee2d28fd0\n,\n \nshapely.geometry.point.Point at 0x7efee2d28080\n,\n ...\n]\n\n\n\n\nSave to permanent storage\n\n\nYou can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.\n\n\n\n\nNote\n\n\nNon-spatial attributes such as price, age and name will also be stored to permanent storage.\n\n\n\n\nSave an SpatialRDD (not indexed)\n\n\nTyped SpatialRDD and generic SpatialRDD can be saved to permanent storage.\n\n\nSave to distributed WKT text file\n\n\nUse the following code to save an SpatialRDD as a distributed WKT text file:\n\n\nobject_rdd\n.\nrawSpatialRDD\n.\nsaveAsTextFile\n(\nhdfs://PATH\n)\n\n\nobject_rdd\n.\nsaveAsWKT\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed WKB text file\n\n\nUse the following code to save an SpatialRDD as a distributed WKB text file:\n\n\nobject_rdd\n.\nsaveAsWKB\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed GeoJSON text file\n\n\nUse the following code to save an SpatialRDD as a distributed GeoJSON text file:\n\n\nobject_rdd\n.\nsaveAsGeoJSON\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed object file\n\n\nUse the following code to save an SpatialRDD as a distributed object file:\n\n\nobject_rdd\n.\nrawJvmSpatialRDD\n.\nsaveAsObjectFile\n(\nhdfs://PATH\n)\n\n\n\n\n\n\n\nNote\n\n\nEach object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.\n\n\n\n\nSave an SpatialRDD (indexed)\n\n\nIndexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.\n\n\nSave to distributed object file\n\n\nUse the following code to save an SpatialRDD as a distributed object file:\n\n\nobject_rdd\n.\nindexedRawRDD\n.\nsaveAsObjectFile\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave an SpatialRDD (spatialPartitioned W/O indexed)\n\n\nA spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!\n\n\nReload a saved SpatialRDD\n\n\nYou can easily reload an SpatialRDD that has been saved to \na distributed object file\n.\n\n\nLoad to a typed SpatialRDD\n\n\nUse the following code to reload the PointRDD/PolygonRDD/LineStringRDD:\n\n\nfrom\n \ngeospark.core.formatMapper.disc_utils\n \nimport\n \nload_spatial_rdd_from_disc\n,\n \nGeoType\n\n\n\npolygon_rdd\n \n=\n \nload_spatial_rdd_from_disc\n(\nsc\n,\n \nhdfs://PATH\n,\n \nGeoType\n.\nPOLYGON\n)\n\n\npoint_rdd\n \n=\n \nload_spatial_rdd_from_disc\n(\nsc\n,\n \nhdfs://PATH\n,\n \nGeoType\n.\nPOINT\n)\n\n\nlinestring_rdd\n \n=\n \nload_spatial_rdd_from_disc\n(\nsc\n,\n \nhdfs://PATH\n,\n \nGeoType\n.\nLINESTRING\n)\n\n\n\n\n\nLoad to a generic SpatialRDD\n\n\nUse the following code to reload the SpatialRDD:\n\n\nsaved_rdd\n \n=\n \nload_spatial_rdd_from_disc\n(\nsc\n,\n \nhdfs://PATH\n,\n \nGeoType\n.\nGEOMETRY\n)\n\n\n\n\n\nUse the following code to reload the indexed SpatialRDD:\n\nsaved_rdd\n \n=\n \nSpatialRDD\n()\n\n\nsaved_rdd\n.\nindexedRawRDD\n \n=\n \nload_spatial_index_rdd_from_disc\n(\nsc\n,\n \nhdfs://PATH\n)\n\n\n\n\nRead from other Geometry files\n\n\nAll below methods will return SpatialRDD object which can be used with Spatial functions such as Spatial Join etc.\n\n\nRead from WKT file\n\n\nfrom\n \ngeospark.core.formatMapper\n \nimport\n \nWktReader\n\n\n\nWktReader\n.\nreadToGeometryRDD\n(\nsc\n,\n \nwkt_geometries_location\n,\n \n0\n,\n \nTrue\n,\n \nFalse\n)\n\n\n\n\ngeospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2fbf250\n\n\n\n\nRead from WKB file\n\n\nfrom\n \ngeospark.core.formatMapper\n \nimport\n \nWkbReader\n\n\n\nWkbReader\n.\nreadToGeometryRDD\n(\nsc\n,\n \nwkb_geometries_location\n,\n \n0\n,\n \nTrue\n,\n \nFalse\n)\n\n\n\n\ngeospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2eece50\n\n\n\n\nRead from GeoJson file\n\n\nfrom\n \ngeospark.core.formatMapper\n \nimport\n \nGeoJsonReader\n\n\n\nGeoJsonReader\n.\nreadToGeometryRDD\n(\nsc\n,\n \ngeo_json_file_location\n)\n\n\n\n\ngeospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2eecb90\n\n\n\n\nRead from Shapefile\n\n\nfrom\n \ngeospark.core.formatMapper.shapefileParser\n \nimport\n \nShapefileReader\n\n\n\nShapefileReader\n.\nreadToGeometryRDD\n(\nsc\n,\n \nshape_file_location\n)\n\n\n\n\ngeospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2ee0710\n\n\n\n\nSupported versions\n\n\nCurrently this python wrapper supports the following Spark, GeoSpark and Python versions\n\n\nApache Spark\n\n\n 2.2 \n\n\n\n 2.3 \n\n\n\n 2.4 \n\n\n\nGeoSpark\n\n\n 1.3.1 \n\n\n\n 1.2.0 \n\n\n\nPython\n\n\n 3.6 \n\n\n\n 3.7 \n\n\n\n\n\nNote\n\n\nOther versions may also work (or partially) but were not tested yet", 
            "title": "Spatial RDD in Python"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#spatial-rdd-applications-in-python", 
            "text": "", 
            "title": "Spatial RDD Applications in Python"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#introduction", 
            "text": "GeoSpark provides a Python wrapper on GeoSpark core Java/Scala library.\nGeoSpark SpatialRDDs (and other classes when it was necessary) have implemented meta classes which allow \nto use overloaded functions, methods and constructors to be the most similar to Java/Scala API as possible.   GeoSpark-core provides five special SpatialRDDs:   PointRDD    PolygonRDD    LineStringRDD    CircleRDD    RectangleRDD    \nAll of them can be imported from   geospark.core.SpatialRDD   module  geospark   has written serializers which convert GeoSpark SpatialRDD to Python objects.\nConverting will produce GeoData objects which have 2 attributes:    geom: shapely.geometry.BaseGeometry    userData: str   geom attribute holds geometry representation as shapely objects. \nuserData is string representation of other attributes separated by \"\\t\"  GeoData has one method to get user data.  getUserData() -  str", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#installing-the-package", 
            "text": "GeoSpark extends pyspark functions which depend on Python packages and Scala libraries. To see all dependencies\nplease look at the dependencies section. https://pypi.org/project/pyspark/ .  This package needs 2 jar files to work properly:   geospark.jar  geo_wrapper.jar    Note  To enable GeoSpark Core functionality without GeoSparkSQL there is no need to copy jar files from geospark/jars\nlocation. You can use jar files from Maven repositories. Since GeoSpark 1.3.0 it is possible also to use maven \njars for GeoSparkSQL instead of geospark/jars/../geospark-sql jars files.   This package automatically copies the newest GeoSpark jar files using upload_jars function, please follow the example below.   upload_jars   from   pyspark.sql   import   SparkSession  from   geospark.register   import   upload_jars  from   geospark.register   import   GeoSparkRegistrator  upload_jars ()  spark   =   SparkSession . builder . \\\n       getOrCreate ()  GeoSparkRegistrator . registerAll ( spark )   Function  upload_jars ()   uses findspark Python package to upload jar files to executor and nodes. To avoid copying all the time, jar files can be put in directory SPARK_HOME/jars or any other path specified in Spark config files.", 
            "title": "Installing the package"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#installing-from-pypi-repositories", 
            "text": "Please use command below  pip install geospark", 
            "title": "Installing from PyPi repositories"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#installing-from-wheel-file", 
            "text": "pipenv run python -m pip install dist/geospark-1.3.1-py3-none-any.whl  or  pip install dist/geospark-1.3.1-py3-none-any.whl", 
            "title": "Installing from wheel file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#installing-from-source", 
            "text": "python3 setup.py install", 
            "title": "Installing from source"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#geospark-serializers", 
            "text": "GeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.  conf . set ( spark.serializer ,   KryoSerializer . getName )  conf . set ( spark.kryo.registrator ,   GeoSparkKryoRegistrator . getName )", 
            "title": "GeoSpark Serializers"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#create-a-spatialrdd", 
            "text": "", 
            "title": "Create a SpatialRDD"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#create-a-typed-spatialrdd", 
            "text": "GeoSpark-core provides three special SpatialRDDs:  PointRDD    PolygonRDD     LineStringRDD    CircleRDD    RectangleRDD    They can be loaded from CSV, TSV, WKT, WKB, Shapefiles, GeoJSON formats.\nTo pass the format to SpatialRDD constructor please use   FileDataSplitter   enumeration.   geospark SpatialRDDs (and other classes when it was necessary) have implemented meta classes which allow \nto use overloaded functions how Scala/Java GeoSpark API allows. ex.   from   pyspark   import   StorageLevel  from   geospark.core.SpatialRDD   import   PointRDD  from   geospark.core.enums   import   FileDataSplitter  input_location   =   checkin.csv  offset   =   0    # The point long/lat starts from Column 0  splitter   =   FileDataSplitter . CSV   # FileDataSplitter enumeration  carry_other_attributes   =   True    # Carry Column 2 (hotel, gas, bar...)  level   =   StorageLevel . MEMORY_ONLY   # Storage level from pyspark  s_epsg   =   epsg:4326   # Source epsg code  t_epsg   =   epsg:5070   # target epsg code  point_rdd   =   PointRDD ( sc ,   input_location ,   offset ,   splitter ,   carry_other_attributes )  point_rdd   =   PointRDD ( sc ,   input_location ,   splitter ,   carry_other_attributes ,   level ,   s_epsg ,   t_epsg )  point_rdd   =   PointRDD ( \n     sparkContext = sc , \n     InputLocation = input_location , \n     Offset = offset , \n     splitter = splitter , \n     carryInputData = carry_other_attributes  )", 
            "title": "Create a typed SpatialRDD"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#from-sparksql-dataframe", 
            "text": "To create spatialRDD from other formats you can use adapter between Spark DataFrame and SpatialRDD   Load data in GeoSparkSQL.   csv_point_input_location =   /tests/resources/county_small.tsv  df   =   spark . read . \\\n     format ( csv ) . \\\n     option ( delimiter ,   \\t ) . \\\n     option ( header ,   false ) . \\\n     load ( csv_point_input_location )  df . createOrReplaceTempView ( counties )    Create a Geometry type column in GeoSparkSQL   spatial_df   =   spark . sql ( \n              SELECT ST_GeomFromWKT(_c0) as geom, _c6 as county_name          FROM counties        )  spatial_df . printSchema ()   root\n |-- geom: geometry (nullable = false)\n |-- county_name: string (nullable = true)   Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD   Note that, you have to name your column geometry  from   geospark.utils.adapter   import   Adapter  spatial_rdd   =   Adapter . toSpatialRdd ( spatial_df )  spatial_rdd . analyze ()  spatial_rdd . boundaryEnvelope   geospark.core.geom_types.Envelope object at 0x7f1e5f29fe10   or pass Geometry column name as a second argument  spatial_rdd   =   Adapter . toSpatialRdd ( spatial_df ,   geom )   For WKT/WKB/GeoJSON data, please use  ST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON  instead.", 
            "title": "From SparkSQL DataFrame"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#read-other-attributes-in-an-spatialrdd", 
            "text": "Each SpatialRDD can carry non-spatial attributes such as price, age and name as long as the user sets  carryOtherAttributes  as  TRUE .  The other attributes are combined together to a string and stored in  UserData  field of each geometry.  To retrieve the UserData field, use the following code: rdd_with_other_attributes   =   object_rdd . rawSpatialRDD . map ( lambda   x :   x . getUserData ())", 
            "title": "Read other attributes in an SpatialRDD"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#write-a-spatial-range-query", 
            "text": "from   geospark.core.geom.envelope   import   Envelope  from   geospark.core.spatialOperator   import   RangeQuery  range_query_window   =   Envelope ( - 90.01 ,   - 80.01 ,   30.01 ,   40.01 )  consider_boundary_intersection   =   False    ## Only return gemeotries fully covered by the window  using_index   =   False  query_result   =   RangeQuery . SpatialRangeQuery ( spatial_rdd ,   range_query_window ,   consider_boundary_intersection ,   using_index )", 
            "title": "Write a Spatial Range Query"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#range-query-window", 
            "text": "Besides the rectangle (Envelope) type range query window, GeoSpark range query window can be   Point     Polygon    LineString    The code to create a point is as follows:\nTo create shapely geometries please follow official shapely   documentation", 
            "title": "Range query window"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#use-spatial-indexes", 
            "text": "GeoSpark provides two types of spatial indexes,  Quad-Tree    R-Tree  \nOnce you specify an index type, \nGeoSpark will build a local tree index on each of the SpatialRDD partition.  To utilize a spatial index in a spatial range query, use the following code:  from   geospark.core.geom.envelope   import   Envelope  from   geospark.core.enums   import   IndexType  from   geospark.core.spatialOperator   import   RangeQuery  range_query_window   =   Envelope ( - 90.01 ,   - 80.01 ,   30.01 ,   40.01 )  consider_boundary_intersection   =   False   ## Only return gemeotries fully covered by the window  build_on_spatial_partitioned_rdd   =   False   ## Set to TRUE only if run join query  spatial_rdd . buildIndex ( IndexType . QUADTREE ,   build_on_spatial_partitioned_rdd )  using_index   =   True  query_result   =   RangeQuery . SpatialRangeQuery ( \n     spatial_rdd , \n     range_query_window , \n     consider_boundary_intersection , \n     using_index  )", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#output-format", 
            "text": "The output format of the spatial range query is another RDD which consists of GeoData objects.  SpatialRangeQuery result can be used as RDD with map or other spark RDD funtions. Also it can be used as \nPython objects when using collect method.\nExample:  query_result . map ( lambda   x :   x . geom . length ) . collect ()   [\n 1.5900840000000045,\n 1.5906639999999896,\n 1.1110299999999995,\n 1.1096700000000084,\n 1.1415619999999933,\n 1.1386399999999952,\n 1.1415619999999933,\n 1.1418860000000137,\n 1.1392780000000045,\n ...\n]  Or transformed to GeoPandas GeoDataFrame  import   geopandas   as   gpd  gpd . GeoDataFrame ( \n     query_result . map ( lambda   x :   [ x . geom ,   x . userData ]) . collect (), \n     columns = [ geom ,   user_data ], \n     geometry = geom  )", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#write-a-spatial-knn-query", 
            "text": "A spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.  Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.  from   geospark.core.spatialOperator   import   KNNQuery  from   shapely.geometry   import   Point  point   =   Point ( - 84.01 ,   34.01 )  k   =   1000   ## K Nearest Neighbors  using_index   =   False  result   =   KNNQuery . SpatialKnnQuery ( object_rdd ,   point ,   k ,   using_index )", 
            "title": "Write a Spatial KNN Query"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#query-center-geometry", 
            "text": "Besides the Point type, GeoSpark KNN query center can be   Polygon    LineString   To create Polygon or Linestring object please follow Shapely official   documentation", 
            "title": "Query center geometry"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#use-spatial-indexes_1", 
            "text": "To utilize a spatial index in a spatial KNN query, use the following code:  from   geospark.core.spatialOperator   import   KNNQuery  from   geospark.core.enums   import   IndexType  from   shapely.geometry   import   Point  point   =   Point ( - 84.01 ,   34.01 )  k   =   5   ## K Nearest Neighbors  build_on_spatial_partitioned_rdd   =   False   ## Set to TRUE only if run join query  spatial_rdd . buildIndex ( IndexType . RTREE ,   build_on_spatial_partitioned_rdd )  using_index   =   True  result   =   KNNQuery . SpatialKnnQuery ( spatial_rdd ,   point ,   k ,   using_index )    Warning  Only R-Tree index supports Spatial KNN query", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#output-format_1", 
            "text": "The output format of the spatial KNN query is a list of GeoData objects. \nThe list has K GeoData objects.  Example:   result  [ GeoData ,   GeoData ,   GeoData ,   GeoData ,   GeoData ]", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#write-a-spatial-join-query", 
            "text": "A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.  Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.  from   geospark.core.enums   import   GridType  from   geospark.core.spatialOperator   import   JoinQuery  consider_boundary_intersection   =   False   ## Only return geometries fully covered by each query window in queryWindowRDD  using_index   =   False  object_rdd . analyze ()  object_rdd . spatialPartitioning ( GridType . KDBTREE )  query_window_rdd . spatialPartitioning ( object_rdd . getPartitioner ())  result   =   JoinQuery . SpatialJoinQuery ( object_rdd ,   query_window_rdd ,   using_index ,   consider_boundary_intersection )   Result of SpatialJoinQuery is RDD which consists of GeoData instance and list of GeoData instances which spatially intersects or \nare covered by GeoData.   result . collect ())   [\n    [GeoData, [GeoData, GeoData, GeoData, GeoData]],\n    [GeoData, [GeoData, GeoData, GeoData]],\n    [GeoData, [GeoData]],\n    [GeoData, [GeoData, GeoData]],\n    ...\n    [GeoData, [GeoData, GeoData]]\n]", 
            "title": "Write a Spatial Join Query"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#use-spatial-partitioning", 
            "text": "GeoSpark spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.  If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.  object_rdd . spatialPartitioning ( GridType . KDBTREE )  query_window_rdd . spatialPartitioning ( object_rdd . getPartitioner ())   Or   query_window_rdd . spatialPartitioning ( GridType . KDBTREE )  object_rdd . spatialPartitioning ( query_window_rdd . getPartitioner ())", 
            "title": "Use spatial partitioning"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#use-spatial-indexes_2", 
            "text": "To utilize a spatial index in a spatial join query, use the following code:  from   geospark.core.enums   import   GridType  from   geospark.core.enums   import   IndexType  from   geospark.core.spatialOperator   import   JoinQuery  object_rdd . spatialPartitioning ( GridType . KDBTREE )  query_window_rdd . spatialPartitioning ( object_rdd . getPartitioner ())  build_on_spatial_partitioned_rdd   =   True   ## Set to TRUE only if run join query  using_index   =   True  query_window_rdd . buildIndex ( IndexType . QUADTREE ,   build_on_spatial_partitioned_rdd )  result   =   JoinQuery . SpatialJoinQueryFlat ( object_rdd ,   query_window_rdd ,   using_index ,   True )   The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#output-format_2", 
            "text": "The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two GeoData objects.\nThe left one is the GeoData from object_rdd and the right one is the GeoData from the query_window_rdd.  Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...  example  result . collect ()   [\n [GeoData, GeoData],\n [GeoData, GeoData],\n [GeoData, GeoData],\n [GeoData, GeoData],\n ...\n [GeoData, GeoData],\n [GeoData, GeoData]\n]  Each object on the left is covered/intersected by the object on the right.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#write-a-distance-join-query", 
            "text": "A distance join query takes two spatial RDD assuming that we have two SpatialRDD's:  object_rdd    spatial_rdd   And finds the geometries (from spatial_rdd) are within given distance to it. spatial_rdd and object_rdd\ncan be any geometry type (point, line, polygon) and are not necessary to have the same geometry type  You can use the following code to issue an Distance Join Query on them.  from   geospark.core.SpatialRDD   import   CircleRDD  from   geospark.core.enums   import   GridType  from   geospark.core.spatialOperator   import   JoinQuery  object_rdd . analyze ()  circle_rdd   =   CircleRDD ( object_rdd ,   0.1 )   ## Create a CircleRDD using the given distance  circle_rdd . analyze ()  circle_rdd . spatialPartitioning ( GridType . KDBTREE )  spatial_rdd . spatialPartitioning ( circle_rdd . getPartitioner ())  consider_boundary_intersection   =   False   ## Only return gemeotries fully covered by each query window in queryWindowRDD  using_index   =   False  result   =   JoinQuery . DistanceJoinQueryFlat ( spatial_rdd ,   circle_rdd ,   using_index ,   consider_boundary_intersection )", 
            "title": "Write a Distance Join Query"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#output-format_3", 
            "text": "Result for this query is RDD which holds two GeoData objects within list of lists.\nExample: result . collect ()   [[GeoData, GeoData], [GeoData, GeoData] ...]  It is possible to do some RDD operation on result data ex. Getting polygon centroid. result . map ( lambda   x :   x [ 0 ] . geom . centroid ) . collect ()   [\n  shapely.geometry.point.Point at 0x7efee2d28128 ,\n  shapely.geometry.point.Point at 0x7efee2d280b8 ,\n  shapely.geometry.point.Point at 0x7efee2d28fd0 ,\n  shapely.geometry.point.Point at 0x7efee2d28080 ,\n ...\n]", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-to-permanent-storage", 
            "text": "You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.   Note  Non-spatial attributes such as price, age and name will also be stored to permanent storage.", 
            "title": "Save to permanent storage"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-an-spatialrdd-not-indexed", 
            "text": "Typed SpatialRDD and generic SpatialRDD can be saved to permanent storage.", 
            "title": "Save an SpatialRDD (not indexed)"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-to-distributed-wkt-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed WKT text file:  object_rdd . rawSpatialRDD . saveAsTextFile ( hdfs://PATH )  object_rdd . saveAsWKT ( hdfs://PATH )", 
            "title": "Save to distributed WKT text file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-to-distributed-wkb-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed WKB text file:  object_rdd . saveAsWKB ( hdfs://PATH )", 
            "title": "Save to distributed WKB text file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-to-distributed-geojson-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed GeoJSON text file:  object_rdd . saveAsGeoJSON ( hdfs://PATH )", 
            "title": "Save to distributed GeoJSON text file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-to-distributed-object-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed object file:  object_rdd . rawJvmSpatialRDD . saveAsObjectFile ( hdfs://PATH )    Note  Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.", 
            "title": "Save to distributed object file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-an-spatialrdd-indexed", 
            "text": "Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.", 
            "title": "Save an SpatialRDD (indexed)"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-to-distributed-object-file_1", 
            "text": "Use the following code to save an SpatialRDD as a distributed object file:  object_rdd . indexedRawRDD . saveAsObjectFile ( hdfs://PATH )", 
            "title": "Save to distributed object file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#save-an-spatialrdd-spatialpartitioned-wo-indexed", 
            "text": "A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!", 
            "title": "Save an SpatialRDD (spatialPartitioned W/O indexed)"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#reload-a-saved-spatialrdd", 
            "text": "You can easily reload an SpatialRDD that has been saved to  a distributed object file .", 
            "title": "Reload a saved SpatialRDD"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#load-to-a-typed-spatialrdd", 
            "text": "Use the following code to reload the PointRDD/PolygonRDD/LineStringRDD:  from   geospark.core.formatMapper.disc_utils   import   load_spatial_rdd_from_disc ,   GeoType  polygon_rdd   =   load_spatial_rdd_from_disc ( sc ,   hdfs://PATH ,   GeoType . POLYGON )  point_rdd   =   load_spatial_rdd_from_disc ( sc ,   hdfs://PATH ,   GeoType . POINT )  linestring_rdd   =   load_spatial_rdd_from_disc ( sc ,   hdfs://PATH ,   GeoType . LINESTRING )", 
            "title": "Load to a typed SpatialRDD"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#load-to-a-generic-spatialrdd", 
            "text": "Use the following code to reload the SpatialRDD:  saved_rdd   =   load_spatial_rdd_from_disc ( sc ,   hdfs://PATH ,   GeoType . GEOMETRY )   Use the following code to reload the indexed SpatialRDD: saved_rdd   =   SpatialRDD ()  saved_rdd . indexedRawRDD   =   load_spatial_index_rdd_from_disc ( sc ,   hdfs://PATH )", 
            "title": "Load to a generic SpatialRDD"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#read-from-other-geometry-files", 
            "text": "All below methods will return SpatialRDD object which can be used with Spatial functions such as Spatial Join etc.", 
            "title": "Read from other Geometry files"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#read-from-wkt-file", 
            "text": "from   geospark.core.formatMapper   import   WktReader  WktReader . readToGeometryRDD ( sc ,   wkt_geometries_location ,   0 ,   True ,   False )   geospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2fbf250", 
            "title": "Read from WKT file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#read-from-wkb-file", 
            "text": "from   geospark.core.formatMapper   import   WkbReader  WkbReader . readToGeometryRDD ( sc ,   wkb_geometries_location ,   0 ,   True ,   False )   geospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2eece50", 
            "title": "Read from WKB file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#read-from-geojson-file", 
            "text": "from   geospark.core.formatMapper   import   GeoJsonReader  GeoJsonReader . readToGeometryRDD ( sc ,   geo_json_file_location )   geospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2eecb90", 
            "title": "Read from GeoJson file"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#read-from-shapefile", 
            "text": "from   geospark.core.formatMapper.shapefileParser   import   ShapefileReader  ShapefileReader . readToGeometryRDD ( sc ,   shape_file_location )   geospark.core.SpatialRDD.spatial_rdd.SpatialRDD at 0x7f8fd2ee0710", 
            "title": "Read from Shapefile"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#supported-versions", 
            "text": "Currently this python wrapper supports the following Spark, GeoSpark and Python versions", 
            "title": "Supported versions"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#apache-spark", 
            "text": "2.2    2.3    2.4", 
            "title": "Apache Spark"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#geospark", 
            "text": "1.3.1    1.2.0", 
            "title": "GeoSpark"
        }, 
        {
            "location": "/tutorial/geospark-core-python/#python", 
            "text": "3.6    3.7    Note  Other versions may also work (or partially) but were not tested yet", 
            "title": "Python"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/", 
            "text": "Spatial SQL Application in Python\n\n\nIntroduction\n\n\nGeoSPark provides a Python wrapper for its Spatial SQL / DataFrame interface. The official repository for GeoSpark can be found at \nhttps://github.com/DataSystemsLab/GeoSpark\n.\n\n\nThis package allows users to use all GeoSparkSQL functions and transform it to Python Shapely geometry objects. Also it allows to create Spark DataFrame with GeoSpark UDT from Shapely geometry objects. Spark DataFrame can be converted to GeoPandas easily, in addition all fiona drivers for shape file are available to load data from files and convert them to Spark DataFrame. Please look at examples.\n\n\nInstallation\n\n\nGeoSpark extends pyspark functions which depends on Python packages and Scala libraries. To see all dependencies\nplease look at Dependencies section.\n\nhttps://pypi.org/project/pyspark/\n.\n\n\nPackage needs 2 jar files to work properly:\n\n\n\n\ngeospark.jar\n\n\ngeospark-sql.jar\n\n\ngeo_wrapper.jar\n\n\n\n\n\n\nNote\n\n\nSince GeoSpark 1.3.0 it is possible also to use maven jars for GeoSparkSQL instead of geospark/jars/../geospark-sql jars files.\n\n\n\n\nThis package automatically copies the newest GeoSpark jar files using function, please follow the example below.\n\n\n upload_jars \n\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\nfrom\n \ngeospark.register\n \nimport\n \nupload_jars\n\n\nfrom\n \ngeospark.register\n \nimport\n \nGeoSparkRegistrator\n\n\n\nupload_jars\n()\n\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\n\\\n      \ngetOrCreate\n()\n\n\n\nGeoSparkRegistrator\n.\nregisterAll\n(\nspark\n)\n\n\n\n\n\nFunction\n\n\nupload_jars\n()\n\n\n\n\n\nuses findspark Python package to upload jar files to executor and nodes. To avoid copying all the time, jar files can be put in directory SPARK_HOME/jars or any other path specified in Spark config files.\n\n\nInstalling from PyPi repositories\n\n\nPlease use command below\n\n\npip install geospark\n\n\n\n\nInstalling from wheel file\n\n\npipenv run python -m pip install dist/geospark-1.3.1-py3-none-any.whl\n\n\n\n\nor\n\n\npip install dist/geospark-1.3.1-py3-none-any.whl\n\n\n\n\nInstalling from source\n\n\npython3 setup.py install\n\n\n\n\nCore Classes and methods.\n\n\nGeoSparkRegistrator.registerAll(spark: pyspark.sql.SparkSession) -\n bool\n\n\nThis is the core of whole package. Class method registers all GeoSparkSQL functions (available for used GeoSparkSQL version).\nTo check available functions please look at GeoSparkSQL section.\n:param spark: pyspark.sql.SparkSession, spark session instance\n\n\nupload_jars() -\n NoReturn\n\n\nFunction uses \nfindspark\n Python module to upload newest GeoSpark jars to Spark executor and nodes.\n\n\nGeometryType()\n\n\nClass which handle serialization and deserialization between GeoSpark geometries and Shapely BaseGeometry types.\n\n\nKryoSerializer.getName -\n str\n\nClass property which returns org.apache.spark.serializer.KryoSerializer string, which simplify using GeoSpark Serializers.\n\n\nGeoSparkKryoRegistrator.getName -\n str\n\nClass property which returns org.datasyslab.geospark.serde.GeoSparkKryoRegistrator string, which simplify using GeoSpark Serializers.\n\n\nWriting Application\n\n\nUse KryoSerializer.getName and GeoSparkKryoRegistrator.getName class properties to reduce memory impact, reffering to  \n GeoSpark docs \n. To do that use spark config as follows:\n\n\n.\nconfig\n(\nspark.serializer\n,\n \nKryoSerializer\n.\ngetName\n)\n\n\n.\nconfig\n(\nspark.kryo.registrator\n,\n \nGeoSparkKryoRegistrator\n.\ngetName\n)\n\n\n\n\n\nIf jars was not uploaded manually please use function \nupload_jars()\n\n\nTo turn on GeoSparkSQL function inside pyspark code use GeoSparkRegistrator.registerAll method on existing pyspark.sql.SparkSession instance ex.\n\n\nGeoSparkRegistrator.registerAll(spark)\n\n\nAfter that all the functions from GeoSparkSQL will be available, moreover using collect or toPandas methods on Spark DataFrame will return Shapely BaseGeometry objects. Based on GeoPandas DataFrame, Pandas DataFrame with shapely objects or Sequence with shapely objects, Spark DataFrame can be created using spark.createDataFrame method. To specify Schema with geometry inside please use \nGeometryType()\n instance (look at examples section to see that in practice).\n\n\nExamples\n\n\nGeoSparkSQL\n\n\nAll GeoSparkSQL functions (list depends on GeoSparkSQL version) are available in Python API. For documentation please look at \n GeoSpark website\n\n\nFor example use GeoSparkSQL for Spatial Join.\n\n\ncounties\n \n=\n \nspark\n.\n\\\n    \nread\n.\n\\\n    \noption\n(\ndelimiter\n,\n \n|\n)\n.\n\\\n    \noption\n(\nheader\n,\n \ntrue\n)\n.\n\\\n    \ncsv\n(\ncounties.csv\n)\n\n\n\ncounties\n.\ncreateOrReplaceTempView\n(\ncounty\n)\n\n\n\ncounties_geom\n \n=\n \nspark\n.\nsql\n(\n\n      \nSELECT county_code, st_geomFromWKT(geom) as geometry from county\n\n\n)\n\n\n\ncounties_geom\n.\nshow\n(\n5\n)\n\n\n\n\n+-----------+--------------------+\n|county_code|            geometry|\n+-----------+--------------------+\n|       1815|POLYGON ((21.6942...|\n|       1410|POLYGON ((22.7238...|\n|       1418|POLYGON ((21.1100...|\n|       1425|POLYGON ((20.9891...|\n|       1427|POLYGON ((19.5087...|\n+-----------+--------------------+\n\n\n\nimport\n \ngeopandas\n \nas\n \ngpd\n\n\n\npoints\n \n=\n \ngpd\n.\nread_file\n(\ngis_osm_pois_free_1.shp\n)\n\n\n\npoints_geom\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \npoints\n[[\nfclass\n,\n \ngeometry\n]]\n\n\n)\n\n\n\npoints_geom\n.\nshow\n(\n5\n,\n \nFalse\n)\n\n\n\n\n+---------+-----------------------------+\n|fclass   |geometry                     |\n+---------+-----------------------------+\n|camp_site|POINT (15.3393145 52.3504247)|\n|chalet   |POINT (14.8709625 52.691693) |\n|motel    |POINT (15.0946636 52.3130396)|\n|atm      |POINT (15.0732014 52.3141083)|\n|hotel    |POINT (15.0696777 52.3143013)|\n+---------+-----------------------------+\n\n\n\npoints_geom\n.\ncreateOrReplaceTempView\n(\npois\n)\n\n\ncounties_geom\n.\ncreateOrReplaceTempView\n(\ncounties\n)\n\n\n\nspatial_join_result\n \n=\n \nspark\n.\nsql\n(\n\n    \n\n\n        SELECT c.county_code, p.fclass\n\n\n        FROM pois AS p, counties AS c\n\n\n        WHERE ST_Intersects(p.geometry, c.geometry)\n\n\n    \n\n\n)\n\n\n\nspatial_join_result\n.\nexplain\n()\n\n\n\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\n*\n(\n2\n)\n \nProject\n \n[\ncounty_code#\n230\n,\n \nfclass#\n239\n]\n\n\n+-\n \nRangeJoin\n \ngeometry#\n240\n:\n \ngeometry\n,\n \ngeometry#\n236\n:\n \ngeometry\n,\n \ntrue\n\n   \n:-\n \nScan\n \nExistingRDD\n[\nfclass#\n239\n,\ngeometry#\n240\n]\n\n   \n+-\n \nProject\n \n[\ncounty_code#\n230\n,\n \nst_geomfromwkt\n(\ngeom#\n232\n)\n \nAS\n \ngeometry#\n236\n]\n\n      \n+-\n \n*\n(\n1\n)\n \nFileScan\n \ncsv\n \n[\ncounty_code#\n230\n,\ngeom#\n232\n]\n \nBatched\n:\n \nfalse\n,\n \nFormat\n:\n \nCSV\n,\n \nLocation\n:\n \nInMemoryFileIndex\n[\nfile\n:\n/\nprojects\n/\ngeospark\n/\ncounties\n.\ncsv\n],\n \nPartitionFilters\n:\n \n[],\n \nPushedFilters\n:\n \n[],\n \nReadSchema\n:\n \nstruct\ncounty_code:string\n,\ngeom\n:\nstring\n\n\n\nCalculating Number of Pois within counties per fclass.\n\n\npois_per_county\n \n=\n \nspatial_join_result\n.\ngroupBy\n(\ncounty_code\n,\n \nfclass\n)\n.\n \\\n    \ncount\n()\n\n\n\npois_per_county\n.\nshow\n(\n5\n,\n \nFalse\n)\n\n\n\n\n+-----------+---------+-----+\n|county_code|fclass   |count|\n+-----------+---------+-----+\n|0805       |atm      |6    |\n|0805       |bench    |75   |\n|0803       |museum   |9    |\n|0802       |fast_food|5    |\n|0862       |atm      |20   |\n+-----------+---------+-----+\n\n\n\nIntegration with GeoPandas and Shapely\n\n\ngeospark has implemented serializers and deserializers which allows to convert GeoSpark Geometry objects into Shapely BaseGeometry objects. Based on that it is possible to load the data with geopandas from file (look at Fiona possible drivers) and create Spark DataFrame based on GeoDataFrame object.\n\n\nExample, loading the data from shapefile using geopandas read_file method and create Spark DataFrame based on GeoDataFrame:\n\n\nimport\n \ngeopandas\n \nas\n \ngpd\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\nfrom\n \ngeospark.register\n \nimport\n \nGeoSparkRegistrator\n\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\n\\\n      \ngetOrCreate\n()\n\n\n\nGeoSparkRegistrator\n.\nregisterAll\n(\nspark\n)\n\n\n\ngdf\n \n=\n \ngpd\n.\nread_file\n(\ngis_osm_pois_free_1.shp\n)\n\n\n\nspark\n.\ncreateDataFrame\n(\n\n  \ngdf\n\n\n)\n.\nshow\n()\n\n\n\n\n\n+---------+----+-----------+--------------------+--------------------+\n|   osm_id|code|     fclass|                name|            geometry|\n+---------+----+-----------+--------------------+--------------------+\n| 26860257|2422|  camp_site|            de Kroon|POINT (15.3393145...|\n| 26860294|2406|     chalet|      Le\u015bne Ustronie|POINT (14.8709625...|\n| 29947493|2402|      motel|                null|POINT (15.0946636...|\n| 29947498|2602|        atm|                null|POINT (15.0732014...|\n| 29947499|2401|      hotel|                null|POINT (15.0696777...|\n| 29947505|2401|      hotel|                null|POINT (15.0155749...|\n+---------+----+-----------+--------------------+--------------------+\n\n\n\n\nReading data with Spark and converting to GeoPandas\n\n\nimport\n \ngeopandas\n \nas\n \ngpd\n\n\nfrom\n \npyspark.sql\n \nimport\n \nSparkSession\n\n\n\nfrom\n \ngeospark.register\n \nimport\n \nGeoSparkRegistrator\n\n\n\nspark\n \n=\n \nSparkSession\n.\nbuilder\n.\n\\\n    \ngetOrCreate\n()\n\n\n\nGeoSparkRegistrator\n.\nregisterAll\n(\nspark\n)\n\n\n\ncounties\n \n=\n \nspark\n.\n\\\n    \nread\n.\n\\\n    \noption\n(\ndelimiter\n,\n \n|\n)\n.\n\\\n    \noption\n(\nheader\n,\n \ntrue\n)\n.\n\\\n    \ncsv\n(\ncounties.csv\n)\n\n\n\ncounties\n.\ncreateOrReplaceTempView\n(\ncounty\n)\n\n\n\ncounties_geom\n \n=\n \nspark\n.\nsql\n(\n\n    \nSELECT *, st_geomFromWKT(geom) as geometry from county\n\n\n)\n\n\n\ndf\n \n=\n \ncounties_geom\n.\ntoPandas\n()\n\n\ngdf\n \n=\n \ngpd\n.\nGeoDataFrame\n(\ndf\n,\n \ngeometry\n=\ngeometry\n)\n\n\n\ngdf\n.\nplot\n(\n\n    \nfigsize\n=\n(\n10\n,\n \n8\n),\n\n    \ncolumn\n=\nvalue\n,\n\n    \nlegend\n=\nTrue\n,\n\n    \ncmap\n=\nYlOrBr\n,\n\n    \nscheme\n=\nquantiles\n,\n\n    \nedgecolor\n=\nlightgray\n\n\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Spark DataFrame based on shapely objects\n\n\nSupported Shapely objects\n\n\n\n\n\n\n\n\nshapely object\n\n\nAvailable\n\n\n\n\n\n\n\n\n\n\nPoint\n\n\n\n\n\n\n\n\nMultiPoint\n\n\n\n\n\n\n\n\nLineString\n\n\n\n\n\n\n\n\nMultiLinestring\n\n\n\n\n\n\n\n\nPolygon\n\n\n\n\n\n\n\n\nMultiPolygon\n\n\n\n\n\n\n\n\n\n\nTo create Spark DataFrame based on mentioned Geometry types, please use \n GeometryType \n from  \n geospark.sql.types \n module. Converting works for list or tuple with shapely objects.\n\n\nSchema for target table with integer id and geometry type can be defined as follow:\n\n\nfrom\n \npyspark.sql.types\n \nimport\n \nIntegerType\n,\n \nStructField\n,\n \nStructType\n\n\n\nfrom\n \ngeospark.sql.types\n \nimport\n \nGeometryType\n\n\n\nschema\n \n=\n \nStructType\n(\n\n    \n[\n\n        \nStructField\n(\nid\n,\n \nIntegerType\n(),\n \nFalse\n),\n\n        \nStructField\n(\ngeom\n,\n \nGeometryType\n(),\n \nFalse\n)\n\n    \n]\n\n\n)\n\n\n\n\n\nAlso Spark DataFrame with geometry type can be converted to list of shapely objects with \n collect \n method.\n\n\nExample usage for Shapely objects\n\n\nPoint\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nPoint\n\n\n\ndata\n \n=\n \n[\n\n    \n[\n1\n,\n \nPoint\n(\n21.0\n,\n \n52.0\n)],\n\n    \n[\n1\n,\n \nPoint\n(\n23.0\n,\n \n42.0\n)],\n\n    \n[\n1\n,\n \nPoint\n(\n26.0\n,\n \n32.0\n)]\n\n\n]\n\n\n\n\ngdf\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \ndata\n,\n\n    \nschema\n\n\n)\n\n\n\ngdf\n.\nshow\n()\n\n\n\n\n\n+---+-------------+\n| id|         geom|\n+---+-------------+\n|  1|POINT (21 52)|\n|  1|POINT (23 42)|\n|  1|POINT (26 32)|\n+---+-------------+\n\n\n\n\ngdf\n.\nprintSchema\n()\n\n\n\n\n\nroot\n |-- id: integer (nullable = false)\n |-- geom: geometry (nullable = false)\n\n\n\n\nMultiPoint\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nMultiPoint\n\n\n\ndata\n \n=\n \n[\n\n    \n[\n1\n,\n \nMultiPoint\n([[\n19.511463\n,\n \n51.765158\n],\n \n[\n19.446408\n,\n \n51.779752\n]])]\n\n\n]\n\n\n\ngdf\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \ndata\n,\n\n    \nschema\n\n\n)\n.\nshow\n(\n1\n,\n \nFalse\n)\n\n\n\n\n\n+---+---------------------------------------------------------+\n|id |geom                                                     |\n+---+---------------------------------------------------------+\n|1  |MULTIPOINT ((19.511463 51.765158), (19.446408 51.779752))|\n+---+---------------------------------------------------------+\n\n\n\n\nLineString\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nLineString\n\n\n\nline\n \n=\n \n[(\n40\n,\n \n40\n),\n \n(\n30\n,\n \n30\n),\n \n(\n40\n,\n \n20\n),\n \n(\n30\n,\n \n10\n)]\n\n\n\ndata\n \n=\n \n[\n\n    \n[\n1\n,\n \nLineString\n(\nline\n)]\n\n\n]\n\n\n\ngdf\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \ndata\n,\n\n    \nschema\n\n\n)\n\n\n\ngdf\n.\nshow\n(\n1\n,\n \nFalse\n)\n\n\n\n\n\n+---+--------------------------------+\n|id |geom                            |\n+---+--------------------------------+\n|1  |LINESTRING (10 10, 20 20, 10 40)|\n+---+--------------------------------+\n\n\n\n\nMultiLineString\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nMultiLineString\n\n\n\nline1\n \n=\n \n[(\n10\n,\n \n10\n),\n \n(\n20\n,\n \n20\n),\n \n(\n10\n,\n \n40\n)]\n\n\nline2\n \n=\n \n[(\n40\n,\n \n40\n),\n \n(\n30\n,\n \n30\n),\n \n(\n40\n,\n \n20\n),\n \n(\n30\n,\n \n10\n)]\n\n\n\ndata\n \n=\n \n[\n\n    \n[\n1\n,\n \nMultiLineString\n([\nline1\n,\n \nline2\n])]\n\n\n]\n\n\n\ngdf\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \ndata\n,\n\n    \nschema\n\n\n)\n\n\n\ngdf\n.\nshow\n(\n1\n,\n \nFalse\n)\n\n\n\n\n\n+---+---------------------------------------------------------------------+\n|id |geom                                                                 |\n+---+---------------------------------------------------------------------+\n|1  |MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))|\n+---+---------------------------------------------------------------------+\n\n\n\n\nPolygon\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nPolygon\n\n\n\npolygon\n \n=\n \nPolygon\n(\n\n    \n[\n\n         \n[\n19.51121\n,\n \n51.76426\n],\n\n         \n[\n19.51056\n,\n \n51.76583\n],\n\n         \n[\n19.51216\n,\n \n51.76599\n],\n\n         \n[\n19.51280\n,\n \n51.76448\n],\n\n         \n[\n19.51121\n,\n \n51.76426\n]\n\n    \n]\n\n\n)\n\n\n\ndata\n \n=\n \n[\n\n    \n[\n1\n,\n \npolygon\n]\n\n\n]\n\n\n\ngdf\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \ndata\n,\n\n    \nschema\n\n\n)\n\n\n\ngdf\n.\nshow\n(\n1\n,\n \nFalse\n)\n\n\n\n\n\n+---+--------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                    |\n+---+--------------------------------------------------------------------------------------------------------+\n|1  |POLYGON ((19.51121 51.76426, 19.51056 51.76583, 19.51216 51.76599, 19.5128 51.76448, 19.51121 51.76426))|\n+---+--------------------------------------------------------------------------------------------------------+\n\n\n\n\nMultiPolygon\n\n\nfrom\n \nshapely.geometry\n \nimport\n \nMultiPolygon\n\n\n\nexterior_p1\n \n=\n \n[(\n0\n,\n \n0\n),\n \n(\n0\n,\n \n2\n),\n \n(\n2\n,\n \n2\n),\n \n(\n2\n,\n \n0\n),\n \n(\n0\n,\n \n0\n)]\n\n\ninterior_p1\n \n=\n \n[(\n1\n,\n \n1\n),\n \n(\n1\n,\n \n1.5\n),\n \n(\n1.5\n,\n \n1.5\n),\n \n(\n1.5\n,\n \n1\n),\n \n(\n1\n,\n \n1\n)]\n\n\n\nexterior_p2\n \n=\n \n[(\n0\n,\n \n0\n),\n \n(\n1\n,\n \n0\n),\n \n(\n1\n,\n \n1\n),\n \n(\n0\n,\n \n1\n),\n \n(\n0\n,\n \n0\n)]\n\n\n\npolygons\n \n=\n \n[\n\n    \nPolygon\n(\nexterior_p1\n,\n \n[\ninterior_p1\n]),\n\n    \nPolygon\n(\nexterior_p2\n)\n\n\n]\n\n\n\ndata\n \n=\n \n[\n\n    \n[\n1\n,\n \nMultiPolygon\n(\npolygons\n)]\n\n\n]\n\n\n\ngdf\n \n=\n \nspark\n.\ncreateDataFrame\n(\n\n    \ndata\n,\n\n    \nschema\n\n\n)\n\n\n\ngdf\n.\nshow\n(\n1\n,\n \nFalse\n)\n\n\n\n\n\n+---+----------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------+\n|1  |MULTIPOLYGON (((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1.5 1, 1.5 1.5, 1 1.5, 1 1)), ((0 0, 0 1, 1 1, 1 0, 0 0)))|\n+---+----------------------------------------------------------------------------------------------------------+\n\n\n\n\nSupported versions\n\n\nCurrently this python wrapper supports the following Spark, GeoSpark and Python versions:\n\n\nApache Spark\n\n\n 2.2 \n\n\n\n 2.3 \n\n\n\n 2.4 \n\n\n\nGeoSparkSQL\n\n\n 1.3.1 \n\n\n\n 1.2.0 \n\n\n\n 1.1.3 \n\n\n\nPython\n\n\n 3.6 \n\n\n\n 3.7", 
            "title": "Spatial SQL in Python"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#spatial-sql-application-in-python", 
            "text": "", 
            "title": "Spatial SQL Application in Python"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#introduction", 
            "text": "GeoSPark provides a Python wrapper for its Spatial SQL / DataFrame interface. The official repository for GeoSpark can be found at  https://github.com/DataSystemsLab/GeoSpark .  This package allows users to use all GeoSparkSQL functions and transform it to Python Shapely geometry objects. Also it allows to create Spark DataFrame with GeoSpark UDT from Shapely geometry objects. Spark DataFrame can be converted to GeoPandas easily, in addition all fiona drivers for shape file are available to load data from files and convert them to Spark DataFrame. Please look at examples.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#installation", 
            "text": "GeoSpark extends pyspark functions which depends on Python packages and Scala libraries. To see all dependencies\nplease look at Dependencies section. https://pypi.org/project/pyspark/ .  Package needs 2 jar files to work properly:   geospark.jar  geospark-sql.jar  geo_wrapper.jar    Note  Since GeoSpark 1.3.0 it is possible also to use maven jars for GeoSparkSQL instead of geospark/jars/../geospark-sql jars files.   This package automatically copies the newest GeoSpark jar files using function, please follow the example below.   upload_jars   from   pyspark.sql   import   SparkSession  from   geospark.register   import   upload_jars  from   geospark.register   import   GeoSparkRegistrator  upload_jars ()  spark   =   SparkSession . builder . \\\n       getOrCreate ()  GeoSparkRegistrator . registerAll ( spark )   Function  upload_jars ()   uses findspark Python package to upload jar files to executor and nodes. To avoid copying all the time, jar files can be put in directory SPARK_HOME/jars or any other path specified in Spark config files.", 
            "title": "Installation"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#installing-from-pypi-repositories", 
            "text": "Please use command below  pip install geospark", 
            "title": "Installing from PyPi repositories"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#installing-from-wheel-file", 
            "text": "pipenv run python -m pip install dist/geospark-1.3.1-py3-none-any.whl  or  pip install dist/geospark-1.3.1-py3-none-any.whl", 
            "title": "Installing from wheel file"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#installing-from-source", 
            "text": "python3 setup.py install", 
            "title": "Installing from source"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#core-classes-and-methods", 
            "text": "GeoSparkRegistrator.registerAll(spark: pyspark.sql.SparkSession) -  bool  This is the core of whole package. Class method registers all GeoSparkSQL functions (available for used GeoSparkSQL version).\nTo check available functions please look at GeoSparkSQL section.\n:param spark: pyspark.sql.SparkSession, spark session instance  upload_jars() -  NoReturn  Function uses  findspark  Python module to upload newest GeoSpark jars to Spark executor and nodes.  GeometryType()  Class which handle serialization and deserialization between GeoSpark geometries and Shapely BaseGeometry types.  KryoSerializer.getName -  str \nClass property which returns org.apache.spark.serializer.KryoSerializer string, which simplify using GeoSpark Serializers.  GeoSparkKryoRegistrator.getName -  str \nClass property which returns org.datasyslab.geospark.serde.GeoSparkKryoRegistrator string, which simplify using GeoSpark Serializers.", 
            "title": "Core Classes and methods."
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#writing-application", 
            "text": "Use KryoSerializer.getName and GeoSparkKryoRegistrator.getName class properties to reduce memory impact, reffering to    GeoSpark docs  . To do that use spark config as follows:  . config ( spark.serializer ,   KryoSerializer . getName )  . config ( spark.kryo.registrator ,   GeoSparkKryoRegistrator . getName )   If jars was not uploaded manually please use function  upload_jars()  To turn on GeoSparkSQL function inside pyspark code use GeoSparkRegistrator.registerAll method on existing pyspark.sql.SparkSession instance ex.  GeoSparkRegistrator.registerAll(spark)  After that all the functions from GeoSparkSQL will be available, moreover using collect or toPandas methods on Spark DataFrame will return Shapely BaseGeometry objects. Based on GeoPandas DataFrame, Pandas DataFrame with shapely objects or Sequence with shapely objects, Spark DataFrame can be created using spark.createDataFrame method. To specify Schema with geometry inside please use  GeometryType()  instance (look at examples section to see that in practice).", 
            "title": "Writing Application"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#geosparksql", 
            "text": "All GeoSparkSQL functions (list depends on GeoSparkSQL version) are available in Python API. For documentation please look at   GeoSpark website  For example use GeoSparkSQL for Spatial Join.  counties   =   spark . \\\n     read . \\\n     option ( delimiter ,   | ) . \\\n     option ( header ,   true ) . \\\n     csv ( counties.csv )  counties . createOrReplaceTempView ( county )  counties_geom   =   spark . sql ( \n       SELECT county_code, st_geomFromWKT(geom) as geometry from county  )  counties_geom . show ( 5 )   +-----------+--------------------+\n|county_code|            geometry|\n+-----------+--------------------+\n|       1815|POLYGON ((21.6942...|\n|       1410|POLYGON ((22.7238...|\n|       1418|POLYGON ((21.1100...|\n|       1425|POLYGON ((20.9891...|\n|       1427|POLYGON ((19.5087...|\n+-----------+--------------------+  import   geopandas   as   gpd  points   =   gpd . read_file ( gis_osm_pois_free_1.shp )  points_geom   =   spark . createDataFrame ( \n     points [[ fclass ,   geometry ]]  )  points_geom . show ( 5 ,   False )   +---------+-----------------------------+\n|fclass   |geometry                     |\n+---------+-----------------------------+\n|camp_site|POINT (15.3393145 52.3504247)|\n|chalet   |POINT (14.8709625 52.691693) |\n|motel    |POINT (15.0946636 52.3130396)|\n|atm      |POINT (15.0732014 52.3141083)|\n|hotel    |POINT (15.0696777 52.3143013)|\n+---------+-----------------------------+  points_geom . createOrReplaceTempView ( pois )  counties_geom . createOrReplaceTempView ( counties )  spatial_join_result   =   spark . sql ( \n              SELECT c.county_code, p.fclass          FROM pois AS p, counties AS c          WHERE ST_Intersects(p.geometry, c.geometry)        )  spatial_join_result . explain ()   ==   Physical   Plan   ==  * ( 2 )   Project   [ county_code# 230 ,   fclass# 239 ]  +-   RangeJoin   geometry# 240 :   geometry ,   geometry# 236 :   geometry ,   true \n    :-   Scan   ExistingRDD [ fclass# 239 , geometry# 240 ] \n    +-   Project   [ county_code# 230 ,   st_geomfromwkt ( geom# 232 )   AS   geometry# 236 ] \n       +-   * ( 1 )   FileScan   csv   [ county_code# 230 , geom# 232 ]   Batched :   false ,   Format :   CSV ,   Location :   InMemoryFileIndex [ file : / projects / geospark / counties . csv ],   PartitionFilters :   [],   PushedFilters :   [],   ReadSchema :   struct county_code:string , geom : string  \nCalculating Number of Pois within counties per fclass.  pois_per_county   =   spatial_join_result . groupBy ( county_code ,   fclass ) .  \\\n     count ()  pois_per_county . show ( 5 ,   False )   +-----------+---------+-----+\n|county_code|fclass   |count|\n+-----------+---------+-----+\n|0805       |atm      |6    |\n|0805       |bench    |75   |\n|0803       |museum   |9    |\n|0802       |fast_food|5    |\n|0862       |atm      |20   |\n+-----------+---------+-----+", 
            "title": "GeoSparkSQL"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#integration-with-geopandas-and-shapely", 
            "text": "geospark has implemented serializers and deserializers which allows to convert GeoSpark Geometry objects into Shapely BaseGeometry objects. Based on that it is possible to load the data with geopandas from file (look at Fiona possible drivers) and create Spark DataFrame based on GeoDataFrame object.  Example, loading the data from shapefile using geopandas read_file method and create Spark DataFrame based on GeoDataFrame:  import   geopandas   as   gpd  from   pyspark.sql   import   SparkSession  from   geospark.register   import   GeoSparkRegistrator  spark   =   SparkSession . builder . \\\n       getOrCreate ()  GeoSparkRegistrator . registerAll ( spark )  gdf   =   gpd . read_file ( gis_osm_pois_free_1.shp )  spark . createDataFrame ( \n   gdf  ) . show ()   +---------+----+-----------+--------------------+--------------------+\n|   osm_id|code|     fclass|                name|            geometry|\n+---------+----+-----------+--------------------+--------------------+\n| 26860257|2422|  camp_site|            de Kroon|POINT (15.3393145...|\n| 26860294|2406|     chalet|      Le\u015bne Ustronie|POINT (14.8709625...|\n| 29947493|2402|      motel|                null|POINT (15.0946636...|\n| 29947498|2602|        atm|                null|POINT (15.0732014...|\n| 29947499|2401|      hotel|                null|POINT (15.0696777...|\n| 29947505|2401|      hotel|                null|POINT (15.0155749...|\n+---------+----+-----------+--------------------+--------------------+  Reading data with Spark and converting to GeoPandas  import   geopandas   as   gpd  from   pyspark.sql   import   SparkSession  from   geospark.register   import   GeoSparkRegistrator  spark   =   SparkSession . builder . \\\n     getOrCreate ()  GeoSparkRegistrator . registerAll ( spark )  counties   =   spark . \\\n     read . \\\n     option ( delimiter ,   | ) . \\\n     option ( header ,   true ) . \\\n     csv ( counties.csv )  counties . createOrReplaceTempView ( county )  counties_geom   =   spark . sql ( \n     SELECT *, st_geomFromWKT(geom) as geometry from county  )  df   =   counties_geom . toPandas ()  gdf   =   gpd . GeoDataFrame ( df ,   geometry = geometry )  gdf . plot ( \n     figsize = ( 10 ,   8 ), \n     column = value , \n     legend = True , \n     cmap = YlOrBr , \n     scheme = quantiles , \n     edgecolor = lightgray  )", 
            "title": "Integration with GeoPandas and Shapely"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#creating-spark-dataframe-based-on-shapely-objects", 
            "text": "", 
            "title": "Creating Spark DataFrame based on shapely objects"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#supported-shapely-objects", 
            "text": "shapely object  Available      Point     MultiPoint     LineString     MultiLinestring     Polygon     MultiPolygon      To create Spark DataFrame based on mentioned Geometry types, please use   GeometryType   from    geospark.sql.types   module. Converting works for list or tuple with shapely objects.  Schema for target table with integer id and geometry type can be defined as follow:  from   pyspark.sql.types   import   IntegerType ,   StructField ,   StructType  from   geospark.sql.types   import   GeometryType  schema   =   StructType ( \n     [ \n         StructField ( id ,   IntegerType (),   False ), \n         StructField ( geom ,   GeometryType (),   False ) \n     ]  )   Also Spark DataFrame with geometry type can be converted to list of shapely objects with   collect   method.", 
            "title": "Supported Shapely objects"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#example-usage-for-shapely-objects", 
            "text": "", 
            "title": "Example usage for Shapely objects"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#point", 
            "text": "from   shapely.geometry   import   Point  data   =   [ \n     [ 1 ,   Point ( 21.0 ,   52.0 )], \n     [ 1 ,   Point ( 23.0 ,   42.0 )], \n     [ 1 ,   Point ( 26.0 ,   32.0 )]  ]  gdf   =   spark . createDataFrame ( \n     data , \n     schema  )  gdf . show ()   +---+-------------+\n| id|         geom|\n+---+-------------+\n|  1|POINT (21 52)|\n|  1|POINT (23 42)|\n|  1|POINT (26 32)|\n+---+-------------+  gdf . printSchema ()   root\n |-- id: integer (nullable = false)\n |-- geom: geometry (nullable = false)", 
            "title": "Point"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#multipoint", 
            "text": "from   shapely.geometry   import   MultiPoint  data   =   [ \n     [ 1 ,   MultiPoint ([[ 19.511463 ,   51.765158 ],   [ 19.446408 ,   51.779752 ]])]  ]  gdf   =   spark . createDataFrame ( \n     data , \n     schema  ) . show ( 1 ,   False )   +---+---------------------------------------------------------+\n|id |geom                                                     |\n+---+---------------------------------------------------------+\n|1  |MULTIPOINT ((19.511463 51.765158), (19.446408 51.779752))|\n+---+---------------------------------------------------------+", 
            "title": "MultiPoint"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#linestring", 
            "text": "from   shapely.geometry   import   LineString  line   =   [( 40 ,   40 ),   ( 30 ,   30 ),   ( 40 ,   20 ),   ( 30 ,   10 )]  data   =   [ \n     [ 1 ,   LineString ( line )]  ]  gdf   =   spark . createDataFrame ( \n     data , \n     schema  )  gdf . show ( 1 ,   False )   +---+--------------------------------+\n|id |geom                            |\n+---+--------------------------------+\n|1  |LINESTRING (10 10, 20 20, 10 40)|\n+---+--------------------------------+", 
            "title": "LineString"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#multilinestring", 
            "text": "from   shapely.geometry   import   MultiLineString  line1   =   [( 10 ,   10 ),   ( 20 ,   20 ),   ( 10 ,   40 )]  line2   =   [( 40 ,   40 ),   ( 30 ,   30 ),   ( 40 ,   20 ),   ( 30 ,   10 )]  data   =   [ \n     [ 1 ,   MultiLineString ([ line1 ,   line2 ])]  ]  gdf   =   spark . createDataFrame ( \n     data , \n     schema  )  gdf . show ( 1 ,   False )   +---+---------------------------------------------------------------------+\n|id |geom                                                                 |\n+---+---------------------------------------------------------------------+\n|1  |MULTILINESTRING ((10 10, 20 20, 10 40), (40 40, 30 30, 40 20, 30 10))|\n+---+---------------------------------------------------------------------+", 
            "title": "MultiLineString"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#polygon", 
            "text": "from   shapely.geometry   import   Polygon  polygon   =   Polygon ( \n     [ \n          [ 19.51121 ,   51.76426 ], \n          [ 19.51056 ,   51.76583 ], \n          [ 19.51216 ,   51.76599 ], \n          [ 19.51280 ,   51.76448 ], \n          [ 19.51121 ,   51.76426 ] \n     ]  )  data   =   [ \n     [ 1 ,   polygon ]  ]  gdf   =   spark . createDataFrame ( \n     data , \n     schema  )  gdf . show ( 1 ,   False )   +---+--------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                    |\n+---+--------------------------------------------------------------------------------------------------------+\n|1  |POLYGON ((19.51121 51.76426, 19.51056 51.76583, 19.51216 51.76599, 19.5128 51.76448, 19.51121 51.76426))|\n+---+--------------------------------------------------------------------------------------------------------+", 
            "title": "Polygon"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#multipolygon", 
            "text": "from   shapely.geometry   import   MultiPolygon  exterior_p1   =   [( 0 ,   0 ),   ( 0 ,   2 ),   ( 2 ,   2 ),   ( 2 ,   0 ),   ( 0 ,   0 )]  interior_p1   =   [( 1 ,   1 ),   ( 1 ,   1.5 ),   ( 1.5 ,   1.5 ),   ( 1.5 ,   1 ),   ( 1 ,   1 )]  exterior_p2   =   [( 0 ,   0 ),   ( 1 ,   0 ),   ( 1 ,   1 ),   ( 0 ,   1 ),   ( 0 ,   0 )]  polygons   =   [ \n     Polygon ( exterior_p1 ,   [ interior_p1 ]), \n     Polygon ( exterior_p2 )  ]  data   =   [ \n     [ 1 ,   MultiPolygon ( polygons )]  ]  gdf   =   spark . createDataFrame ( \n     data , \n     schema  )  gdf . show ( 1 ,   False )   +---+----------------------------------------------------------------------------------------------------------+\n|id |geom                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------+\n|1  |MULTIPOLYGON (((0 0, 0 2, 2 2, 2 0, 0 0), (1 1, 1.5 1, 1.5 1.5, 1 1.5, 1 1)), ((0 0, 0 1, 1 1, 1 0, 0 0)))|\n+---+----------------------------------------------------------------------------------------------------------+", 
            "title": "MultiPolygon"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#supported-versions", 
            "text": "Currently this python wrapper supports the following Spark, GeoSpark and Python versions:", 
            "title": "Supported versions"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#apache-spark", 
            "text": "2.2    2.3    2.4", 
            "title": "Apache Spark"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#geosparksql_1", 
            "text": "1.3.1    1.2.0    1.1.3", 
            "title": "GeoSparkSQL"
        }, 
        {
            "location": "/tutorial/geospark-sql-python/#python", 
            "text": "3.6    3.7", 
            "title": "Python"
        }, 
        {
            "location": "/tutorial/GeoSpark-Runnable-DEMO/", 
            "text": "GeoSpark Template Project contains six template projects for GeoSpark, GeoSparkSQL and GeoSparkViz. The template projects have been configured properly. You are able to compile, package, and run the code locally \nwithout any extra coding\n.\n\n\nScala/Java Template Project", 
            "title": "GeoSpark template project"
        }, 
        {
            "location": "/tutorial/faq/", 
            "text": "Coming soon...\n\n\nFor now, please read \nGeoSpark Github FAQ Issues\n.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/", 
            "text": "Advanced tutorial: Tune your GeoSpark RDD application\n\n\nBefore getting into this advanced tutorial, please make sure that you have tried several GeoSpark functions on your local machine.\n\n\nPick a proper GeoSpark version\n\n\nThe versions of GeoSpark have three levels: X.X.X (i.e., 0.8.1). In addition, GeoSpark also supports Spark 1.X in Spark1.X version.\n\n\nThe first level means that this verion contains big structure redesign which may bring big changes in APIs and performance. Hopefully, we can see these big changes in GeoSpark 1.X version.\n\n\nThe second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old GeoSpark user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read \nGeoSpark version release notes\n and make sure you are ready to accept the API changes.\n\n\nThe third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all GeoSpark users that stay at the same level move to the latest version in this level.\n\n\nChoose a proper Spatial RDD constructor\n\n\nGeoSpark provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.\n\n\n\n\nInitialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows:\n\npublic\n \nPointRDD\n(\nJavaSparkContext\n \nsparkContext\n,\n \nString\n \nInputLocation\n,\n \nInteger\n \nOffset\n,\n \nFileDataSplitter\n \nsplitter\n,\n \nboolean\n \ncarryInputData\n,\n \nInteger\n \npartitions\n,\n \nStorageLevel\n \nnewLevel\n)\n\n\n\n\nInitialize a SpatialRDD from an existing RDD. A typical example is as follows:\n\npublic\n \nPointRDD\n(\nJavaRDD\nPoint\n \nrawSpatialRDD\n,\n \nStorageLevel\n \nnewLevel\n)\n\n\n\n\n\n\nYou may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why GeoSpark does this is that GeoSpark wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.\n\n\nHowever, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:\n\n\npublic\n \nPointRDD\n(\nJavaSparkContext\n \nsparkContext\n,\n \nString\n \nInputLocation\n,\n \nInteger\n \nOffset\n,\n \nFileDataSplitter\n \nsplitter\n,\n \nboolean\n \ncarryInputData\n,\n \nInteger\n \npartitions\n,\n \nEnvelope\n \ndatasetBoundary\n,\n \nInteger\n \napproximateTotalCount\n)\n \n{\n\n\n\nManually providing the dataset boundary and approxmiate total count helps GeoSpark avoiding several slow \"Action\"s during initialization.\n\n\nCache the Spatial RDD that is repeatedly used\n\n\nEach SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:\n\n\n\n\nrawSpatialRDD: The RDD generated by SpatialRDD constructors.\n\n\nspatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.\n\n\nindexedRawRDD: The RDD generated by indexing a rawSpatialRDD.\n\n\nindexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.\n\n\n\n\nThese four RDDs don't co-exist so you don't need to worry about the memory issue.\nThese four RDDs are invoked in different queries:\n\n\n\n\nSpatial Range Query / KNN Query, no index: rawSpatialRDD is used.\n\n\nSpatial Range Query / KNN Query, use index: indexedRawRDD is used.\n\n\nSpatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.\n\n\nSpatial Join Query / Distance Join Query, use index: indexed RDD is used.\n\n\n\n\nTherefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:\n\n\n\n\nIn Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.\n\n\nIn Spark RDD sharing applications such as \nLivy\n and \nSpark Job Server\n, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.\n\n\n\n\nBe aware of Spatial RDD partitions\n\n\nSometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.\n\n\nAfter that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.", 
            "title": "Tune GeoSpark RDD application"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#advanced-tutorial-tune-your-geospark-rdd-application", 
            "text": "Before getting into this advanced tutorial, please make sure that you have tried several GeoSpark functions on your local machine.", 
            "title": "Advanced tutorial: Tune your GeoSpark RDD application"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#pick-a-proper-geospark-version", 
            "text": "The versions of GeoSpark have three levels: X.X.X (i.e., 0.8.1). In addition, GeoSpark also supports Spark 1.X in Spark1.X version.  The first level means that this verion contains big structure redesign which may bring big changes in APIs and performance. Hopefully, we can see these big changes in GeoSpark 1.X version.  The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old GeoSpark user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read  GeoSpark version release notes  and make sure you are ready to accept the API changes.  The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all GeoSpark users that stay at the same level move to the latest version in this level.", 
            "title": "Pick a proper GeoSpark version"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#choose-a-proper-spatial-rdd-constructor", 
            "text": "GeoSpark provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.   Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows: public   PointRDD ( JavaSparkContext   sparkContext ,   String   InputLocation ,   Integer   Offset ,   FileDataSplitter   splitter ,   boolean   carryInputData ,   Integer   partitions ,   StorageLevel   newLevel )   Initialize a SpatialRDD from an existing RDD. A typical example is as follows: public   PointRDD ( JavaRDD Point   rawSpatialRDD ,   StorageLevel   newLevel )    You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why GeoSpark does this is that GeoSpark wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.  However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:  public   PointRDD ( JavaSparkContext   sparkContext ,   String   InputLocation ,   Integer   Offset ,   FileDataSplitter   splitter ,   boolean   carryInputData ,   Integer   partitions ,   Envelope   datasetBoundary ,   Integer   approximateTotalCount )   {  \nManually providing the dataset boundary and approxmiate total count helps GeoSpark avoiding several slow \"Action\"s during initialization.", 
            "title": "Choose a proper Spatial RDD constructor"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#cache-the-spatial-rdd-that-is-repeatedly-used", 
            "text": "Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:   rawSpatialRDD: The RDD generated by SpatialRDD constructors.  spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.  indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.  indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.   These four RDDs don't co-exist so you don't need to worry about the memory issue.\nThese four RDDs are invoked in different queries:   Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.  Spatial Range Query / KNN Query, use index: indexedRawRDD is used.  Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.  Spatial Join Query / Distance Join Query, use index: indexed RDD is used.   Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:   In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.  In Spark RDD sharing applications such as  Livy  and  Spark Job Server , many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.", 
            "title": "Cache the Spatial RDD that is repeatedly used"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#be-aware-of-spatial-rdd-partitions", 
            "text": "Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.  After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.", 
            "title": "Be aware of Spatial RDD partitions"
        }, 
        {
            "location": "/tutorial/benchmark/", 
            "text": "Benchmark\n\n\nWe welcome people to use GeoSpark for benchmark purpose. To achieve the best performance or enjoy all features of GeoSpark,\n\n\n\n\nPlease always use the latest version or state the version used in your benchmark so that we can trace back to the issues.\n\n\nPlease consider using GeoSpark core instead of GeoSparkSQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.\n\n\nPlease open GeoSpark kryo serializer to reduce the memory footprint.", 
            "title": "Benchmark"
        }, 
        {
            "location": "/tutorial/benchmark/#benchmark", 
            "text": "We welcome people to use GeoSpark for benchmark purpose. To achieve the best performance or enjoy all features of GeoSpark,   Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.  Please consider using GeoSpark core instead of GeoSparkSQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.  Please open GeoSpark kryo serializer to reduce the memory footprint.", 
            "title": "Benchmark"
        }, 
        {
            "location": "/api/GeoSpark-Scala-and-Java-API/", 
            "text": "Scala and Java API\n\n\nGeoSpark Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/geospark/javadoc/\n\n\nThe \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala/Java doc"
        }, 
        {
            "location": "/api/GeoSpark-Scala-and-Java-API/#scala-and-java-api", 
            "text": "GeoSpark Scala and Java API:  http://www.public.asu.edu/~jiayu2/geospark/javadoc/  The \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.  Note: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala and Java API"
        }, 
        {
            "location": "/api/GeoSpark-Python-API/", 
            "text": "Will be available soon.", 
            "title": "Python doc"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-javadoc/", 
            "text": "Scala and Java API\n\n\nGeoSparkSQL Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/geosparksql/javadoc/\n\n\nThe \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "JavaDoc"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-javadoc/#scala-and-java-api", 
            "text": "GeoSparkSQL Scala and Java API:  http://www.public.asu.edu/~jiayu2/geosparksql/javadoc/  The \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.  Note: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala and Java API"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/", 
            "text": "Introduction\n\n\nFunction list\n\n\nGeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:\n\nvar\n \nmyDataFrame\n \n=\n \nsparkSession\n.\nsql\n(\nYOUR_SQL\n)\n\n\n\n\n\n\nConstructor: Construct a Geometry given an input string or coordinates\n\n\nExample: ST_GeomFromWKT (string). Create a Geometry from a WKT String.\n\n\nDocumentation: \nHere\n\n\n\n\n\n\nFunction: Execute a function on the given column or columns\n\n\nExample: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.\n\n\nDocumentation: \nHere\n\n\n\n\n\n\nAggregate function: Return the aggregated value on the given column\n\n\nExample: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.\n\n\nDocumentation: \nHere\n\n\n\n\n\n\nPredicate: Execute a logic judgement on the given columns and return true or false\n\n\nExample: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".\n\n\nDocumentation: \nHere\n\n\n\n\n\n\n\n\nGeoSparkSQL supports SparkSQL query optimizer, documentation is \nHere\n\n\nQuick start\n\n\nThe detailed explanation is here \nWrite a SQL/DataFrame application\n.\n\n\n\n\nAdd GeoSpark-core and GeoSparkSQL into your project POM.xml or build.sbt\n\n\nDeclare your Spark Session\n\nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n().\n\n      \nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n).\n\n      \nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n).\n\n      \nmaster\n(\nlocal[*]\n).\nappName\n(\nmyGeoSparkSQLdemo\n).\ngetOrCreate\n()\n\n\n\n\nAdd the following line after your SparkSession declaration:\n\nGeoSparkSQLRegistrator\n.\nregisterAll\n(\nsparkSession\n)", 
            "title": "Quick start"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#function-list", 
            "text": "GeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through: var   myDataFrame   =   sparkSession . sql ( YOUR_SQL )    Constructor: Construct a Geometry given an input string or coordinates  Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.  Documentation:  Here    Function: Execute a function on the given column or columns  Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.  Documentation:  Here    Aggregate function: Return the aggregated value on the given column  Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.  Documentation:  Here    Predicate: Execute a logic judgement on the given columns and return true or false  Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".  Documentation:  Here     GeoSparkSQL supports SparkSQL query optimizer, documentation is  Here", 
            "title": "Function list"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#quick-start", 
            "text": "The detailed explanation is here  Write a SQL/DataFrame application .   Add GeoSpark-core and GeoSparkSQL into your project POM.xml or build.sbt  Declare your Spark Session sparkSession   =   SparkSession . builder (). \n       config ( spark.serializer , classOf [ KryoSerializer ]. getName ). \n       config ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName ). \n       master ( local[*] ). appName ( myGeoSparkSQLdemo ). getOrCreate ()   Add the following line after your SparkSession declaration: GeoSparkSQLRegistrator . registerAll ( sparkSession )", 
            "title": "Quick start"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/", 
            "text": "Note\n\n\nUUIDs ensure the shape uniqueness of a geometry. It can be any strings. This is only needed when you want to convert an Spatial DataFrame to an Spatial RDD and let each geometry carry some non-spatial attributes (e.g., price, age, ...).\n\n\n\n\nST_GeomFromWKT\n\n\nIntroduction: Construct a Geometry from Wkt. Unlimited UUID strings can be appended.\n\n\nFormat:\n\nST_GeomFromWKT (Wkt:string, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_GeomFromWKT\n(\npolygontable\n.\n_c0\n)\n \nAS\n \npolygonshape\n\n\nFROM\n \npolygontable\n\n\n\n\nSELECT\n \nST_GeomFromWKT\n(\nPOINT(40.7128,-74.0060)\n)\n \nAS\n \ngeometry\n\n\n\n\n\nST_GeomFromWKB\n\n\nIntroduction: Construct a Geometry from WKB string. Unlimited UUID strings can be appended.\n\n\nFormat:\n\nST_GeomFromWKB (Wkb:string, UUID1, UUID2, ...)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_GeomFromWKB\n(\npolygontable\n.\n_c0\n)\n \nAS\n \npolygonshape\n\n\nFROM\n \npolygontable\n\n\n\n\nST_GeomFromGeoJSON\n\n\nIntroduction: Construct a Geometry from GeoJson. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_GeomFromGeoJSON (GeoJson:string, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nvar\n \npolygonJsonDf\n \n=\n \nsparkSession\n.\nread\n.\nformat\n(\ncsv\n).\noption\n(\ndelimiter\n,\n\\t\n).\noption\n(\nheader\n,\nfalse\n).\nload\n(\ngeoJsonGeomInputLocation\n)\n\n\npolygonJsonDf\n.\ncreateOrReplaceTempView\n(\npolygontable\n)\n\n\npolygonJsonDf\n.\nshow\n()\n\n\nvar\n \npolygonDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n        \n\n\n          | SELECT ST_GeomFromGeoJSON(polygontable._c0) AS countyshape\n\n\n          | FROM polygontable\n\n\n        \n.\nstripMargin\n)\n\n\npolygonDf\n.\nshow\n()\n\n\n\n\n\n\nWarning\n\n\nThe way that GeoSparkSQL reads GeoJSON is different from that in SparkSQL\n\n\n\n\nRead ESRI Shapefile\n\n\nIntroduction: Construct a DataFrame from a Shapefile\n\n\nSince: \nv1.0.0\n\n\nSparkSQL example:\n\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nShapefileReader\n.\nreadToGeometryRDD\n(\nsparkSession\n.\nsparkContext\n,\n \nshapefileInputLocation\n)\n\n\nvar\n \nrawSpatialDf\n \n=\n \nAdapter\n.\ntoDf\n(\nspatialRDD\n,\nsparkSession\n)\n\n\nrawSpatialDf\n.\ncreateOrReplaceTempView\n(\nrawSpatialDf\n)\n\n\nvar\n \nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n\n          | ST_GeomFromWKT(rddshape), _c1, _c2\n\n\n          | FROM rawSpatialDf\n\n\n        \n.\nstripMargin\n)\n\n\nspatialDf\n.\nshow\n()\n\n\nspatialDf\n.\nprintSchema\n()\n\n\n\n\n\n\n\nNote\n\n\nThe file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called \nmyShapefile\n, the file structure should be like this:\n\n- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n\n\n\n\n\n\n\nWarning\n\n\nPlease make sure you use \nST_GeomFromWKT\n to create Geometry type column otherwise that column cannot be used in GeoSparkSQL.\n\n\n\n\nIf the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding\nvia \ngeospark.global.charset\n system property before the call to \nShapefileReader.readToGeometryRDD\n.\n\n\nExample:\n\n\nSystem\n.\nsetProperty\n(\ngeospark.global.charset\n,\n \nutf8\n)\n\n\n\n\n\nST_Point\n\n\nIntroduction: Construct a Point from X and Y. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_Point (X:decimal, Y:decimal, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Point\n(\nCAST\n(\npointtable\n.\n_c0\n \nAS\n \nDecimal\n(\n24\n,\n20\n)),\n \nCAST\n(\npointtable\n.\n_c1\n \nAS\n \nDecimal\n(\n24\n,\n20\n)))\n \nAS\n \npointshape\n\n\nFROM\n \npointtable\n\n\n\n\nST_PointFromText\n\n\nIntroduction: Construct a Point from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_PointFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_PointFromText\n(\npointtable\n.\n_c0\n,\n,\n)\n \nAS\n \npointshape\n\n\nFROM\n \npointtable\n\n\n\n\nSELECT\n \nST_PointFromText\n(\n40.7128,-74.0060\n,\n \n,\n)\n \nAS\n \npointshape\n\n\n\n\n\nST_PolygonFromText\n\n\nIntroduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_PolygonFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_PolygonFromText\n(\npolygontable\n.\n_c0\n,\n,\n)\n \nAS\n \npolygonshape\n\n\nFROM\n \npolygontable\n\n\n\n\nSELECT\n \nST_PolygonFromText\n(\n-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969\n,\n \n,\n)\n \nAS\n \npolygonshape\n\n\n\n\n\nST_LineStringFromText\n\n\nIntroduction: Construct a LineString from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_LineStringFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_LineStringFromText\n(\nlinestringtable\n.\n_c0\n,\n,\n)\n \nAS\n \nlinestringshape\n\n\nFROM\n \nlinestringtable\n\n\n\n\nSELECT\n \nST_LineStringFromText\n(\n-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794\n,\n \n,\n)\n \nAS\n \nlinestringshape\n\n\n\n\n\nST_PolygonFromEnvelope\n\n\nIntroduction: Construct a Polygon from MinX, MinY, MaxX, MaxY. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n\n\nFROM\n \npointdf\n\n\nWHERE\n \nST_Contains\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n),\n \npointdf\n.\npointshape\n)\n\n\n\n\nST_Circle\n\n\nIntroduction: Construct a Circle from A with a Radius.\n\n\nFormat: \nST_Circle (A:Geometry, Radius:decimal)\n\n\nSince: \nv1.0.0\n - \nv1.1.3\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_Circle\n(\npointdf\n.\npointshape\n,\n \n1\n.\n0\n)\n\n\nFROM\n \npointdf\n\n\n\n\n\n\n\nNote\n\n\nGeoSpark doesn't control the radius's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See \nST_Transform\n.", 
            "title": "Constructor"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromwkt", 
            "text": "Introduction: Construct a Geometry from Wkt. Unlimited UUID strings can be appended.  Format: ST_GeomFromWKT (Wkt:string, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_GeomFromWKT ( polygontable . _c0 )   AS   polygonshape  FROM   polygontable   SELECT   ST_GeomFromWKT ( POINT(40.7128,-74.0060) )   AS   geometry", 
            "title": "ST_GeomFromWKT"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromwkb", 
            "text": "Introduction: Construct a Geometry from WKB string. Unlimited UUID strings can be appended.  Format: ST_GeomFromWKB (Wkb:string, UUID1, UUID2, ...)  Since:  v1.2.0  Spark SQL example: SELECT   ST_GeomFromWKB ( polygontable . _c0 )   AS   polygonshape  FROM   polygontable", 
            "title": "ST_GeomFromWKB"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromgeojson", 
            "text": "Introduction: Construct a Geometry from GeoJson. Unlimited UUID strings can be appended.  Format:  ST_GeomFromGeoJSON (GeoJson:string, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: var   polygonJsonDf   =   sparkSession . read . format ( csv ). option ( delimiter , \\t ). option ( header , false ). load ( geoJsonGeomInputLocation )  polygonJsonDf . createOrReplaceTempView ( polygontable )  polygonJsonDf . show ()  var   polygonDf   =   sparkSession . sql ( \n                    | SELECT ST_GeomFromGeoJSON(polygontable._c0) AS countyshape            | FROM polygontable           . stripMargin )  polygonDf . show ()    Warning  The way that GeoSparkSQL reads GeoJSON is different from that in SparkSQL", 
            "title": "ST_GeomFromGeoJSON"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#read-esri-shapefile", 
            "text": "Introduction: Construct a DataFrame from a Shapefile  Since:  v1.0.0  SparkSQL example:  var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   ShapefileReader . readToGeometryRDD ( sparkSession . sparkContext ,   shapefileInputLocation )  var   rawSpatialDf   =   Adapter . toDf ( spatialRDD , sparkSession )  rawSpatialDf . createOrReplaceTempView ( rawSpatialDf )  var   spatialDf   =   sparkSession . sql (            | ST_GeomFromWKT(rddshape), _c1, _c2            | FROM rawSpatialDf           . stripMargin )  spatialDf . show ()  spatialDf . printSchema ()    Note  The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called  myShapefile , the file structure should be like this: - shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...    Warning  Please make sure you use  ST_GeomFromWKT  to create Geometry type column otherwise that column cannot be used in GeoSparkSQL.   If the file you are reading contains non-ASCII characters you'll need to explicitly set the encoding\nvia  geospark.global.charset  system property before the call to  ShapefileReader.readToGeometryRDD .  Example:  System . setProperty ( geospark.global.charset ,   utf8 )", 
            "title": "Read ESRI Shapefile"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_point", 
            "text": "Introduction: Construct a Point from X and Y. Unlimited UUID strings can be appended.  Format:  ST_Point (X:decimal, Y:decimal, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Point ( CAST ( pointtable . _c0   AS   Decimal ( 24 , 20 )),   CAST ( pointtable . _c1   AS   Decimal ( 24 , 20 )))   AS   pointshape  FROM   pointtable", 
            "title": "ST_Point"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_pointfromtext", 
            "text": "Introduction: Construct a Point from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Format:  ST_PointFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_PointFromText ( pointtable . _c0 , , )   AS   pointshape  FROM   pointtable   SELECT   ST_PointFromText ( 40.7128,-74.0060 ,   , )   AS   pointshape", 
            "title": "ST_PointFromText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_polygonfromtext", 
            "text": "Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed. Unlimited UUID strings can be appended.  Format:  ST_PolygonFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_PolygonFromText ( polygontable . _c0 , , )   AS   polygonshape  FROM   polygontable   SELECT   ST_PolygonFromText ( -74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969 ,   , )   AS   polygonshape", 
            "title": "ST_PolygonFromText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_linestringfromtext", 
            "text": "Introduction: Construct a LineString from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Format:  ST_LineStringFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_LineStringFromText ( linestringtable . _c0 , , )   AS   linestringshape  FROM   linestringtable   SELECT   ST_LineStringFromText ( -74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794 ,   , )   AS   linestringshape", 
            "title": "ST_LineStringFromText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_polygonfromenvelope", 
            "text": "Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY. Unlimited UUID strings can be appended.  Format:  ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   *  FROM   pointdf  WHERE   ST_Contains ( ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ),   pointdf . pointshape )", 
            "title": "ST_PolygonFromEnvelope"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_circle", 
            "text": "Introduction: Construct a Circle from A with a Radius.  Format:  ST_Circle (A:Geometry, Radius:decimal)  Since:  v1.0.0  -  v1.1.3  Spark SQL example:  SELECT   ST_Circle ( pointdf . pointshape ,   1 . 0 )  FROM   pointdf    Note  GeoSpark doesn't control the radius's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See  ST_Transform .", 
            "title": "ST_Circle"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/", 
            "text": "ST_Distance\n\n\nIntroduction: Return the Euclidean distance between A and B\n\n\nFormat: \nST_Distance (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Distance\n(\npolygondf\n.\ncountyshape\n,\n \npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_ConvexHull\n\n\nIntroduction: Return the Convex Hull of polgyon A\n\n\nFormat: \nST_ConvexHull (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_ConvexHull\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Envelope\n\n\nIntroduction: Return the envelop boundary of A\n\n\nFormat: \nST_Envelope (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_Envelope\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_Length\n\n\nIntroduction: Return the perimeter of A\n\n\nFormat: ST_Length (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Length\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Area\n\n\nIntroduction: Return the area of A\n\n\nFormat: \nST_Area (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Area\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Centroid\n\n\nIntroduction: Return the centroid point of A\n\n\nFormat: \nST_Centroid (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Centroid\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Transform\n\n\nIntroduction:\n\n\nTransform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS\n\n\n\n\nNote\n\n\nBy default, \nST_Transform\n assumes Longitude/Latitude is your coordinate X/Y. If this is not the case, set UseLongitudeLatitudeOrder as \"false\".\n\n\n\n\n\n\nNote\n\n\nIf \nST_Transform\n throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.\n\n\n\n\nFormat: \nST_Transform (A:geometry, SourceCRS:string, TargetCRS:string, [Optional] UseLongitudeLatitudeOrder:Boolean, [Optional] DisableError)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example (simple):\n\nSELECT\n \nST_Transform\n(\npolygondf\n.\ncountyshape\n,\n \nepsg:4326\n,\nepsg:3857\n)\n \n\nFROM\n \npolygondf\n\n\n\n\nSpark SQL example (with optional parameters):\n\nSELECT\n \nST_Transform\n(\npolygondf\n.\ncountyshape\n,\n \nepsg:4326\n,\nepsg:3857\n,\ntrue\n,\n \nfalse\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\n\nNote\n\n\nThe detailed EPSG information can be searched on \nEPSG.io\n.\n\n\n\n\nST_Intersection\n\n\nIntroduction: Return the intersection geometry of A and B\n\n\nFormat: \nST_Intersection (A:geometry, B:geometry)\n\n\nSince: \nv1.1.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_Intersection\n(\npolygondf\n.\ncountyshape\n,\n \npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_IsValid\n\n\nIntroduction: Test if a geometry is well formed\n\n\nFormat: \nST_IsValid (A:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_IsValid\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_MakeValid\n\n\nIntroduction: Given an invalid polygon or multipolygon and removeHoles boolean flag,\n create a valid representation of the geometry.\n\n\nFormat: \nST_MakeValid (A:geometry, removeHoles:Boolean)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\n\nSELECT\n \ngeometryValid\n.\npolygon\n\n\nFROM\n \ntable\n\n\nLATERAL\n \nVIEW\n \nST_MakeValid\n(\npolygon\n,\n \nfalse\n)\n \ngeometryValid\n \nAS\n \npolygon\n\n\n\n\n\n\n\nNote\n\n\nMight return multiple polygons from a only one invalid polygon\nThat's the reason why we need to use the LATERAL VIEW expression\n\n\n\n\n\n\nNote\n\n\nThrows an exception if the geometry isn't polygon or multipolygon\n\n\n\n\nST_PrecisionReduce\n\n\nIntroduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.\n\n\nFormat: \nST_PrecisionReduce (A:geometry, B:int)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_PrecisionReduce\n(\npolygondf\n.\ncountyshape\n,\n \n9\n)\n\n\nFROM\n \npolygondf\n\n\n\nThe new coordinates will only have 9 decimal places.\n\n\nST_IsSimple\n\n\nIntroduction: Test if geometry's only self-intersections are at boundary points.\n\n\nFormat: \nST_IsSimple (A:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_IsSimple\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_Buffer\n\n\nIntroduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.\n\n\nFormat: \nST_Buffer (A:geometry, buffer: Double)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Buffer\n(\npolygondf\n.\ncountyshape\n,\n \n1\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_AsText\n\n\nIntroduction: Return the Well-Known Text string representation of a geometry\n\n\nFormat: \nST_AsText (A:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_AsText\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_AsGeoJSON\n\n\nIntroduction: Return the \nGeoJSON\n string representation of a geometry\n\n\nFormat: \nST_AsGeoJSON (A:geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_AsGeoJSON\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_NPoints\n\n\nIntroduction: Return points of the geometry\n\n\nSince: \nv1.2.1\n\n\nFormat: \nST_NPoints (A:geometry)\n\n\nSELECT\n \nST_NPoints\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_SimplifyPreserveTopology\n\n\nIntroduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input,\n              and with the components having the same topological relationship.\n\n\nSince: \nv1.2.1\n\n\nFormat: \nST_SimplifyPreserveTopology (A:geometry, distanceTolerance: Double)\n\n\nSELECT\n \nST_SimplifyPreserveTopology\n(\npolygondf\n.\ncountyshape\n,\n \n10\n.\n0\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_GeometryType\n\n\nIntroduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.\n\n\nFormat: \nST_GeometryType (A:geometry)\n\n\nSince: \nv1.2.1\n\n\nSpark SQL example:\n\nSELECT\n \nST_GeometryType\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_LineMerge\n\n\nIntroduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.\n\n\n\n\nNote\n\n\nOnly works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.\n\n\n\n\nFormat: \nST_LineMerge (A:geometry)\n\n\nSince: \nv1.3.2\n\n\nSELECT\n \nST_LineMerge\n(\ngeometry\n)\n\n\nFROM\n \ndf\n\n\n\n\n\nST_Azimuth\n\n\nIntroduction: Returns Azimuth for two given points in radians null otherwise.\n\n\nFormat: \nST_Azimuth(pointA: Point, pointB: Point)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_Azimuth\n(\nST_POINT\n(\n0\n.\n0\n \n25\n.\n0\n),\n \nST_POINT\n(\n0\n.\n0\n \n0\n.\n0\n))\n\n\n\n\nOutput: \n3.141592653589793\n\n\nST_X\n\n\nIntroduction: Returns X Coordinate of given Point null otherwise.\n\n\nFormat: \nST_X(pointA: Point)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_X\n(\nST_POINT\n(\n0\n.\n0\n \n25\n.\n0\n))\n\n\n\nOutput: \n0.0\n\n\nST_Y\n\n\nIntroduction: Returns Y Coordinate of given Point, null otherwise.\n\n\nFormat: \nST_Y(pointA: Point)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_Y\n(\nST_POINT\n(\n0\n.\n0\n \n25\n.\n0\n))\n\n\n\n\nOutput: \n25.0\n\n\nST_StartPoint\n\n\nIntroduction: Returns first point of given linestring.\n\n\nFormat: \nST_StartPoint(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_StartPoint\n(\nST_GeomFromText\n(\nLINESTRING(100 150,50 60, 70 80, 160 170)\n))\n\n\n\n\nOutput: \nPOINT(100 150)\n\n\nST_EndPoint\n\n\nIntroduction: Returns last point of given linestring.\n\n\nFormat: \nST_EndPoint(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_EndPoint\n(\nST_GeomFromText\n(\nLINESTRING(100 150,50 60, 70 80, 160 170)\n))\n\n\n\n\nOutput: \nPOINT(160 170)\n\n\nST_Boundary\n\n\nIntroduction: Returns the closure of the combinatorial boundary of this Geometry.\n\n\nFormat: \nST_Boundary(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_Boundary\n(\nST_GeomFromText\n(\nPOLYGON((1 1,0 0, -1 1, 1 1))\n))\n\n\n\n\nOutput: \nLINESTRING (1 1, 0 0, -1 1, 1 1)\n\n\nST_ExteriorRing\n\n\nIntroduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.\n\n\nFormat: \nST_ExteriorRing(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_ExteriorRing\n(\nST_GeomFromText\n(\nPOLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1))\n))\n\n\n\n\nOutput: \nLINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)\n\n\nST_GeometryN\n\n\nIntroduction: Return the 1-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON Otherwise, return null\n\n\nFormat: \nST_GeometryN(geom: geometry, n: Int)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_GeometryN\n(\nST_GeomFromText\n(\nMULTIPOINT((1 2), (3 4), (5 6), (8 9))\n),\n \n1\n)\n\n\n\n\nOutput: \nPOINT (3 4)\n\n\nST_InteriorRingN\n\n\nIntroduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range\n\n\nFormat: \nST_InteriorRingN(geom: geometry, n: Int)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_InteriorRingN\n(\nST_GeomFromText\n(\nPOLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3))\n),\n \n0\n)\n\n\n\n\nOutput: \nLINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)\n\n\nST_Dump\n\n\nIntroduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry\nitself, if the geometry is collection or multi it returns record for each of collection components.\n\n\nFormat: \nST_Dump(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_Dump\n(\nST_GeomFromText\n(\nMULTIPOINT ((10 40), (40 30), (20 20), (30 10))\n))\n\n\n\n\nOutput: [POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]\n\n\nST_DumpPoints\n\n\nIntroduction: Returns list of Points which geometry consists of.\n\n\nFormat: \nST_DumpPoints(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_DumpPoints\n(\nST_GeomFromText\n(\nLINESTRING (0 0, 1 1, 1 0)\n))\n \n\n\n\nOutput: \n[POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]\n\n\nST_IsClosed\n\n\nIntroduction: RETURNS true if the LINESTRING start and end point are the same.\n\n\nFormat: \nST_IsClosed(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_IsClosed\n(\nST_GeomFromText\n(\nLINESTRING(0 0, 1 1, 1 0)\n))\n\n\n\n\nOutput: \nfalse\n\n\nST_NumInteriorRings\n\n\nIntroduction: RETURNS number of interior rings of polygon geometries.\n\n\nFormat: \nST_NumInteriorRings(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_NumInteriorRings\n(\nST_GeomFromText\n(\nPOLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1))\n))\n\n\n\n\nOutput: \n1\n\n\nST_AddPoint\n\n\nIntroduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.\n\n\nFormat: \nST_AddPoint(geom: geometry, point: geometry, position: integer)\n\n\nFormat: \nST_AddPoint(geom: geometry, point: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_AddPoint\n(\nST_GeomFromText\n(\nLINESTRING(0 0, 1 1, 1 0)\n),\n \nST_GeomFromText\n(\nPoint(21 52)\n),\n \n1\n)\n\n\n\nSELECT\n \nST_AddPoint\n(\nST_GeomFromText\n(\nLinestring(0 0, 1 1, 1 0)\n),\n \nST_GeomFromText\n(\nPoint(21 52)\n))\n\n\n\n\nOutput:\n\nLINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)\n\n\n\nST_RemovePoint\n\n\nIntroduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.\n\n\nFormat: \nST_RemovePoint(geom: geometry, position: integer)\n\n\nFormat: \nST_RemovePoint(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_RemovePoint\n(\nST_GeomFromText\n(\nLINESTRING(0 0, 1 1, 1 0)\n),\n \n1\n)\n\n\n\n\nOutput: \nLINESTRING(0 0, 1 0)\n\n\nST_IsRing\n\n\nIntroduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.\n\n\nFormat: \nST_IsRing(geom: geometry)\n\n\nSince: \nv1.3.2\n\n\nSpark SQL example:\n\nSELECT\n \nST_IsRing\n(\nST_GeomFromText\n(\nLINESTRING(0 0, 0 1, 1 1, 1 0, 0 0)\n))\n\n\n\n\nOutput: \ntrue\n\n\nST_NumGeometries\n\n\nIntroduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.\n\n\nFormat: \nST_NumGeometries (A:geometry)\n\n\nSince: \nv1.3.2\n\n\nSELECT\n \nST_NumGeometries\n(\ndf\n.\ngeometry\n)\n\n\nFROM\n \ndf", 
            "title": "Function"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_distance", 
            "text": "Introduction: Return the Euclidean distance between A and B  Format:  ST_Distance (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Distance ( polygondf . countyshape ,   polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Distance"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_convexhull", 
            "text": "Introduction: Return the Convex Hull of polgyon A  Format:  ST_ConvexHull (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_ConvexHull ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_ConvexHull"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_envelope", 
            "text": "Introduction: Return the envelop boundary of A  Format:  ST_Envelope (A:geometry)  Since:  v1.0.0  Spark SQL example:  SELECT   ST_Envelope ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Envelope"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_length", 
            "text": "Introduction: Return the perimeter of A  Format: ST_Length (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Length ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Length"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_area", 
            "text": "Introduction: Return the area of A  Format:  ST_Area (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Area ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Area"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_centroid", 
            "text": "Introduction: Return the centroid point of A  Format:  ST_Centroid (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Centroid ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Centroid"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_transform", 
            "text": "Introduction:  Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS   Note  By default,  ST_Transform  assumes Longitude/Latitude is your coordinate X/Y. If this is not the case, set UseLongitudeLatitudeOrder as \"false\".    Note  If  ST_Transform  throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.   Format:  ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string, [Optional] UseLongitudeLatitudeOrder:Boolean, [Optional] DisableError)  Since:  v1.0.0  Spark SQL example (simple): SELECT   ST_Transform ( polygondf . countyshape ,   epsg:4326 , epsg:3857 )   FROM   polygondf   Spark SQL example (with optional parameters): SELECT   ST_Transform ( polygondf . countyshape ,   epsg:4326 , epsg:3857 , true ,   false )  FROM   polygondf    Note  The detailed EPSG information can be searched on  EPSG.io .", 
            "title": "ST_Transform"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_intersection", 
            "text": "Introduction: Return the intersection geometry of A and B  Format:  ST_Intersection (A:geometry, B:geometry)  Since:  v1.1.0  Spark SQL example:  SELECT   ST_Intersection ( polygondf . countyshape ,   polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Intersection"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_isvalid", 
            "text": "Introduction: Test if a geometry is well formed  Format:  ST_IsValid (A:geometry)  Since:  v1.2.0  Spark SQL example:  SELECT   ST_IsValid ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_IsValid"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_makevalid", 
            "text": "Introduction: Given an invalid polygon or multipolygon and removeHoles boolean flag,\n create a valid representation of the geometry.  Format:  ST_MakeValid (A:geometry, removeHoles:Boolean)  Since:  v1.2.0  Spark SQL example:  SELECT   geometryValid . polygon  FROM   table  LATERAL   VIEW   ST_MakeValid ( polygon ,   false )   geometryValid   AS   polygon    Note  Might return multiple polygons from a only one invalid polygon\nThat's the reason why we need to use the LATERAL VIEW expression    Note  Throws an exception if the geometry isn't polygon or multipolygon", 
            "title": "ST_MakeValid"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_precisionreduce", 
            "text": "Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.  Format:  ST_PrecisionReduce (A:geometry, B:int)  Since:  v1.2.0  Spark SQL example:  SELECT   ST_PrecisionReduce ( polygondf . countyshape ,   9 )  FROM   polygondf  \nThe new coordinates will only have 9 decimal places.", 
            "title": "ST_PrecisionReduce"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_issimple", 
            "text": "Introduction: Test if geometry's only self-intersections are at boundary points.  Format:  ST_IsSimple (A:geometry)  Since:  v1.2.0  Spark SQL example:  SELECT   ST_IsSimple ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_IsSimple"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_buffer", 
            "text": "Introduction: Returns a geometry/geography that represents all points whose distance from this Geometry/geography is less than or equal to distance.  Format:  ST_Buffer (A:geometry, buffer: Double)  Since:  v1.2.0  Spark SQL example: SELECT   ST_Buffer ( polygondf . countyshape ,   1 )  FROM   polygondf", 
            "title": "ST_Buffer"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_astext", 
            "text": "Introduction: Return the Well-Known Text string representation of a geometry  Format:  ST_AsText (A:geometry)  Since:  v1.2.0  Spark SQL example: SELECT   ST_AsText ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_AsText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_asgeojson", 
            "text": "Introduction: Return the  GeoJSON  string representation of a geometry  Format:  ST_AsGeoJSON (A:geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_AsGeoJSON ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_AsGeoJSON"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_npoints", 
            "text": "Introduction: Return points of the geometry  Since:  v1.2.1  Format:  ST_NPoints (A:geometry)  SELECT   ST_NPoints ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_NPoints"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_simplifypreservetopology", 
            "text": "Introduction: Simplifies a geometry and ensures that the result is a valid geometry having the same dimension and number of components as the input,\n              and with the components having the same topological relationship.  Since:  v1.2.1  Format:  ST_SimplifyPreserveTopology (A:geometry, distanceTolerance: Double)  SELECT   ST_SimplifyPreserveTopology ( polygondf . countyshape ,   10 . 0 )  FROM   polygondf", 
            "title": "ST_SimplifyPreserveTopology"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_geometrytype", 
            "text": "Introduction: Returns the type of the geometry as a string. EG: 'ST_Linestring', 'ST_Polygon' etc.  Format:  ST_GeometryType (A:geometry)  Since:  v1.2.1  Spark SQL example: SELECT   ST_GeometryType ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_GeometryType"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_linemerge", 
            "text": "Introduction: Returns a LineString formed by sewing together the constituent line work of a MULTILINESTRING.   Note  Only works for MULTILINESTRING. Using other geometry will return a GEOMETRYCOLLECTION EMPTY. If the MultiLineString can't be merged, the original MULTILINESTRING is returned.   Format:  ST_LineMerge (A:geometry)  Since:  v1.3.2  SELECT   ST_LineMerge ( geometry )  FROM   df", 
            "title": "ST_LineMerge"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_azimuth", 
            "text": "Introduction: Returns Azimuth for two given points in radians null otherwise.  Format:  ST_Azimuth(pointA: Point, pointB: Point)  Since:  v1.3.2  Spark SQL example: SELECT   ST_Azimuth ( ST_POINT ( 0 . 0   25 . 0 ),   ST_POINT ( 0 . 0   0 . 0 ))   Output:  3.141592653589793", 
            "title": "ST_Azimuth"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_x", 
            "text": "Introduction: Returns X Coordinate of given Point null otherwise.  Format:  ST_X(pointA: Point)  Since:  v1.3.2  Spark SQL example: SELECT   ST_X ( ST_POINT ( 0 . 0   25 . 0 ))  \nOutput:  0.0", 
            "title": "ST_X"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_y", 
            "text": "Introduction: Returns Y Coordinate of given Point, null otherwise.  Format:  ST_Y(pointA: Point)  Since:  v1.3.2  Spark SQL example: SELECT   ST_Y ( ST_POINT ( 0 . 0   25 . 0 ))   Output:  25.0", 
            "title": "ST_Y"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_startpoint", 
            "text": "Introduction: Returns first point of given linestring.  Format:  ST_StartPoint(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_StartPoint ( ST_GeomFromText ( LINESTRING(100 150,50 60, 70 80, 160 170) ))   Output:  POINT(100 150)", 
            "title": "ST_StartPoint"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_endpoint", 
            "text": "Introduction: Returns last point of given linestring.  Format:  ST_EndPoint(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_EndPoint ( ST_GeomFromText ( LINESTRING(100 150,50 60, 70 80, 160 170) ))   Output:  POINT(160 170)", 
            "title": "ST_EndPoint"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_boundary", 
            "text": "Introduction: Returns the closure of the combinatorial boundary of this Geometry.  Format:  ST_Boundary(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_Boundary ( ST_GeomFromText ( POLYGON((1 1,0 0, -1 1, 1 1)) ))   Output:  LINESTRING (1 1, 0 0, -1 1, 1 1)", 
            "title": "ST_Boundary"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_exteriorring", 
            "text": "Introduction: Returns a line string representing the exterior ring of the POLYGON geometry. Return NULL if the geometry is not a polygon.  Format:  ST_ExteriorRing(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_ExteriorRing ( ST_GeomFromText ( POLYGON((0 0 1, 1 1 1, 1 2 1, 1 1 1, 0 0 1)) ))   Output:  LINESTRING (0 0, 1 1, 1 2, 1 1, 0 0)", 
            "title": "ST_ExteriorRing"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_geometryn", 
            "text": "Introduction: Return the 1-based Nth geometry if the geometry is a GEOMETRYCOLLECTION, (MULTI)POINT, (MULTI)LINESTRING, MULTICURVE or (MULTI)POLYGON Otherwise, return null  Format:  ST_GeometryN(geom: geometry, n: Int)  Since:  v1.3.2  Spark SQL example: SELECT   ST_GeometryN ( ST_GeomFromText ( MULTIPOINT((1 2), (3 4), (5 6), (8 9)) ),   1 )   Output:  POINT (3 4)", 
            "title": "ST_GeometryN"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_interiorringn", 
            "text": "Introduction: Returns the Nth interior linestring ring of the polygon geometry. Returns NULL if the geometry is not a polygon or the given N is out of range  Format:  ST_InteriorRingN(geom: geometry, n: Int)  Since:  v1.3.2  Spark SQL example: SELECT   ST_InteriorRingN ( ST_GeomFromText ( POLYGON((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1), (1 3, 2 3, 2 4, 1 4, 1 3), (3 3, 4 3, 4 4, 3 4, 3 3)) ),   0 )   Output:  LINESTRING (1 1, 2 1, 2 2, 1 2, 1 1)", 
            "title": "ST_InteriorRingN"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_dump", 
            "text": "Introduction: It expands the geometries. If the geometry is simple (Point, Polygon Linestring etc.) it returns the geometry\nitself, if the geometry is collection or multi it returns record for each of collection components.  Format:  ST_Dump(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_Dump ( ST_GeomFromText ( MULTIPOINT ((10 40), (40 30), (20 20), (30 10)) ))   Output: [POINT (10 40), POINT (40 30), POINT (20 20), POINT (30 10)]", 
            "title": "ST_Dump"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_dumppoints", 
            "text": "Introduction: Returns list of Points which geometry consists of.  Format:  ST_DumpPoints(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_DumpPoints ( ST_GeomFromText ( LINESTRING (0 0, 1 1, 1 0) ))    Output:  [POINT (0 0), POINT (0 1), POINT (1 1), POINT (1 0), POINT (0 0)]", 
            "title": "ST_DumpPoints"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_isclosed", 
            "text": "Introduction: RETURNS true if the LINESTRING start and end point are the same.  Format:  ST_IsClosed(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_IsClosed ( ST_GeomFromText ( LINESTRING(0 0, 1 1, 1 0) ))   Output:  false", 
            "title": "ST_IsClosed"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_numinteriorrings", 
            "text": "Introduction: RETURNS number of interior rings of polygon geometries.  Format:  ST_NumInteriorRings(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_NumInteriorRings ( ST_GeomFromText ( POLYGON ((0 0, 0 5, 5 5, 5 0, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1)) ))   Output:  1", 
            "title": "ST_NumInteriorRings"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_addpoint", 
            "text": "Introduction: RETURN Linestring with additional point at the given index, if position is not available the point will be added at the end of line.  Format:  ST_AddPoint(geom: geometry, point: geometry, position: integer)  Format:  ST_AddPoint(geom: geometry, point: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_AddPoint ( ST_GeomFromText ( LINESTRING(0 0, 1 1, 1 0) ),   ST_GeomFromText ( Point(21 52) ),   1 )  SELECT   ST_AddPoint ( ST_GeomFromText ( Linestring(0 0, 1 1, 1 0) ),   ST_GeomFromText ( Point(21 52) ))   Output: LINESTRING(0 0, 21 52, 1 1, 1 0)\nLINESTRING(0 0, 1 1, 1 0, 21 52)", 
            "title": "ST_AddPoint"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_removepoint", 
            "text": "Introduction: RETURN Line with removed point at given index, position can be omitted and then last one will be removed.  Format:  ST_RemovePoint(geom: geometry, position: integer)  Format:  ST_RemovePoint(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_RemovePoint ( ST_GeomFromText ( LINESTRING(0 0, 1 1, 1 0) ),   1 )   Output:  LINESTRING(0 0, 1 0)", 
            "title": "ST_RemovePoint"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_isring", 
            "text": "Introduction: RETURN true if LINESTRING is ST_IsClosed and ST_IsSimple.  Format:  ST_IsRing(geom: geometry)  Since:  v1.3.2  Spark SQL example: SELECT   ST_IsRing ( ST_GeomFromText ( LINESTRING(0 0, 0 1, 1 1, 1 0, 0 0) ))   Output:  true", 
            "title": "ST_IsRing"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_numgeometries", 
            "text": "Introduction: Returns the number of Geometries. If geometry is a GEOMETRYCOLLECTION (or MULTI*) return the number of geometries, for single geometries will return 1.  Format:  ST_NumGeometries (A:geometry)  Since:  v1.3.2  SELECT   ST_NumGeometries ( df . geometry )  FROM   df", 
            "title": "ST_NumGeometries"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/", 
            "text": "ST_Contains\n\n\nIntroduction: Return true if A fully contains B\n\n\nFormat: \nST_Contains (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Contains\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n),\n \npointdf\n.\narealandmark\n)\n\n\n\n\nST_Intersects\n\n\nIntroduction: Return true if A intersects B\n\n\nFormat: \nST_Intersects (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Intersects\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n),\n \npointdf\n.\narealandmark\n)\n\n\n\n\nST_Within\n\n\nIntroduction: Return true if A is fully contained by B\n\n\nFormat: \nST_Within (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Within\n(\npointdf\n.\narealandmark\n,\n \nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n))\n\n\n\n\nST_Equals\n\n\nIntroduction: Return true if A equals to B\n\n\nFormat: \nST_Equals (A:geometry, B:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Equals\n(\npointdf\n.\narealandmark\n,\n \nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n))\n\n\n\n\nST_Crosses\n\n\nIntroduction: Return true if A crosses B\n\n\nFormat: \nST_Crosses (A:geometry, B:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Crosses\n(\npointdf\n.\narealandmark\n,\n \nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n))\n\n\n\n\nST_Touches\n\n\nIntroduction: Return true if A touches B\n\n\nFormat: \nST_Touches (A:geometry, B:geometry)\n\n\nSince: \nv1.2.0\n\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Touches\n(\npointdf\n.\narealandmark\n,\n \nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n))\n\n\n\n\n\nST_Overlaps\n\n\nIntroduction: Return true if A overlaps B\n\n\nFormat: \nST_Overlaps (A:geometry, B:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n\n\nFROM\n \ngeom\n\n\nWHERE\n \nST_Overlaps\n(\ngeom\n.\ngeom_a\n,\n \ngeom\n.\ngeom_b\n)", 
            "title": "Predicate"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_contains", 
            "text": "Introduction: Return true if A fully contains B  Format:  ST_Contains (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Contains ( ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ),   pointdf . arealandmark )", 
            "title": "ST_Contains"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_intersects", 
            "text": "Introduction: Return true if A intersects B  Format:  ST_Intersects (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Intersects ( ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ),   pointdf . arealandmark )", 
            "title": "ST_Intersects"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_within", 
            "text": "Introduction: Return true if A is fully contained by B  Format:  ST_Within (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Within ( pointdf . arealandmark ,   ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ))", 
            "title": "ST_Within"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_equals", 
            "text": "Introduction: Return true if A equals to B  Format:  ST_Equals (A:geometry, B:geometry)  Since:  v1.2.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Equals ( pointdf . arealandmark ,   ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ))", 
            "title": "ST_Equals"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_crosses", 
            "text": "Introduction: Return true if A crosses B  Format:  ST_Crosses (A:geometry, B:geometry)  Since:  v1.2.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Crosses ( pointdf . arealandmark ,   ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ))", 
            "title": "ST_Crosses"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_touches", 
            "text": "Introduction: Return true if A touches B  Format:  ST_Touches (A:geometry, B:geometry)  Since:  v1.2.0  SELECT   *   FROM   pointdf   WHERE   ST_Touches ( pointdf . arealandmark ,   ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ))", 
            "title": "ST_Touches"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_overlaps", 
            "text": "Introduction: Return true if A overlaps B  Format:  ST_Overlaps (A:geometry, B:geometry)  Since:  v1.2.0  Spark SQL example: SELECT   *  FROM   geom  WHERE   ST_Overlaps ( geom . geom_a ,   geom . geom_b )", 
            "title": "ST_Overlaps"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/", 
            "text": "ST_Envelope_Aggr\n\n\nIntroduction: Return the entire envelope boundary of all geometries in A\n\n\nFormat: \nST_Envelope_Aggr (A:geometryColumn)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Envelope_Aggr\n(\npointdf\n.\narealandmark\n)\n\n\nFROM\n \npointdf\n\n\n\n\nST_Union_Aggr\n\n\nIntroduction: Return the polygon union of all polygons in A\n\n\nFormat: \nST_Union_Aggr (A:geometryColumn)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Union_Aggr\n(\npolygondf\n.\npolygonshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Intersection_Aggr\n\n\nIntroduction: Return the polygon intersection of all polygons in A\n\n\nFormat: \nST_Intersection_Aggr (A:geometryColumn)\n\n\nSince: \nv1.2.1\n\n\nSpark SQL example:\n\nSELECT\n \nST_Intersection_Aggr\n(\npolygondf\n.\npolygonshape\n)\n\n\nFROM\n \npolygondf", 
            "title": "Aggregate function"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_envelope_aggr", 
            "text": "Introduction: Return the entire envelope boundary of all geometries in A  Format:  ST_Envelope_Aggr (A:geometryColumn)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Envelope_Aggr ( pointdf . arealandmark )  FROM   pointdf", 
            "title": "ST_Envelope_Aggr"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_union_aggr", 
            "text": "Introduction: Return the polygon union of all polygons in A  Format:  ST_Union_Aggr (A:geometryColumn)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Union_Aggr ( polygondf . polygonshape )  FROM   polygondf", 
            "title": "ST_Union_Aggr"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_intersection_aggr", 
            "text": "Introduction: Return the polygon intersection of all polygons in A  Format:  ST_Intersection_Aggr (A:geometryColumn)  Since:  v1.2.1  Spark SQL example: SELECT   ST_Intersection_Aggr ( polygondf . polygonshape )  FROM   polygondf", 
            "title": "ST_Intersection_Aggr"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/", 
            "text": "GeoSparkSQL query optimizer\n\n\nGeoSpark Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:\n\n\n\n\nAutomatically optimizes range join query and distance join query.\n\n\nAutomatically performs predicate pushdown.\n\n\n\n\nRange join\n\n\nIntroduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate. Most predicates supported by GeoSparkSQL can trigger a range join.\n\n\nSpark SQL Example:\n\n\nSELECT\n \n*\n\n\nFROM\n \npolygondf\n,\n \npointdf\n\n\nWHERE\n \nST_Contains\n(\npolygondf\n.\npolygonshape\n,\npointdf\n.\npointshape\n)\n\n\n\n\n\nSELECT\n \n*\n\n\nFROM\n \npolygondf\n,\n \npointdf\n\n\nWHERE\n \nST_Intersects\n(\npolygondf\n.\npolygonshape\n,\npointdf\n.\npointshape\n)\n\n\n\n\n\nSELECT\n \n*\n\n\nFROM\n \npointdf\n,\n \npolygondf\n\n\nWHERE\n \nST_Within\n(\npointdf\n.\npointshape\n,\n \npolygondf\n.\npolygonshape\n)\n\n\n\nSpark SQL Physical plan:\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\nRangeJoin\n \npolygonshape#\n20\n:\n \ngeometry\n,\n \npointshape#\n43\n:\n \ngeometry\n,\n \nfalse\n\n\n:-\n \nProject\n \n[\nst_polygonfromenvelope\n(\ncast\n(\n_\nc0#\n0\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n1\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc2#\n2\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc3#\n3\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmypolygonid\n)\n \nAS\n \npolygonshape#\n20\n]\n\n\n:\n  \n+-\n \n*\nFileScan\n \ncsv\n\n\n+-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n31\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n32\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape#\n43\n]\n\n   \n+-\n \n*\nFileScan\n \ncsv\n\n\n\n\n\n\nNote\n\n\nAll join queries in GeoSparkSQL are inner joins\n\n\n\n\nDistance join\n\n\nIntroduction: Find geometries from A and geometries from B such that the internal Euclidean distance of each geometry pair is less or equal than a certain distance\n\n\nSpark SQL Example:\n\n\nOnly consider \nfully within a certain distance\n\n\nSELECT\n \n*\n\n\nFROM\n \npointdf1\n,\n \npointdf2\n\n\nWHERE\n \nST_Distance\n(\npointdf1\n.\npointshape1\n,\npointdf2\n.\npointshape2\n)\n \n \n2\n\n\n\n\nConsider \nintersects within a certain distance\n\n\nSELECT\n \n*\n\n\nFROM\n \npointdf1\n,\n \npointdf2\n\n\nWHERE\n \nST_Distance\n(\npointdf1\n.\npointshape1\n,\npointdf2\n.\npointshape2\n)\n \n=\n \n2\n\n\n\n\nSpark SQL Physical plan:\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\nDistanceJoin\n \npointshape1#\n12\n:\n \ngeometry\n,\n \npointshape2#\n33\n:\n \ngeometry\n,\n \n2.0\n,\n \ntrue\n\n\n:-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n0\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n1\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape1#\n12\n]\n\n\n:\n  \n+-\n \n*\nFileScan\n \ncsv\n\n\n+-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n21\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n22\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape2#\n33\n]\n\n   \n+-\n \n*\nFileScan\n \ncsv\n\n\n\n\n\n\nWarning\n\n\nGeoSpark doesn't control the distance's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See \nST_Transform\n.\n\n\n\n\nPredicate pushdown\n\n\nIntroduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query*\n\n\nSpark SQL Example:\n\n\nSELECT\n \n*\n\n\nFROM\n \npolygondf\n,\n \npointdf\n \n\nWHERE\n \nST_Contains\n(\npolygondf\n.\npolygonshape\n,\npointdf\n.\npointshape\n)\n\n\nAND\n \nST_Contains\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n101\n.\n0\n,\n501\n.\n0\n,\n601\n.\n0\n),\n \npolygondf\n.\npolygonshape\n)\n\n\n\n\n\nSpark SQL Physical plan:\n\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\nRangeJoin\n \npolygonshape#\n20\n:\n \ngeometry\n,\n \npointshape#\n43\n:\n \ngeometry\n,\n \nfalse\n\n\n:-\n \nProject\n \n[\nst_polygonfromenvelope\n(\ncast\n(\n_\nc0#\n0\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n1\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc2#\n2\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc3#\n3\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmypolygonid\n)\n \nAS\n \npolygonshape#\n20\n]\n\n\n:\n  \n+-\n \nFilter\n  \n**org\n.\napache\n.\nspark\n.\nsql\n.\ngeosparksql\n.\nexpressions\n.\nST_Contains\n$\n**\n\n\n:\n     \n+-\n \n*\nFileScan\n \ncsv\n\n\n+-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n31\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n32\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape#\n43\n]\n\n   \n+-\n \n*\nFileScan\n \ncsv", 
            "title": "Join query (optimizer)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#geosparksql-query-optimizer", 
            "text": "GeoSpark Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:   Automatically optimizes range join query and distance join query.  Automatically performs predicate pushdown.", 
            "title": "GeoSparkSQL query optimizer"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#range-join", 
            "text": "Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate. Most predicates supported by GeoSparkSQL can trigger a range join.  Spark SQL Example:  SELECT   *  FROM   polygondf ,   pointdf  WHERE   ST_Contains ( polygondf . polygonshape , pointdf . pointshape )   SELECT   *  FROM   polygondf ,   pointdf  WHERE   ST_Intersects ( polygondf . polygonshape , pointdf . pointshape )   SELECT   *  FROM   pointdf ,   polygondf  WHERE   ST_Within ( pointdf . pointshape ,   polygondf . polygonshape )  \nSpark SQL Physical plan: ==   Physical   Plan   ==  RangeJoin   polygonshape# 20 :   geometry ,   pointshape# 43 :   geometry ,   false  :-   Project   [ st_polygonfromenvelope ( cast ( _ c0# 0   as   decimal ( 24 , 20 )),   cast ( _ c1# 1   as   decimal ( 24 , 20 )),   cast ( _ c2# 2   as   decimal ( 24 , 20 )),   cast ( _ c3# 3   as   decimal ( 24 , 20 )),   mypolygonid )   AS   polygonshape# 20 ]  :    +-   * FileScan   csv  +-   Project   [ st_point ( cast ( _ c0# 31   as   decimal ( 24 , 20 )),   cast ( _ c1# 32   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape# 43 ] \n    +-   * FileScan   csv    Note  All join queries in GeoSparkSQL are inner joins", 
            "title": "Range join"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#distance-join", 
            "text": "Introduction: Find geometries from A and geometries from B such that the internal Euclidean distance of each geometry pair is less or equal than a certain distance  Spark SQL Example:  Only consider  fully within a certain distance  SELECT   *  FROM   pointdf1 ,   pointdf2  WHERE   ST_Distance ( pointdf1 . pointshape1 , pointdf2 . pointshape2 )     2   Consider  intersects within a certain distance  SELECT   *  FROM   pointdf1 ,   pointdf2  WHERE   ST_Distance ( pointdf1 . pointshape1 , pointdf2 . pointshape2 )   =   2   Spark SQL Physical plan: ==   Physical   Plan   ==  DistanceJoin   pointshape1# 12 :   geometry ,   pointshape2# 33 :   geometry ,   2.0 ,   true  :-   Project   [ st_point ( cast ( _ c0# 0   as   decimal ( 24 , 20 )),   cast ( _ c1# 1   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape1# 12 ]  :    +-   * FileScan   csv  +-   Project   [ st_point ( cast ( _ c0# 21   as   decimal ( 24 , 20 )),   cast ( _ c1# 22   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape2# 33 ] \n    +-   * FileScan   csv    Warning  GeoSpark doesn't control the distance's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See  ST_Transform .", 
            "title": "Distance join"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#predicate-pushdown", 
            "text": "Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query*  Spark SQL Example:  SELECT   *  FROM   polygondf ,   pointdf   WHERE   ST_Contains ( polygondf . polygonshape , pointdf . pointshape )  AND   ST_Contains ( ST_PolygonFromEnvelope ( 1 . 0 , 101 . 0 , 501 . 0 , 601 . 0 ),   polygondf . polygonshape )   Spark SQL Physical plan:  ==   Physical   Plan   ==  RangeJoin   polygonshape# 20 :   geometry ,   pointshape# 43 :   geometry ,   false  :-   Project   [ st_polygonfromenvelope ( cast ( _ c0# 0   as   decimal ( 24 , 20 )),   cast ( _ c1# 1   as   decimal ( 24 , 20 )),   cast ( _ c2# 2   as   decimal ( 24 , 20 )),   cast ( _ c3# 3   as   decimal ( 24 , 20 )),   mypolygonid )   AS   polygonshape# 20 ]  :    +-   Filter    **org . apache . spark . sql . geosparksql . expressions . ST_Contains $ **  :       +-   * FileScan   csv  +-   Project   [ st_point ( cast ( _ c0# 31   as   decimal ( 24 , 20 )),   cast ( _ c1# 32   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape# 43 ] \n    +-   * FileScan   csv", 
            "title": "Predicate pushdown"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/", 
            "text": "Usage\n\n\nGeoSparkSQL supports many parameters. To change their values,\n\n\n\n\nSet it through SparkConf:\n\nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n().\n\n      \nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n).\n\n      \nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n).\n\n      \nconfig\n(\ngeospark.global.index\n,\ntrue\n)\n\n      \nmaster\n(\nlocal[*]\n).\nappName\n(\nmyGeoSparkSQLdemo\n).\ngetOrCreate\n()\n\n\n\n\nCheck your current GeoSparkSQL configuration:\n\nval\n \ngeosparkConf\n \n=\n \nnew\n \nGeoSparkConf\n(\nsparkSession\n.\nsparkContext\n.\ngetConf\n)\n\n\nprintln\n(\ngeosparkConf\n)\n\n\n\n\n\n\nExplanation\n\n\n\n\ngeospark.global.index\n\n\nUse spatial index (currently, only supports in SQL range join and SQL distance join)\n\n\nDefault: true\n\n\nPossible values: true, false\n\n\n\n\n\n\ngeospark.global.indextype\n\n\nSpatial index type, only valid when \"geospark.global.index\" is true\n\n\nDefault: rtree\n\n\nPossible values: rtree, quadtree\n\n\n\n\n\n\ngeospark.join.gridtype\n\n\nSpatial partitioning grid type for join query\n\n\nDefault: quadtree\n\n\nPossible values: quadtree, kdbtree, rtree, voronoi\n\n\n\n\n\n\ngeospark.join.numpartition \n(Advanced users only!)\n\n\nNumber of partitions for both sides in a join query\n\n\nDefault: -1, which means use the existing partitions\n\n\nPossible values: any integers\n\n\n\n\n\n\ngeospark.join.indexbuildside \n(Advanced users only!)\n\n\nThe side which GeoSpark builds spatial indices on\n\n\nDefault: left\n\n\nPossible values: left, right\n\n\n\n\n\n\ngeospark.join.spatitionside \n(Advanced users only!)\n\n\nThe dominant side in spatial partitioning stage\n\n\nDefault: left\n\n\nPossible values: left, right", 
            "title": "Parameter"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/#usage", 
            "text": "GeoSparkSQL supports many parameters. To change their values,   Set it through SparkConf: sparkSession   =   SparkSession . builder (). \n       config ( spark.serializer , classOf [ KryoSerializer ]. getName ). \n       config ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName ). \n       config ( geospark.global.index , true ) \n       master ( local[*] ). appName ( myGeoSparkSQLdemo ). getOrCreate ()   Check your current GeoSparkSQL configuration: val   geosparkConf   =   new   GeoSparkConf ( sparkSession . sparkContext . getConf )  println ( geosparkConf )", 
            "title": "Usage"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/#explanation", 
            "text": "geospark.global.index  Use spatial index (currently, only supports in SQL range join and SQL distance join)  Default: true  Possible values: true, false    geospark.global.indextype  Spatial index type, only valid when \"geospark.global.index\" is true  Default: rtree  Possible values: rtree, quadtree    geospark.join.gridtype  Spatial partitioning grid type for join query  Default: quadtree  Possible values: quadtree, kdbtree, rtree, voronoi    geospark.join.numpartition  (Advanced users only!)  Number of partitions for both sides in a join query  Default: -1, which means use the existing partitions  Possible values: any integers    geospark.join.indexbuildside  (Advanced users only!)  The side which GeoSpark builds spatial indices on  Default: left  Possible values: left, right    geospark.join.spatitionside  (Advanced users only!)  The dominant side in spatial partitioning stage  Default: left  Possible values: left, right", 
            "title": "Explanation"
        }, 
        {
            "location": "/api/viz/sql/", 
            "text": "Quick start\n\n\nThe detailed explanation is here: \nVisualize Spatial DataFrame/RDD\n.\n\n\n\n\nAdd GeoSpark-core, GeoSparkSQL, GeoSparkViz into your project POM.xml or build.sbt\n\n\nDeclare your Spark Session\n\nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n().\n\n\nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n).\n\n\nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkVizKryoRegistrator\n].\ngetName\n).\n\n\nmaster\n(\nlocal[*]\n).\nappName\n(\nmyGeoSparkVizDemo\n).\ngetOrCreate\n()\n\n\n\n\nAdd the following lines after your SparkSession declaration:\n\nGeoSparkSQLRegistrator\n.\nregisterAll\n(\nsparkSession\n)\n\n\nGeoSparkVizRegistrator\n.\nregisterAll\n(\nsparkSession\n)\n\n\n\n\n\n\nRegular functions\n\n\nST_Pixelize\n\n\nIntroduction: Return a pixel for a given resolution\n\n\nFormat: \nST_Pixelize (A:geometry, ResolutionX:int, ResolutionY:int, Boundary:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Pixelize\n(\nshape\n,\n \n256\n,\n \n256\n,\n \n(\nST_Envelope_Aggr\n(\nshape\n)\n \nFROM\n \npointtable\n))\n\n\nFROM\n \npolygondf\n\n\n\n\nST_TileName\n\n\nIntroduction: Return the map tile name for a given zoom level. Please refer to \nOpenStreetMap ZoomLevel\n and \nOpenStreetMap tile name\n.\n\n\n\n\nNote\n\n\nTile name is formatted as a \"Z-X-Y\" string. Z is zoom level. X is tile coordinate on X axis. Y is tile coordinate on Y axis.\n\n\n\n\nFormat: \nST_TileName (A:pixel, ZoomLevel:int)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_TileName\n(\npixels\n.\npx\n,\n \n3\n)\n\n\nFROM\n \npixels\n\n\n\n\nST_Colorize\n\n\nIntroduction: Given the weight of a pixel, return the corresponding color. The weight can be the spatial aggregation of spatial objects or spatial observations such as temperature and humidity.\n\n\n\n\nNote\n\n\nThe color is encoded to an Integer type value in DataFrame. When you print it, it will show some nonsense values. You can just treat them as colors in GeoSparkViz.\n\n\n\n\nFormat: \nST_Colorize (weight:Double, maxWeight:Double, mandatory color: string (Optional))\n\n\nSince: \nv1.2.0\n\n\nProduce various colors - heat map\n\n\nThis function will normalize the weight according to the max weight among all pixels. Different pixel obtains different color.\n\n\nSpark SQL example:\n\nSELECT\n \npixels\n.\npx\n,\n \nST_Colorize\n(\npixels\n.\nweight\n,\n \n999\n)\n \nAS\n \ncolor\n\n\nFROM\n \npixels\n\n\n\n\nProduce uniform colors - scatter plot\n\n\nIf a mandatory color name is put as the third input argument, this function will directly ouput this color, without considering the weights. In this case, every pixel will possess the same color.\n\n\nSpark SQL example:\n\nSELECT\n \npixels\n.\npx\n,\n \nST_Colorize\n(\npixels\n.\nweight\n,\n \n999\n,\n \nred\n)\n \nAS\n \ncolor\n\n\nFROM\n \npixels\n\n\n\n\nHere are some example color names can be entered:\n\nfirebrick\n\n\n#aa38e0\n\n\n0x40A8CC\n\n\nrgba(112,36,228,0.9)\n\n\n\n\nPlease refer to \nAWT Colors\n for a list of pre-defined colors.\n\n\nST_EncodeImage\n\n\nIntroduction: Return the base64 string representation of a Java PNG BufferedImage. This is specific for the server-client environment. For example, transfer the base64 string from GeoSparkViz to Apache Zeppelin.\n\n\nFormat: \nST_EncodeImage (A:image)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_EncodeImage\n(\nimages\n.\nimg\n)\n\n\nFROM\n \nimages\n\n\n\n\nAggregate functions\n\n\nST_Render\n\n\nIntroduction: Given a group of pixels and their colors, return a single Java PNG BufferedImage\n\n\nFormat: \nST_Render (A:pixel, B:color)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \ntilename\n,\n \nST_Render\n(\npixels\n.\npx\n,\n \npixels\n.\ncolor\n)\n \nAS\n \ntileimg\n\n\nFROM\n \npixels\n\n\nGROUP\n \nBY\n \ntilename", 
            "title": "DataFrame/SQL"
        }, 
        {
            "location": "/api/viz/sql/#quick-start", 
            "text": "The detailed explanation is here:  Visualize Spatial DataFrame/RDD .   Add GeoSpark-core, GeoSparkSQL, GeoSparkViz into your project POM.xml or build.sbt  Declare your Spark Session sparkSession   =   SparkSession . builder ().  config ( spark.serializer , classOf [ KryoSerializer ]. getName ).  config ( spark.kryo.registrator ,   classOf [ GeoSparkVizKryoRegistrator ]. getName ).  master ( local[*] ). appName ( myGeoSparkVizDemo ). getOrCreate ()   Add the following lines after your SparkSession declaration: GeoSparkSQLRegistrator . registerAll ( sparkSession )  GeoSparkVizRegistrator . registerAll ( sparkSession )", 
            "title": "Quick start"
        }, 
        {
            "location": "/api/viz/sql/#regular-functions", 
            "text": "", 
            "title": "Regular functions"
        }, 
        {
            "location": "/api/viz/sql/#st_pixelize", 
            "text": "Introduction: Return a pixel for a given resolution  Format:  ST_Pixelize (A:geometry, ResolutionX:int, ResolutionY:int, Boundary:geometry)  Since:  v1.2.0  Spark SQL example: SELECT   ST_Pixelize ( shape ,   256 ,   256 ,   ( ST_Envelope_Aggr ( shape )   FROM   pointtable ))  FROM   polygondf", 
            "title": "ST_Pixelize"
        }, 
        {
            "location": "/api/viz/sql/#st_tilename", 
            "text": "Introduction: Return the map tile name for a given zoom level. Please refer to  OpenStreetMap ZoomLevel  and  OpenStreetMap tile name .   Note  Tile name is formatted as a \"Z-X-Y\" string. Z is zoom level. X is tile coordinate on X axis. Y is tile coordinate on Y axis.   Format:  ST_TileName (A:pixel, ZoomLevel:int)  Since:  v1.2.0  Spark SQL example: SELECT   ST_TileName ( pixels . px ,   3 )  FROM   pixels", 
            "title": "ST_TileName"
        }, 
        {
            "location": "/api/viz/sql/#st_colorize", 
            "text": "Introduction: Given the weight of a pixel, return the corresponding color. The weight can be the spatial aggregation of spatial objects or spatial observations such as temperature and humidity.   Note  The color is encoded to an Integer type value in DataFrame. When you print it, it will show some nonsense values. You can just treat them as colors in GeoSparkViz.   Format:  ST_Colorize (weight:Double, maxWeight:Double, mandatory color: string (Optional))  Since:  v1.2.0", 
            "title": "ST_Colorize"
        }, 
        {
            "location": "/api/viz/sql/#produce-various-colors-heat-map", 
            "text": "This function will normalize the weight according to the max weight among all pixels. Different pixel obtains different color.  Spark SQL example: SELECT   pixels . px ,   ST_Colorize ( pixels . weight ,   999 )   AS   color  FROM   pixels", 
            "title": "Produce various colors - heat map"
        }, 
        {
            "location": "/api/viz/sql/#produce-uniform-colors-scatter-plot", 
            "text": "If a mandatory color name is put as the third input argument, this function will directly ouput this color, without considering the weights. In this case, every pixel will possess the same color.  Spark SQL example: SELECT   pixels . px ,   ST_Colorize ( pixels . weight ,   999 ,   red )   AS   color  FROM   pixels   Here are some example color names can be entered: firebrick  #aa38e0  0x40A8CC  rgba(112,36,228,0.9)   Please refer to  AWT Colors  for a list of pre-defined colors.", 
            "title": "Produce uniform colors - scatter plot"
        }, 
        {
            "location": "/api/viz/sql/#st_encodeimage", 
            "text": "Introduction: Return the base64 string representation of a Java PNG BufferedImage. This is specific for the server-client environment. For example, transfer the base64 string from GeoSparkViz to Apache Zeppelin.  Format:  ST_EncodeImage (A:image)  Since:  v1.2.0  Spark SQL example: SELECT   ST_EncodeImage ( images . img )  FROM   images", 
            "title": "ST_EncodeImage"
        }, 
        {
            "location": "/api/viz/sql/#aggregate-functions", 
            "text": "", 
            "title": "Aggregate functions"
        }, 
        {
            "location": "/api/viz/sql/#st_render", 
            "text": "Introduction: Given a group of pixels and their colors, return a single Java PNG BufferedImage  Format:  ST_Render (A:pixel, B:color)  Since:  v1.2.0  Spark SQL example: SELECT   tilename ,   ST_Render ( pixels . px ,   pixels . color )   AS   tileimg  FROM   pixels  GROUP   BY   tilename", 
            "title": "ST_Render"
        }, 
        {
            "location": "/api/viz/Babylon-Scala-and-Java-API/", 
            "text": "Scala and Java API for RDD\n\n\nGeoSpark-Viz Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/geosparkviz/javadoc/\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSparkViz Scala users use the same APIs with GeoSparkViz Java users.", 
            "title": "RDD"
        }, 
        {
            "location": "/api/viz/Babylon-Scala-and-Java-API/#scala-and-java-api-for-rdd", 
            "text": "GeoSpark-Viz Scala and Java API:  http://www.public.asu.edu/~jiayu2/geosparkviz/javadoc/  Note: Scala can call Java APIs seamlessly. That means GeoSparkViz Scala users use the same APIs with GeoSparkViz Java users.", 
            "title": "Scala and Java API for RDD"
        }, 
        {
            "location": "/contribute/rule/", 
            "text": "Contributing to GeoSpark\n\n\nThe project welcomes contributions. You can contribute to GeoSpark code or documentation by making Pull Requests on \nGeoSpark GitHub Repo\n.\n\n\nThe following sections brief the workflow of how to complete a contribution.\n\n\nPick / Annouce a task\n\n\nIt is important to confirm that your contribution is acceptable. Hence, you have two options to start with:\n\n\n\n\n\n\nPick an issue from the Issues tagged by \nHelp Wanted\n on \nGeoSpark Issues\n.\n\n\n\n\n\n\nAnnounce what you are going to work on in advance if no GitHub issues match your scenario. To do this, contact \nGeoSpark project committer\n.\n\n\n\n\n\n\nDevelop a code contribution\n\n\nCode contributions should include the following:\n\n\n\n\nDetailed documentations on classes and methods.\n\n\nUnit Tests to demonstrate code correctness and allow this to be maintained going forward.  In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any).  Unit Tests can be JUnit test or Scala test. Some GeoSpark functions need to be tested in both Scala and Java.\n\n\nUpdates on corresponding GeoSpark documentation if necessary.\n\n\n\n\nCode contributions must include a license header at the top of each file.  A sample header for Scala/Java files is as follows:\n\n/*\n\n\n * FILE: SpatialRDD\n\n\n * Copyright (c) 2015 - 2018 GeoSpark Development Team\n\n\n *\n\n\n * MIT License\n\n\n *\n\n\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n\n\n * of this software and associated documentation files (the \nSoftware\n), to deal\n\n\n * in the Software without restriction, including without limitation the rights\n\n\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\n\n * copies of the Software, and to permit persons to whom the Software is\n\n\n * furnished to do so, subject to the following conditions:\n\n\n *\n\n\n * The above copyright notice and this permission notice shall be included in all\n\n\n * copies or substantial portions of the Software.\n\n\n *\n\n\n * THE SOFTWARE IS PROVIDED \nAS IS\n, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\n\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\n\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\n\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\n\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\n\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\n\n * SOFTWARE.\n\n\n *\n\n\n */\n\n\n\n\nDevelop a document contribution\n\n\nDocumentation contributions should satisfy the following requirements:\n\n\n\n\nDetailed explanation with examples.\n\n\nPlace a newly added document in a proper folder\n\n\nChange the \nmkdocs.yml\n if necessary\n\n\n\n\n\n\nNote\n\n\nPlease read \nCompile the source code\n to learn how to compile GeoSpark website.\n\n\n\n\nMake a Pull Request\n\n\nAfter developing a contribution, the easiest and most visible way to push a contribution is to submit a Pull Request (PR) to the \nGitHub repo\n.  \n\n\nWhen preparing a PR, please answser the following questions in the PR:\n\n\n\n\n\n\nWhat changes were proposed in this pull request?\n\n\n\n\n\n\nHow was this patch tested?\n\n\n\n\n\n\nWhen a PR is submitted Travis CI will check the build correctness. Please check the PR status, and fix any reported problems.", 
            "title": "Contributing rule"
        }, 
        {
            "location": "/contribute/rule/#contributing-to-geospark", 
            "text": "The project welcomes contributions. You can contribute to GeoSpark code or documentation by making Pull Requests on  GeoSpark GitHub Repo .  The following sections brief the workflow of how to complete a contribution.", 
            "title": "Contributing to GeoSpark"
        }, 
        {
            "location": "/contribute/rule/#pick-annouce-a-task", 
            "text": "It is important to confirm that your contribution is acceptable. Hence, you have two options to start with:    Pick an issue from the Issues tagged by  Help Wanted  on  GeoSpark Issues .    Announce what you are going to work on in advance if no GitHub issues match your scenario. To do this, contact  GeoSpark project committer .", 
            "title": "Pick / Annouce a task"
        }, 
        {
            "location": "/contribute/rule/#develop-a-code-contribution", 
            "text": "Code contributions should include the following:   Detailed documentations on classes and methods.  Unit Tests to demonstrate code correctness and allow this to be maintained going forward.  In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any).  Unit Tests can be JUnit test or Scala test. Some GeoSpark functions need to be tested in both Scala and Java.  Updates on corresponding GeoSpark documentation if necessary.   Code contributions must include a license header at the top of each file.  A sample header for Scala/Java files is as follows: /*   * FILE: SpatialRDD   * Copyright (c) 2015 - 2018 GeoSpark Development Team   *   * MIT License   *   * Permission is hereby granted, free of charge, to any person obtaining a copy   * of this software and associated documentation files (the  Software ), to deal   * in the Software without restriction, including without limitation the rights   * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell   * copies of the Software, and to permit persons to whom the Software is   * furnished to do so, subject to the following conditions:   *   * The above copyright notice and this permission notice shall be included in all   * copies or substantial portions of the Software.   *   * THE SOFTWARE IS PROVIDED  AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR   * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE   * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER   * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,   * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE   * SOFTWARE.   *   */", 
            "title": "Develop a code contribution"
        }, 
        {
            "location": "/contribute/rule/#develop-a-document-contribution", 
            "text": "Documentation contributions should satisfy the following requirements:   Detailed explanation with examples.  Place a newly added document in a proper folder  Change the  mkdocs.yml  if necessary    Note  Please read  Compile the source code  to learn how to compile GeoSpark website.", 
            "title": "Develop a document contribution"
        }, 
        {
            "location": "/contribute/rule/#make-a-pull-request", 
            "text": "After developing a contribution, the easiest and most visible way to push a contribution is to submit a Pull Request (PR) to the  GitHub repo .    When preparing a PR, please answser the following questions in the PR:    What changes were proposed in this pull request?    How was this patch tested?    When a PR is submitted Travis CI will check the build correctness. Please check the PR status, and fix any reported problems.", 
            "title": "Make a Pull Request"
        }, 
        {
            "location": "/contribute/contributor/", 
            "text": "GeoSpark has received numerous help from the community. This page lists the contributors and committers of GeoSpark. People on this page are ordered by their last name.\n\n\nCommitters\n\n\nA committer has the write access to GeoSpark main repository and is in charge at least a major component of this project.\n\n\n\n\n\n\n\n\nName\n\n\nAffiliation\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nPawe\u0142 Koci\u0144ski\n\n\nAllegro.pl\n\n\nGeoSpark Python API\n\n\n\n\n\n\nMohamed Sarwat\n\n\nArizona State University\n\n\nAll modules\n\n\n\n\n\n\nJia Yu\n\n\nArizona State University\n\n\nAll modules\n\n\n\n\n\n\nZongsi Zhang\n\n\nGrabTaxi\n\n\nGeoSpark Shapefile and RDD API\n\n\n\n\n\n\n\n\nContributors\n\n\nA contributor contributed at least a function to GeoSpark main repository.\n\n\n\n\n\n\n\n\nName\n\n\nAffiliation\n\n\n\n\n\n\n\n\n\n\nLucas C\n\n\nBrazil\n\n\n\n\n\n\nMasha Basmanova\n\n\nFacebook\n\n\n\n\n\n\nOmkar Kaptan\n\n\nQuantcast\n\n\n\n\n\n\nPawe\u0142 Koci\u0144ski\n\n\nAccenture\n\n\n\n\n\n\nNetanel Malka\n\n\n\n\n\n\n\n\nSergii Mikhtoniuk\n\n\n\n\n\n\n\n\nAvshalom Orenstein\n\n\n\n\n\n\n\n\nAnton Peniaziev\n\n\n\n\n\n\n\n\nMohamed Sarwat\n\n\nArizona State University\n\n\n\n\n\n\nSachio Wakai\n\n\n\n\n\n\n\n\nHui Wang\n\n\nGuangzhou Urban Planning \n Design Survey Research Institute\n\n\n\n\n\n\nJinxuan Wu\n\n\nBloomberg\n\n\n\n\n\n\nJia Yu\n\n\nArizona State University\n\n\n\n\n\n\nZongsi Zhang\n\n\nGrabTaxi\n\n\n\n\n\n\nHarry Zhu\n\n\nMoBike", 
            "title": "Contributors and committers"
        }, 
        {
            "location": "/contribute/contributor/#committers", 
            "text": "A committer has the write access to GeoSpark main repository and is in charge at least a major component of this project.     Name  Affiliation  Role      Pawe\u0142 Koci\u0144ski  Allegro.pl  GeoSpark Python API    Mohamed Sarwat  Arizona State University  All modules    Jia Yu  Arizona State University  All modules    Zongsi Zhang  GrabTaxi  GeoSpark Shapefile and RDD API", 
            "title": "Committers"
        }, 
        {
            "location": "/contribute/contributor/#contributors", 
            "text": "A contributor contributed at least a function to GeoSpark main repository.     Name  Affiliation      Lucas C  Brazil    Masha Basmanova  Facebook    Omkar Kaptan  Quantcast    Pawe\u0142 Koci\u0144ski  Accenture    Netanel Malka     Sergii Mikhtoniuk     Avshalom Orenstein     Anton Peniaziev     Mohamed Sarwat  Arizona State University    Sachio Wakai     Hui Wang  Guangzhou Urban Planning   Design Survey Research Institute    Jinxuan Wu  Bloomberg    Jia Yu  Arizona State University    Zongsi Zhang  GrabTaxi    Harry Zhu  MoBike", 
            "title": "Contributors"
        }, 
        {
            "location": "/contact/", 
            "text": "Contact\n\n\nTwitter: \nApache Sedona@Twitter\n\n\nGitter chat: \n\n\nSedona JIRA\n: Bugs, Pull Requests, and other similar issues\n\n\nSedona Mailing Lists\n: \n\n\n\n\nissues@sedona.apache.org\n: general questions or tutorials\n\n\ndev@sedona.apache.org\n: project development", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#contact", 
            "text": "Twitter:  Apache Sedona@Twitter  Gitter chat:   Sedona JIRA : Bugs, Pull Requests, and other similar issues  Sedona Mailing Lists :    issues@sedona.apache.org : general questions or tutorials  dev@sedona.apache.org : project development", 
            "title": "Contact"
        }, 
        {
            "location": "/publication/", 
            "text": "Publication\n\n\n\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"\n is the full research paper that talks about the entire GeoSpark ecosystem. Please cite this paper if your work mentions GeoSpark core system.\n\n\n\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\"\n is the full research paper that talks about map visualization system in GeoSpark. Please cite this paper if your work mentions GeoSpark visualization system.\n\n\n\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\"\n is the full research paper that talks about the traffic simulator in GeoSpark. Please cite this paper if your work mentions GeoSparkSim traffic simulator.\n\n\n--\n\n\nGeoSpark Ecosystem\n\n\n\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"\n (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2018.\n\n\n\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\"\n (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016\n\n\n\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\"\n (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015\n\n\nGeoSparkViz Visualization System\n\n\n\"GeoSparkViz in Action: A Data System with built-in support for Geospatial Visualization\"\n (demo paper) Jia Yu, Anique Tahir, and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019\n\n\n\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\"\n (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018\n\n\nGeoSparkSim Traffic Simulator\n\n\n\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\"\n (research paper) Zishan Fu, Jia Yu, and Mohamed Sarwat. In Proceedings of the International Conference on Mobile Data Management, MDM, 2019\n\n\nA Tutorial about Geospatial Data Management in Spark\n\n\n\"Geospatial Data Management in Apache Spark: A Tutorial\"\n (Tutorial) Jia Yu and Mohamed Sarwat.  In Proceedings of the International Conference on Data Engineering, ICDE, 2019", 
            "title": "Publications"
        }, 
        {
            "location": "/publication/#publication", 
            "text": "\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"  is the full research paper that talks about the entire GeoSpark ecosystem. Please cite this paper if your work mentions GeoSpark core system.  \"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\"  is the full research paper that talks about map visualization system in GeoSpark. Please cite this paper if your work mentions GeoSpark visualization system.  \"Building A Microscopic Road Network Traffic Simulator in Apache Spark\"  is the full research paper that talks about the traffic simulator in GeoSpark. Please cite this paper if your work mentions GeoSparkSim traffic simulator.  --", 
            "title": "Publication"
        }, 
        {
            "location": "/publication/#geospark-ecosystem", 
            "text": "\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"  (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2018.  \"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\"  (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016  \"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\"  (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015", 
            "title": "GeoSpark Ecosystem"
        }, 
        {
            "location": "/publication/#geosparkviz-visualization-system", 
            "text": "\"GeoSparkViz in Action: A Data System with built-in support for Geospatial Visualization\"  (demo paper) Jia Yu, Anique Tahir, and Mohamed Sarwat. In Proceedings of the International Conference on Data Engineering, ICDE, 2019  \"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\"  (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018", 
            "title": "GeoSparkViz Visualization System"
        }, 
        {
            "location": "/publication/#geosparksim-traffic-simulator", 
            "text": "\"Building A Microscopic Road Network Traffic Simulator in Apache Spark\"  (research paper) Zishan Fu, Jia Yu, and Mohamed Sarwat. In Proceedings of the International Conference on Mobile Data Management, MDM, 2019", 
            "title": "GeoSparkSim Traffic Simulator"
        }, 
        {
            "location": "/publication/#a-tutorial-about-geospatial-data-management-in-spark", 
            "text": "\"Geospatial Data Management in Apache Spark: A Tutorial\"  (Tutorial) Jia Yu and Mohamed Sarwat.  In Proceedings of the International Conference on Data Engineering, ICDE, 2019", 
            "title": "A Tutorial about Geospatial Data Management in Spark"
        }, 
        {
            "location": "/license/License-before-1-2-0/", 
            "text": "License before v1.2.0\n\n\nMIT License (MIT)\n\n\nCopyright \n 2015 - 2018 GeoSpark Development Team\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.", 
            "title": "License-before-1.2.0"
        }, 
        {
            "location": "/license/License-before-1-2-0/#license-before-v120", 
            "text": "MIT License (MIT)  Copyright   2015 - 2018 GeoSpark Development Team  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.", 
            "title": "License before v1.2.0"
        }, 
        {
            "location": "/license/License/", 
            "text": "License\n\n\n                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \n[]\n\n  replaced with your own identifying information. (Don\nt include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \nprinted page\n as the copyright notice for easier\n  identification within third-party archives.\n\n\n\n\n\nCopyright [yyyy] [name of copyright owner]\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n\n\n\n\nUnless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/license/License/#license", 
            "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION    Definitions.  \"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.  \"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.  \"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.  \"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.  \"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.  \"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.  \"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).  \"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.  \"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"  \"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.    Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.    Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.    Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:  (a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and  (b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and   You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and  (d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.  You may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.    Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.    Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.    Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.    Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.    Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.    END OF TERMS AND CONDITIONS  APPENDIX: How to apply the Apache License to your work.    To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets  [] \n  replaced with your own identifying information. (Don t include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same  printed page  as the copyright notice for easier\n  identification within third-party archives.  Copyright [yyyy] [name of copyright owner]  Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }
    ]
}